{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux-4CkMh68-h"
      },
      "source": [
        "## Download the Kaggle VOC 2012 Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDApBHGv_drW"
      },
      "source": [
        "## Requirements to be installed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb6AlOoy_g4_"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RIcQKjLDSBv8"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uS4ROKA5_hhN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from collections import OrderedDict\n",
        "from copy import deepcopy\n",
        "from typing import Any\n",
        "import sys\n",
        "import re\n",
        "import fnmatch\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from timm.models.registry import register_model\n",
        "import math\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "# LayerNorm2d\n",
        "from timm.models._builder import resolve_pretrained_cfg\n",
        "try:\n",
        "    from timm.models._builder import _update_default_kwargs as update_args\n",
        "except:\n",
        "    from timm.models._builder import _update_default_model_kwargs as update_args\n",
        "from timm.models.vision_transformer import Mlp, PatchEmbed\n",
        "from timm.models.layers import DropPath, trunc_normal_\n",
        "# from timm.models.registry import register_model\n",
        "import torch.nn.functional as F\n",
        "# from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n",
        "from einops import rearrange, repeat\n",
        "# from .registry import register_pip_model\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "from torch import Tensor\n",
        "from typing import Optional\n",
        "from thop import clever_format, profile\n",
        "from timm.models.vision_transformer import VisionTransformer, _cfg\n",
        "# from timm.models.registry import register_model\n",
        "from timm.models.layers import trunc_normal_, lecun_normal_\n",
        "from timm.models.layers import trunc_normal_, DropPath, LayerNorm2d\n",
        "\n",
        "from timm.models.layers import DropPath, to_2tuple\n",
        "from timm.models.vision_transformer import _load_weights\n",
        "\n",
        "import math\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "# from mamba_ssm.modules.mamba_simple import Mamba\n",
        "# from mamba_ssm.utils.generation import GenerationMixin\n",
        "# from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n",
        "\n",
        "# from rope import *\n",
        "import random\n",
        "import xml.etree.ElementTree as ET\n",
        "# from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn, rms_norm_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Hu4n7vVZHhCd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mqsVuYZ_6QT"
      },
      "source": [
        "### Conv, C2F and SPFF Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def autopad(k, p=None, d=1):  # kernel, padding, dilation\n",
        "    \"\"\"Pad to 'same' shape outputs.\"\"\"\n",
        "    if d > 1:\n",
        "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n",
        "    if p is None:\n",
        "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
        "    return p\n",
        "class Conv(nn.Module):\n",
        "    \"\"\"Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation).\"\"\"\n",
        "    default_act = nn.SiLU()  # default activation\n",
        "\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
        "        \"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(c2)\n",
        "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\n",
        "        return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "    def forward_fuse(self, x):\n",
        "        \"\"\"Perform transposed convolution of 2D data.\"\"\"\n",
        "        return self.act(self.conv(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "I0mjVutrz4cN"
      },
      "outputs": [],
      "source": [
        "def autopad(k, p=None, d=1):\n",
        "    if d > 1:\n",
        "        # actual kernel-size\n",
        "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]\n",
        "    if p is None:\n",
        "        # auto-pad\n",
        "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]\n",
        "    return p\n",
        "\n",
        "class SiLU(nn.Module):\n",
        "    @staticmethod\n",
        "    def forward(x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):\n",
        "        super().__init__()\n",
        "        c_ = int(c2 * e)  # hidden channels\n",
        "        self.cv1 = Conv(c1, c_, k[0], 1)\n",
        "        self.cv2 = Conv(c_, c2, k[1], 1, g=g)\n",
        "        self.add = shortcut and c1 == c2\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
        "\n",
        "class C2f(nn.Module):\n",
        "    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):\n",
        "        super().__init__()\n",
        "        self.c      = int(c2 * e)\n",
        "        self.cv1    = Conv(c1, 2 * self.c, 1, 1)\n",
        "        self.cv2    = Conv((2 + n) * self.c, c2, 1)\n",
        "        self.m      = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, e=1.0) for _ in range(n))\n",
        "\n",
        "    def forward(self, x):\n",
        "        y1 = self.cv1(x)\n",
        "        y = [y1[:, :self.c, :, :], y1[:, self.c:, :, :]]\n",
        "        # y = list(self.cv1(x).split((self.c, self.c), 1))\n",
        "        y.extend(m(y[-1]) for m in self.m)\n",
        "        return self.cv2(torch.cat(y, 1))\n",
        "\n",
        "class SPPF(nn.Module):\n",
        "    def __init__(self, c1, c2, k=5):\n",
        "        super().__init__()\n",
        "        c_          = c1 // 2\n",
        "        self.cv1    = Conv(c1, c_, 1, 1)\n",
        "        self.cv2    = Conv(c_ * 4, c2, 1, 1)\n",
        "        self.m      = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cv1(x)\n",
        "        y1 = self.m(x)\n",
        "        y2 = self.m(y1)\n",
        "        return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiwobejoz5PZ"
      },
      "source": [
        "## CBAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6igbDv4jHER_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
        "        super(BasicConv, self).__init__()\n",
        "        self.out_channels = out_planes\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
        "        self.relu = nn.ReLU() if relu else None\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class ChannelGate(nn.Module):\n",
        "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
        "        super(ChannelGate, self).__init__()\n",
        "        self.gate_channels = gate_channels\n",
        "        self.mlp = nn.Sequential(\n",
        "            Flatten(),\n",
        "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
        "            )\n",
        "        self.pool_types = pool_types\n",
        "\n",
        "    def forward(self, x):\n",
        "        channel_att_sum = None\n",
        "        for pool_type in self.pool_types:\n",
        "            if pool_type=='avg':\n",
        "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( avg_pool )\n",
        "            elif pool_type=='max':\n",
        "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( max_pool )\n",
        "            elif pool_type=='lp':\n",
        "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( lp_pool )\n",
        "            elif pool_type=='lse':\n",
        "                # LSE pool only\n",
        "                lse_pool = logsumexp_2d(x)\n",
        "                channel_att_raw = self.mlp( lse_pool )\n",
        "\n",
        "            if channel_att_sum is None:\n",
        "                channel_att_sum = channel_att_raw\n",
        "            else:\n",
        "                channel_att_sum = channel_att_sum + channel_att_raw\n",
        "\n",
        "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
        "        return x * scale\n",
        "\n",
        "def logsumexp_2d(tensor):\n",
        "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
        "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
        "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
        "    return outputs\n",
        "\n",
        "class ChannelPool(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
        "\n",
        "class SpatialGate(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialGate, self).__init__()\n",
        "        kernel_size = 7\n",
        "        self.compress = ChannelPool()\n",
        "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
        "    def forward(self, x):\n",
        "        x_compress = self.compress(x)\n",
        "        x_out = self.spatial(x_compress)\n",
        "        scale = F.sigmoid(x_out) # broadcasting\n",
        "        return x * scale\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    default_act = nn.SiLU()\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
        "        super(Conv, self).__init__()\n",
        "        self.ChannelGate = ChannelGate(c1, 16, ['avg', 'max'])\n",
        "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(c2)\n",
        "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
        "        self.SpatialGate = SpatialGate()\n",
        "    def forward(self, x):\n",
        "        x_out = self.ChannelGate(x)\n",
        "        x_out = self.SpatialGate(x_out)\n",
        "        x_conv_out = self.act(self.bn(self.conv(x)))\n",
        "        return self.conv(x_out)+x_conv_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv22IeBmz-IR"
      },
      "source": [
        "## ConvNeXt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YRjW91A5z1dM"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
        "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=0.):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.drop_prob == 0. or not self.training:\n",
        "            return x\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "        random_tensor.floor_()  # binarize\n",
        "        return x.div(keep_prob) * random_tensor\n",
        "\n",
        "class GRN(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.zeros(1, 1, 1, num_features))\n",
        "        self.beta = nn.Parameter(torch.zeros(1, 1, 1, num_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=(1, 2), keepdim=True)\n",
        "        return x * self.gamma + self.beta\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
        "        super().__init__()\n",
        "        drop_path = 0.\n",
        "        if p is None:\n",
        "            p = (k - 1) // 2  # Default to 'same' padding\n",
        "\n",
        "        self.dwconv = nn.Conv2d(c1, c2, kernel_size=k, stride=s, padding=p, groups=g, dilation=d)  # depthwise conv\n",
        "        self.norm = LayerNorm(c2)\n",
        "        self.pwconv1 = nn.Linear(c2, 4 * c2)  # pointwise/1x1 convs, implemented with linear layers\n",
        "        self.act = nn.GELU() if act else nn.Identity()\n",
        "        self.grn = GRN(4 * c2)\n",
        "        self.pwconv2 = nn.Linear(4 * c2, c2)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.dwconv(x)\n",
        "        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
        "        x = self.norm(x)\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.grn(x)\n",
        "        x = self.pwconv2(x)\n",
        "        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
        "        x = x + self.drop_path(x)  # Residual connection\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXXk70jP5lv5"
      },
      "source": [
        "## InceptionNeXt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "CMU8i4p-DT6C"
      },
      "outputs": [],
      "source": [
        "\n",
        "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from timm.models.helpers import checkpoint_seq\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from timm.models.layers import to_2tuple\n",
        "class MetaNeXtBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim,\n",
        "            token_mixer=nn.Identity,\n",
        "            norm_layer=nn.BatchNorm2d,\n",
        "            mlp_layer=None,\n",
        "            mlp_ratio=4,\n",
        "            act_layer=nn.GELU,\n",
        "            ls_init_value=1e-6,\n",
        "            drop_path=0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.token_mixer = token_mixer(dim)\n",
        "        self.norm = norm_layer(dim)\n",
        "        self.mlp = mlp_layer(dim, int(mlp_ratio * dim), act_layer=act_layer) if mlp_layer else nn.Identity()\n",
        "        self.gamma = nn.Parameter(ls_init_value * torch.ones(dim)) if ls_init_value else None\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.token_mixer(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.mlp(x)\n",
        "        if self.gamma is not None:\n",
        "            x = x * self.gamma.reshape(1, -1, 1, 1)\n",
        "        x = self.drop_path(x) + shortcut\n",
        "        return x\n",
        "\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            c1,\n",
        "            c2,\n",
        "            k=3,\n",
        "            s=1,\n",
        "            p=None,\n",
        "            g=1,\n",
        "            num_blocks=1,\n",
        "            act_layer=nn.ReLU,\n",
        "            norm_layer=nn.BatchNorm2d,\n",
        "            drop_path_rate=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Set default padding if not provided\n",
        "        p = p if p is not None else k // 2\n",
        "\n",
        "        # Initial convolution layer\n",
        "        self.conv = nn.Conv2d(c1, c2, kernel_size=k, stride=s, padding=p, groups=g, bias=False)\n",
        "        self.norm = norm_layer(c2)\n",
        "        self.act = act_layer()\n",
        "\n",
        "        # Sequential MetaNeXt blocks\n",
        "        drop_path_rates = [drop_path_rate] * num_blocks\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            MetaNeXtBlock(\n",
        "                dim=c2,\n",
        "                drop_path=drop_path_rates[i],\n",
        "                mlp_layer=ConvMlp,\n",
        "                norm_layer=norm_layer,\n",
        "                act_layer=nn.GELU\n",
        "            ) for i in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.act(x)\n",
        "        x = self.blocks(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvMlp(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_features, hidden_features=None, out_features=None, act_layer=nn.ReLU,\n",
        "            norm_layer=None, bias=True, drop=0.0):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "\n",
        "        self.fc1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, bias=bias)\n",
        "        self.norm = norm_layer(hidden_features) if norm_layer else nn.Identity()\n",
        "        self.act = act_layer()\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.fc2 = nn.Conv2d(hidden_features, out_features, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9UO_gyEHGOS"
      },
      "source": [
        "## ParNeXt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flosCZ24HtDW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
        "        super(BasicConv, self).__init__()\n",
        "        self.out_channels = out_planes\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
        "        self.relu = nn.ReLU() if relu else None\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class ChannelGate(nn.Module):\n",
        "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
        "        super(ChannelGate, self).__init__()\n",
        "        self.gate_channels = gate_channels\n",
        "        self.mlp = nn.Sequential(\n",
        "            Flatten(),\n",
        "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
        "            )\n",
        "        self.pool_types = pool_types\n",
        "\n",
        "    def forward(self, x):\n",
        "        channel_att_sum = None\n",
        "        for pool_type in self.pool_types:\n",
        "            if pool_type=='avg':\n",
        "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( avg_pool )\n",
        "            elif pool_type=='max':\n",
        "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( max_pool )\n",
        "            elif pool_type=='lp':\n",
        "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
        "                channel_att_raw = self.mlp( lp_pool )\n",
        "            elif pool_type=='lse':\n",
        "                # LSE pool only\n",
        "                lse_pool = logsumexp_2d(x)\n",
        "                channel_att_raw = self.mlp( lse_pool )\n",
        "\n",
        "            if channel_att_sum is None:\n",
        "                channel_att_sum = channel_att_raw\n",
        "            else:\n",
        "                channel_att_sum = channel_att_sum + channel_att_raw\n",
        "\n",
        "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
        "        return x * scale\n",
        "\n",
        "def logsumexp_2d(tensor):\n",
        "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
        "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
        "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
        "    return outputs\n",
        "\n",
        "class ChannelPool(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
        "\n",
        "class SpatialGate(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialGate, self).__init__()\n",
        "        kernel_size = 7\n",
        "        self.compress = ChannelPool()\n",
        "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
        "    def forward(self, x):\n",
        "        x_compress = self.compress(x)\n",
        "        x_out = self.spatial(x_compress)\n",
        "        scale = F.sigmoid(x_out) # broadcasting\n",
        "        return x * scale\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    default_act = nn.SiLU()\n",
        "    def __init__(self, c1, k=1, s=1, p=None, g=1, d=1, act=True):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.ChannelGate = ChannelGate(c1, 16, ['avg', 'max'])\n",
        "        self.conv = nn.Conv2d(c1, c1, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(c1)\n",
        "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
        "        self.SpatialGate = SpatialGate()\n",
        "    def forward(self, x):\n",
        "        x_out = self.ChannelGate(x)\n",
        "        x_out = self.SpatialGate(x_out)\n",
        "        x_conv_out = self.act(self.bn(self.conv(x)))\n",
        "        return self.conv(x_out)+x_conv_out\n",
        "\n",
        "class ParNeXtBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels,k=1, g=1, s=1, p=None,d=1, act=True):\n",
        "        super(ParNeXtBlock, self).__init__()\n",
        "        reduction_ratio=16\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=k, stride=1, bias=False)\n",
        "        self.norm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=k, stride=stride, padding=1, groups=groups, bias=False)\n",
        "        self.norm2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=k, stride=1, bias=False)\n",
        "        self.norm3 = nn.BatchNorm2d(out_channels)\n",
        "        self.cbam = CBAM(out_channels, reduction_ratio)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        ) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.downsample(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm3(x)\n",
        "        x = self.cbam(x)\n",
        "        x += residual\n",
        "        x = self.relu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gehyj6bO_6mx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYoNehwo_kMB"
      },
      "source": [
        "# Backbones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpobFXcX_xqf"
      },
      "source": [
        "## Vanilla Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7zvVFXTI_2M0"
      },
      "outputs": [],
      "source": [
        "class Backbone(nn.Module):\n",
        "    def __init__(self, base_channels, base_depth, deep_mul, phi, pretrained=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.stem = Conv(3, base_channels, 3, 2)\n",
        "\n",
        "        self.dark2 = nn.Sequential(\n",
        "            Conv(base_channels, base_channels * 2, 3, 2),\n",
        "            C2f(base_channels * 2, base_channels * 2, base_depth, True),\n",
        "        )\n",
        "        self.dark3 = nn.Sequential(\n",
        "            Conv(base_channels * 2, base_channels * 4, 3, 2),\n",
        "            C2f(base_channels * 4, base_channels * 4, base_depth * 2, True),\n",
        "        )\n",
        "        self.dark4 = nn.Sequential(\n",
        "            Conv(base_channels * 4, base_channels * 8, 3, 2),\n",
        "            C2f(base_channels * 8, base_channels * 8, base_depth * 2, True),\n",
        "        )\n",
        "        self.dark5 = nn.Sequential(\n",
        "            Conv(base_channels * 8, int(base_channels * 16 * deep_mul), 3, 2),\n",
        "            C2f(int(base_channels * 16 * deep_mul), int(base_channels * 16 * deep_mul), base_depth, True),\n",
        "            SPPF(int(base_channels * 16 * deep_mul), int(base_channels * 16 * deep_mul), k=5)\n",
        "        )\n",
        "\n",
        "        if pretrained:\n",
        "            url = {\n",
        "                \"n\" : 'https://github.com/bubbliiiing/yolov8-pytorch/releases/download/v1.0/yolov8_n_backbone_weights.pth',\n",
        "                \"s\" : 'https://github.com/bubbliiiing/yolov8-pytorch/releases/download/v1.0/yolov8_s_backbone_weights.pth',\n",
        "                \"m\" : 'https://github.com/bubbliiiing/yolov8-pytorch/releases/download/v1.0/yolov8_m_backbone_weights.pth',\n",
        "                \"l\" : 'https://github.com/bubbliiiing/yolov8-pytorch/releases/download/v1.0/yolov8_l_backbone_weights.pth',\n",
        "                \"x\" : 'https://github.com/bubbliiiing/yolov8-pytorch/releases/download/v1.0/yolov8_x_backbone_weights.pth',\n",
        "            }[phi]\n",
        "            checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", model_dir=\"./model_data\")\n",
        "            self.load_state_dict(checkpoint, strict=False)\n",
        "            print(\"Load weights from \" + url.split('/')[-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.dark2(x)\n",
        "        x = self.dark3(x)\n",
        "        feat1 = x\n",
        "        x = self.dark4(x)\n",
        "        feat2 = x\n",
        "        x = self.dark5(x)\n",
        "        feat3 = x\n",
        "        return feat1, feat2, feat3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SRd2SEr_l0y"
      },
      "source": [
        "## ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3cwswhdAxPD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import math\n",
        "\n",
        "# Define the ResNet block components\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "# Basic Block for ResNet\n",
        "class BasicBlock1(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock1, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Bottleneck Block for ResNet\n",
        "class Bottleneck1(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck1, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdKM-D1dA3JH"
      },
      "source": [
        "### ResNet50 Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJzvogaIA2EM",
        "outputId": "9cd06805-d11d-4665-908d-aec5db75efca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80 3 0.5 torch.Size([1, 320, 80, 80]) torch.Size([1, 640, 40, 40]) torch.Size([1, 640, 20, 20])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class ResNet_Backbone(nn.Module):\n",
        "    def __init__(self, base_channels, base_depth, deep_mul, pretrained=False):\n",
        "        super(ResNet_Backbone, self).__init__()\n",
        "        self.inplanes = base_channels\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "        block = Bottleneck1\n",
        "\n",
        "        # Define layers, dynamically adjusting channels and depth\n",
        "        self.conv1 = nn.Conv2d(3, base_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(base_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Layer1 (Stage 1)\n",
        "        self.layer1 = self._make_layer(block, base_channels * 4, int(3 * base_depth))\n",
        "        # Layer2 (Stage 2)\n",
        "        self.layer2 = self._make_layer(block, base_channels * 8, int(4 * base_depth), stride=2)\n",
        "        # Layer3 (Stage 3)\n",
        "        self.layer3 = self._make_layer(block, base_channels * 16, int(6 * base_depth), stride=2)\n",
        "        # Layer4 (Stage 4)\n",
        "        self.layer4 = self._make_layer(block, int(base_channels * 32 * deep_mul), int(3 * base_depth), stride=2)\n",
        "\n",
        "        # Reduction layers to match FPN expected outputs\n",
        "        self.conv3_reduction = nn.Conv2d(base_channels * 8, base_channels * 4, kernel_size=1)\n",
        "        self.conv4_reduction = nn.Conv2d(base_channels * 16, base_channels * 8, kernel_size=1)\n",
        "        self.conv5_reduction = nn.Conv2d(int(base_channels * 32 * deep_mul), int(base_channels * 16 * deep_mul), kernel_size=1)\n",
        "\n",
        "        # Initialize weights if not pretrained\n",
        "        if not pretrained:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes, stride),\n",
        "                norm_layer(planes),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes // block.expansion, stride, downsample))\n",
        "        self.inplanes = planes\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes // block.expansion))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Stem\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        feat1 = self.relu(x)\n",
        "\n",
        "        # Max Pool and Residual Blocks\n",
        "        x = self.maxpool(feat1)\n",
        "        feat2 = self.layer1(x)\n",
        "        feat3 = self.layer2(feat2)\n",
        "        feat4 = self.layer3(feat3)\n",
        "        feat5 = self.layer4(feat4)\n",
        "\n",
        "        feat3_reduced = self.conv3_reduction(feat3)\n",
        "        feat4_reduced = self.conv4_reduction(feat4)\n",
        "        feat5_reduced = self.conv5_reduction(feat5)\n",
        "\n",
        "        return feat3_reduced, feat4_reduced, feat5_reduced\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "phi='x'\n",
        "depth_dict          = {'n' : 0.33, 's' : 0.33, 'm' : 0.67, 'l' : 1.00, 'x' : 1.00,}\n",
        "width_dict          = {'n' : 0.25, 's' : 0.50, 'm' : 0.75, 'l' : 1.00, 'x' : 1.25,}\n",
        "deep_width_dict     = {'n' : 1.00, 's' : 1.00, 'm' : 0.75, 'l' : 0.50, 'x' : 0.50,}\n",
        "dep_mul, wid_mul, deep_mul = depth_dict[phi], width_dict[phi], deep_width_dict[phi]\n",
        "\n",
        "base_channels       = int(wid_mul * 64)  # 64\n",
        "base_depth          = max(round(dep_mul * 3), 1)\n",
        "model = ResNet_Backbone(base_channels, base_depth, deep_mul)\n",
        "\n",
        "a = torch.randn(1, 3, 640, 640)\n",
        "a,b,c = model(a)\n",
        "print(base_channels,base_depth,deep_mul,a.shape, b.shape, c.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It7UDTsn_n3n"
      },
      "source": [
        "## NextViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hSf9_UOA7IF"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from einops import rearrange\n",
        "from timm.models.layers import DropPath, trunc_normal_\n",
        "from torch import nn\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "\n",
        "NORM_EPS = 1e-5\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "def merge_pre_bn(module, pre_bn_1, pre_bn_2=None):\n",
        "    \"\"\" Merge pre BN to reduce inference runtime.\n",
        "    \"\"\"\n",
        "    weight = module.weight.data\n",
        "    if module.bias is None:\n",
        "        zeros = torch.zeros(module.out_channels, device=weight.device).type(weight.type())\n",
        "        module.bias = nn.Parameter(zeros)\n",
        "    bias = module.bias.data\n",
        "    if pre_bn_2 is None:\n",
        "        assert pre_bn_1.track_running_stats is True, \"Unsupport bn_module.track_running_stats is False\"\n",
        "        assert pre_bn_1.affine is True, \"Unsupport bn_module.affine is False\"\n",
        "\n",
        "        scale_invstd = pre_bn_1.running_var.add(pre_bn_1.eps).pow(-0.5)\n",
        "        extra_weight = scale_invstd * pre_bn_1.weight\n",
        "        extra_bias = pre_bn_1.bias - pre_bn_1.weight * pre_bn_1.running_mean * scale_invstd\n",
        "    else:\n",
        "        assert pre_bn_1.track_running_stats is True, \"Unsupport bn_module.track_running_stats is False\"\n",
        "        assert pre_bn_1.affine is True, \"Unsupport bn_module.affine is False\"\n",
        "\n",
        "        assert pre_bn_2.track_running_stats is True, \"Unsupport bn_module.track_running_stats is False\"\n",
        "        assert pre_bn_2.affine is True, \"Unsupport bn_module.affine is False\"\n",
        "\n",
        "        scale_invstd_1 = pre_bn_1.running_var.add(pre_bn_1.eps).pow(-0.5)\n",
        "        scale_invstd_2 = pre_bn_2.running_var.add(pre_bn_2.eps).pow(-0.5)\n",
        "\n",
        "        extra_weight = scale_invstd_1 * pre_bn_1.weight * scale_invstd_2 * pre_bn_2.weight\n",
        "        extra_bias = scale_invstd_2 * pre_bn_2.weight *(pre_bn_1.bias - pre_bn_1.weight * pre_bn_1.running_mean * scale_invstd_1 - pre_bn_2.running_mean) + pre_bn_2.bias\n",
        "\n",
        "    if isinstance(module, nn.Linear):\n",
        "        extra_bias = weight @ extra_bias\n",
        "        weight.mul_(extra_weight.view(1, weight.size(1)).expand_as(weight))\n",
        "    elif isinstance(module, nn.Conv2d):\n",
        "        assert weight.shape[2] == 1 and weight.shape[3] == 1\n",
        "        weight = weight.reshape(weight.shape[0], weight.shape[1])\n",
        "        extra_bias = weight @ extra_bias\n",
        "        weight.mul_(extra_weight.view(1, weight.size(1)).expand_as(weight))\n",
        "        weight = weight.reshape(weight.shape[0], weight.shape[1], 1, 1)\n",
        "    bias.add_(extra_bias)\n",
        "\n",
        "    module.weight.data = weight\n",
        "    module.bias.data = bias\n",
        "class ConvBNReLU(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            groups=1):\n",
        "        super(ConvBNReLU, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                              padding=1, groups=groups, bias=False)\n",
        "        self.norm = nn.BatchNorm2d(out_channels, eps=NORM_EPS)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class PatchEmbed1(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 stride=1):\n",
        "        super(PatchEmbed1, self).__init__()\n",
        "        norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n",
        "        if stride == 2:\n",
        "            self.avgpool = nn.AvgPool2d((2, 2), stride=2, ceil_mode=True, count_include_pad=False)\n",
        "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "            self.norm = norm_layer(out_channels)\n",
        "        elif in_channels != out_channels:\n",
        "            self.avgpool = nn.Identity()\n",
        "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "            self.norm = norm_layer(out_channels)\n",
        "        else:\n",
        "            self.avgpool = nn.Identity()\n",
        "            self.conv = nn.Identity()\n",
        "            self.norm = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.norm(self.conv(self.avgpool(x)))\n",
        "\n",
        "\n",
        "class MHCA(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Convolutional Attention\n",
        "    \"\"\"\n",
        "    def __init__(self, out_channels, head_dim):\n",
        "        super(MHCA, self).__init__()\n",
        "        norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n",
        "        self.group_conv3x3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1,\n",
        "                                       padding=1, groups=out_channels // head_dim, bias=False)\n",
        "        self.norm = norm_layer(out_channels)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "        self.projection = nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.group_conv3x3(x)\n",
        "        out = self.norm(out)\n",
        "        out = self.act(out)\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Mlp2(nn.Module):\n",
        "    def __init__(self, in_features, out_features=None, mlp_ratio=None, drop=0., bias=True):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_dim = _make_divisible(in_features * mlp_ratio, 32)\n",
        "        self.conv1 = nn.Conv2d(in_features, hidden_dim, kernel_size=1, bias=bias)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(hidden_dim, out_features, kernel_size=1, bias=bias)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def merge_bn(self, pre_norm):\n",
        "        merge_pre_bn(self.conv1, pre_norm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class NCB(nn.Module):\n",
        "    \"\"\"\n",
        "    Next Convolution Block\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1, path_dropout=0,\n",
        "                 drop=0, head_dim=32, mlp_ratio=3):\n",
        "        super(NCB, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n",
        "        assert out_channels % head_dim == 0\n",
        "\n",
        "        self.patch_embed = PatchEmbed1(in_channels, out_channels, stride)\n",
        "        self.mhca = MHCA(out_channels, head_dim)\n",
        "        self.attention_path_dropout = DropPath(path_dropout)\n",
        "\n",
        "        self.norm = norm_layer(out_channels)\n",
        "        self.mlp = Mlp2(out_channels, mlp_ratio=mlp_ratio, drop=drop, bias=True)\n",
        "        self.mlp_path_dropout = DropPath(path_dropout)\n",
        "        self.is_bn_merged = False\n",
        "\n",
        "    def merge_bn(self):\n",
        "        if not self.is_bn_merged:\n",
        "            self.mlp.merge_bn(self.norm)\n",
        "            self.is_bn_merged = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x + self.attention_path_dropout(self.mhca(x))\n",
        "        if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n",
        "            out = self.norm(x)\n",
        "        else:\n",
        "            out = x\n",
        "        x = x + self.mlp_path_dropout(self.mlp(out))\n",
        "        return x\n",
        "\n",
        "\n",
        "class E_MHSA(nn.Module):\n",
        "    \"\"\"\n",
        "    Efficient Multi-Head Self Attention\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, out_dim=None, head_dim=32, qkv_bias=True, qk_scale=None,\n",
        "                 attn_drop=0, proj_drop=0., sr_ratio=1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.out_dim = out_dim if out_dim is not None else dim\n",
        "        self.num_heads = self.dim // head_dim\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        self.q = nn.Linear(dim, self.dim, bias=qkv_bias)\n",
        "        self.k = nn.Linear(dim, self.dim, bias=qkv_bias)\n",
        "        self.v = nn.Linear(dim, self.dim, bias=qkv_bias)\n",
        "        self.proj = nn.Linear(self.dim, self.out_dim)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        self.sr_ratio = sr_ratio\n",
        "        self.N_ratio = sr_ratio ** 2\n",
        "        if sr_ratio > 1:\n",
        "            self.sr = nn.AvgPool1d(kernel_size=self.N_ratio, stride=self.N_ratio)\n",
        "            self.norm = nn.BatchNorm1d(dim, eps=NORM_EPS)\n",
        "        self.is_bn_merged = False\n",
        "\n",
        "    def merge_bn(self, pre_bn):\n",
        "        merge_pre_bn(self.q, pre_bn)\n",
        "        if self.sr_ratio > 1:\n",
        "            merge_pre_bn(self.k, pre_bn, self.norm)\n",
        "            merge_pre_bn(self.v, pre_bn, self.norm)\n",
        "        else:\n",
        "            merge_pre_bn(self.k, pre_bn)\n",
        "            merge_pre_bn(self.v, pre_bn)\n",
        "        self.is_bn_merged = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        q = self.q(x)\n",
        "        q = q.reshape(B, N, self.num_heads, int(C // self.num_heads)).permute(0, 2, 1, 3)\n",
        "\n",
        "        if self.sr_ratio > 1:\n",
        "            x_ = x.transpose(1, 2)\n",
        "            x_ = self.sr(x_)\n",
        "            if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n",
        "                x_ = self.norm(x_)\n",
        "            x_ = x_.transpose(1, 2)\n",
        "            k = self.k(x_)\n",
        "            k = k.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 3, 1)\n",
        "            v = self.v(x_)\n",
        "            v = v.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 1, 3)\n",
        "        else:\n",
        "            k = self.k(x)\n",
        "            k = k.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 3, 1)\n",
        "            v = self.v(x)\n",
        "            v = v.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 1, 3)\n",
        "        attn = (q @ k) * self.scale\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class NTB(nn.Module):\n",
        "    \"\"\"\n",
        "    Next Transformer Block\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self, in_channels, out_channels, path_dropout=0.1, stride=1, sr_ratio=1,\n",
        "            mlp_ratio=2, head_dim=32, mix_block_ratio=0.75, attn_drop=0, drop=0,\n",
        "    ):\n",
        "        super(NTB, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.mix_block_ratio = mix_block_ratio\n",
        "        norm_func = partial(nn.BatchNorm2d, eps=NORM_EPS)\n",
        "\n",
        "        self.mhsa_out_channels = _make_divisible(int(out_channels * mix_block_ratio), 32)\n",
        "        self.mhca_out_channels = out_channels\n",
        "        # - self.mhsa_out_channels\n",
        "\n",
        "        self.patch_embed = PatchEmbed1(in_channels, self.mhsa_out_channels, stride)\n",
        "        self.norm1 = norm_func(self.mhsa_out_channels)\n",
        "        self.e_mhsa = E_MHSA(self.mhsa_out_channels, head_dim=head_dim, sr_ratio=sr_ratio,\n",
        "                             attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.mhsa_path_dropout = DropPath(path_dropout * mix_block_ratio)\n",
        "\n",
        "        self.projection = PatchEmbed1(self.mhsa_out_channels, self.mhca_out_channels, stride=1)\n",
        "        self.mhca = MHCA(self.mhca_out_channels, head_dim=head_dim)\n",
        "        self.mhca_path_dropout = DropPath(path_dropout * (1 - mix_block_ratio))\n",
        "\n",
        "        self.norm2 = norm_func(out_channels)\n",
        "        self.mlp = Mlp2(out_channels, mlp_ratio=mlp_ratio, drop=drop)\n",
        "        self.mlp_path_dropout = DropPath(path_dropout)\n",
        "\n",
        "        self.is_bn_merged = False\n",
        "\n",
        "    def merge_bn(self):\n",
        "        if not self.is_bn_merged:\n",
        "            self.e_mhsa.merge_bn(self.norm1)\n",
        "            self.mlp.merge_bn(self.norm2)\n",
        "            self.is_bn_merged = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        B, C, H, W = x.shape\n",
        "        if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n",
        "            out = self.norm1(x)\n",
        "        else:\n",
        "            out = x\n",
        "        out = rearrange(out, \"b c h w -> b (h w) c\")  # b n c\n",
        "        out = self.mhsa_path_dropout(self.e_mhsa(out))\n",
        "        x = x + rearrange(out, \"b (h w) c -> b c h w\", h=H)\n",
        "\n",
        "        out = self.projection(x)\n",
        "        out = out + self.mhca_path_dropout(self.mhca(out))\n",
        "        x = torch.cat([x, out], dim=1)\n",
        "\n",
        "        if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n",
        "            out = self.norm2(x)\n",
        "        else:\n",
        "            out = x\n",
        "        x = x + self.mlp_path_dropout(self.mlp(out))\n",
        "        return x\n",
        "\n",
        "\n",
        "class NextViT_Model(nn.Module):\n",
        "    def __init__(self, stem_chs, depths, path_dropout, attn_drop=0, drop=0, num_classes=1000,\n",
        "                 strides=[1, 2, 2, 2], sr_ratios=[8, 4, 2, 1], head_dim=32, mix_block_ratio=0.75,\n",
        "                 use_checkpoint=False, resume='', with_extra_norm=True, frozen_stages=-1,\n",
        "                 norm_eval=False, norm_cfg=None,):\n",
        "        super(NextViT_Model, self).__init__()\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.frozen_stages = frozen_stages\n",
        "        self.with_extra_norm = with_extra_norm\n",
        "        self.norm_eval = norm_eval\n",
        "        self.stage_out_channels = [[96] * (depths[0]),\n",
        "                                   [192] * (depths[1] - 1) + [256],\n",
        "                                   [384, 384, 384, 384, 512] * (depths[2] // 5),\n",
        "                                   [768] * (depths[3] - 1) + [1024]]\n",
        "\n",
        "        # Next Hybrid Strategy\n",
        "        self.stage_block_types = [[NCB] * depths[0],\n",
        "                                  [NCB] * (depths[1] - 1) + [NTB],\n",
        "                                  [NCB, NCB, NCB, NCB, NTB] * (depths[2] // 5),\n",
        "                                  [NCB] * (depths[3] - 1) + [NTB]]\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            ConvBNReLU(3, stem_chs[0], kernel_size=3, stride=2),\n",
        "            ConvBNReLU(stem_chs[0], stem_chs[1], kernel_size=3, stride=1),\n",
        "            ConvBNReLU(stem_chs[1], stem_chs[2], kernel_size=3, stride=1),\n",
        "            ConvBNReLU(stem_chs[2], stem_chs[2], kernel_size=3, stride=2),\n",
        "        )\n",
        "        input_channel = stem_chs[-1]\n",
        "        features = []\n",
        "        idx = 0\n",
        "        dpr = [x.item() for x in torch.linspace(0, path_dropout, sum(depths))]  # stochastic depth decay rule\n",
        "        for stage_id in range(len(depths)):\n",
        "            numrepeat = depths[stage_id]\n",
        "            output_channels = self.stage_out_channels[stage_id]\n",
        "            block_types = self.stage_block_types[stage_id]\n",
        "            for block_id in range(numrepeat):\n",
        "                if strides[stage_id] == 2 and block_id == 0:\n",
        "                    stride = 2\n",
        "                else:\n",
        "                    stride = 1\n",
        "                output_channel = output_channels[block_id]\n",
        "                block_type = block_types[block_id]\n",
        "                if block_type is NCB:\n",
        "                    layer = NCB(input_channel, output_channel, stride=stride, path_dropout=dpr[idx + block_id],\n",
        "                                drop=drop, head_dim=head_dim)\n",
        "                    features.append(layer)\n",
        "                elif block_type is NTB:\n",
        "                    layer = NTB(input_channel, output_channel, path_dropout=dpr[idx + block_id], stride=stride,\n",
        "                                sr_ratio=sr_ratios[stage_id], head_dim=head_dim, mix_block_ratio=mix_block_ratio,\n",
        "                                attn_drop=attn_drop, drop=drop)\n",
        "                    features.append(layer)\n",
        "                input_channel = output_channel\n",
        "            idx += numrepeat\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        self.extra_norm_list = None\n",
        "        if with_extra_norm:\n",
        "            self.extra_norm_list = []\n",
        "            for stage_id in range(len(self.stage_out_channels)):\n",
        "                self.extra_norm_list.append(nn.BatchNorm2d(\n",
        "                    self.stage_out_channels[stage_id][-1], eps=NORM_EPS))\n",
        "            self.extra_norm_list = nn.Sequential(*self.extra_norm_list)\n",
        "\n",
        "        self.norm = nn.BatchNorm2d(output_channel, eps=NORM_EPS)\n",
        "        #\n",
        "        # self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        # self.proj_head = nn.Sequential(\n",
        "        #     nn.Linear(output_channel, num_classes),\n",
        "        # )\n",
        "\n",
        "        self.stage_out_idx = [sum(depths[:idx + 1]) - 1 for idx in range(len(depths))]\n",
        "        self._initialize_weights()\n",
        "        if resume:\n",
        "            self.init_weights(resume)\n",
        "        if norm_cfg is not None:\n",
        "            self = torch.nn.SyncBatchNorm.convert_sync_batchnorm(self)\n",
        "        self._freeze_stages()\n",
        "\n",
        "    def _freeze_stages(self):\n",
        "        if self.frozen_stages > 0:\n",
        "            self.stem.eval()\n",
        "            for param in self.stem.parameters():\n",
        "                param.requires_grad = False\n",
        "            for idx, layer in enumerate(self.features):\n",
        "                if idx <= self.stage_out_idx[self.frozen_stages - 1]:\n",
        "                    layer.eval()\n",
        "                    for param in layer.parameters():\n",
        "                        param.requires_grad = False\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        \"\"\"Convert the model into training mode while keep normalization layer\n",
        "        freezed.\"\"\"\n",
        "        super(NextViT_Model, self).train(mode)\n",
        "        self._freeze_stages()\n",
        "        if mode and self.norm_eval:\n",
        "            for m in self.modules():\n",
        "                # trick: eval have effect on BatchNorm only\n",
        "                if isinstance(m, _BatchNorm):\n",
        "                    m.eval()\n",
        "\n",
        "    def merge_bn(self):\n",
        "        self.eval()\n",
        "        for idx, module in self.named_modules():\n",
        "            if isinstance(module, NCB) or isinstance(module, NTB):\n",
        "                module.merge_bn()\n",
        "\n",
        "    def init_weights(self, pretrained=None):\n",
        "        if isinstance(pretrained, str):\n",
        "            print('\\n using pretrained model\\n')\n",
        "            checkpoint = torch.load(pretrained, map_location='cpu')['model']\n",
        "            self.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for n, m in self.named_modules():\n",
        "            if isinstance(m, (nn.BatchNorm2d, nn.GroupNorm, nn.LayerNorm, nn.BatchNorm1d)):\n",
        "                nn.init.constant_(m.weight, 1.0)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                trunc_normal_(m.weight, std=.02)\n",
        "                if hasattr(m, 'bias') and m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Conv2d):\n",
        "                trunc_normal_(m.weight, std=.02)\n",
        "                if hasattr(m, 'bias') and m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = list()\n",
        "        x = self.stem(x)\n",
        "        stage_id = 0\n",
        "        for idx, layer in enumerate(self.features):\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(layer, x)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "            if idx == self.stage_out_idx[stage_id]:\n",
        "                if self.with_extra_norm:\n",
        "                    if stage_id < 3:\n",
        "                        x = self.extra_norm_list[stage_id](x)\n",
        "                    else:\n",
        "                        x = self.norm(x)\n",
        "                outputs.append(x)\n",
        "                stage_id += 1\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class nextvit_small(NextViT_Model):\n",
        "    def __init__(self, resume='', **kwargs):\n",
        "        super(nextvit_small, self).__init__(\n",
        "            stem_chs=[64, 32, 64], depths=[3, 4, 10, 3], path_dropout=0.1, resume=resume, **kwargs\n",
        "        )\n",
        "\n",
        "\n",
        "class nextvit_base(NextViT_Model):\n",
        "    def __init__(self, resume='', **kwargs):\n",
        "        super(nextvit_base, self).__init__(\n",
        "            stem_chs=[64, 32, 64], depths=[3, 4, 20, 3], path_dropout=0.2, resume=resume, **kwargs\n",
        "        )\n",
        "\n",
        "\n",
        "class nextvit_large(NextViT_Model):\n",
        "    def __init__(self, resume='', **kwargs):\n",
        "        super(nextvit_large, self).__init__(\n",
        "            stem_chs=[64, 32, 64], depths=[3, 4, 30, 3], path_dropout=0.2, resume=resume, **kwargs\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def NextViT(model_size:str, base_channels, base_depth, deep_mul, input_shape=(640,640,3), num_classes=80):\n",
        "    if model_size=='small':\n",
        "        model = NextViT_Backbone(nextvit_small(), base_channels, base_depth, deep_mul)\n",
        "    elif model_size=='base':\n",
        "        model = NextViT_Backbone(nextvit_base(), base_channels, base_depth, deep_mul)\n",
        "    elif model_size=='large':\n",
        "        model = NextViT_Backbone(nextvit_large(), base_channels, base_depth, deep_mul)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model size: {model_size}. Choose 'small', 'base', or 'large'.\")\n",
        "\n",
        "    return model\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxTyCnmhK5cm",
        "outputId": "c246cac2-876d-4cf6-9bfe-31e18b1a7462"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "NTB                                      --\n",
              "PatchEmbed1: 1-1                       --\n",
              "    Identity: 2-1                     --\n",
              "    Conv2d: 2-2                       6,144\n",
              "    BatchNorm2d: 2-3                  192\n",
              "BatchNorm2d: 1-2                       192\n",
              "E_MHSA: 1-3                            --\n",
              "    Linear: 2-4                       9,312\n",
              "    Linear: 2-5                       9,312\n",
              "    Linear: 2-6                       9,312\n",
              "    Linear: 2-7                       9,312\n",
              "    Dropout: 2-8                      --\n",
              "    Dropout: 2-9                      --\n",
              "DropPath: 1-4                          --\n",
              "PatchEmbed1: 1-5                       --\n",
              "    Identity: 2-10                    --\n",
              "    Conv2d: 2-11                      12,288\n",
              "    BatchNorm2d: 2-12                 256\n",
              "MHCA: 1-6                              --\n",
              "    Conv2d: 2-13                      36,864\n",
              "    BatchNorm2d: 2-14                 256\n",
              "    ReLU: 2-15                        --\n",
              "    Conv2d: 2-16                      16,384\n",
              "DropPath: 1-7                          --\n",
              "BatchNorm2d: 1-8                       256\n",
              "Mlp2: 1-9                              --\n",
              "    Conv2d: 2-17                      33,024\n",
              "    ReLU: 2-18                        --\n",
              "    Conv2d: 2-19                      32,896\n",
              "    Dropout: 2-20                     --\n",
              "DropPath: 1-10                         --\n",
              "=================================================================\n",
              "Total params: 176,000\n",
              "Trainable params: 176,000\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "summary(NTB(in_channels=64, out_channels=128,path_dropout=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpiB3dkVHVCV",
        "outputId": "c88c8a48-250c-4182-a476-03b49e7039c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "128 96\n",
            "MHA out: 32\n"
          ]
        }
      ],
      "source": [
        "ntb_layer = NTB(in_channels=64, out_channels=128,path_dropout=0)\n",
        "out = ntb_layer(torch.randn(1, 64, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8_ECM_9HB6j",
        "outputId": "2805fd7a-a4f0-4862-8619-24542ffc7932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 32, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "input_tensor = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "ncb_layer = NCB(in_channels=3, out_channels=32,path_dropout=0)\n",
        "\n",
        "output_tensor = ncb_layer(input_tensor)\n",
        "\n",
        "print(output_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uyLDb95BDTu"
      },
      "source": [
        "### NextViT Backbone:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vkWjFziA_ph"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NextViT_Backbone(nn.Module):\n",
        "    def __init__(self, nextvit_model, base_channels, base_depth, deep_mul):\n",
        "        super(NextViT_Backbone, self).__init__()\n",
        "        self.model = nextvit_model\n",
        "\n",
        "        # Dynamically set the reduction layers based on the actual input size from the model\n",
        "        self.conv3_reduction = nn.Conv2d(0, base_channels * 4, kernel_size=1)  # Will set actual input size after first forward pass\n",
        "        self.conv4_reduction = nn.Conv2d(0, base_channels * 8, kernel_size=1)  # Will set actual input size after first forward pass\n",
        "        self.conv5_reduction = nn.Conv2d(0, int(base_channels * 16 * deep_mul), kernel_size=1)  # Will set actual input size after first forward pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.model(x)\n",
        "\n",
        "        feat3 = outputs[1]  # Low-level feature map\n",
        "        feat4 = outputs[2]  # Medium-level feature map\n",
        "        feat5 = outputs[3]  # High-level feature map\n",
        "\n",
        "        # Dynamically adjust the input channels based on the feature maps output size\n",
        "        if self.conv3_reduction.in_channels == 0:\n",
        "            self.conv3_reduction = nn.Conv2d(feat3.shape[1], self.conv3_reduction.out_channels, kernel_size=1).to(feat3.device)\n",
        "        if self.conv4_reduction.in_channels == 0:\n",
        "            self.conv4_reduction = nn.Conv2d(feat4.shape[1], self.conv4_reduction.out_channels, kernel_size=1).to(feat4.device)\n",
        "        if self.conv5_reduction.in_channels == 0:\n",
        "            self.conv5_reduction = nn.Conv2d(feat5.shape[1], self.conv5_reduction.out_channels, kernel_size=1).to(feat5.device)\n",
        "\n",
        "        # Perform the reduction using the dynamically set layers\n",
        "        feat3 = self.conv3_reduction(feat3)\n",
        "        feat4 = self.conv4_reduction(feat4)\n",
        "        feat5 = self.conv5_reduction(feat5)\n",
        "\n",
        "        return feat3, feat4, feat5  # Return feature maps for FPN\n",
        "\n",
        "\n",
        "# phi='n'\n",
        "# depth_dict          = {'n' : 0.33, 's' : 0.33, 'm' : 0.67, 'l' : 1.00, 'x' : 1.00,}\n",
        "# width_dict          = {'n' : 0.25, 's' : 0.50, 'm' : 0.75, 'l' : 1.00, 'x' : 1.25,}\n",
        "# deep_width_dict     = {'n' : 1.00, 's' : 1.00, 'm' : 0.75, 'l' : 0.50, 'x' : 0.50,}\n",
        "# dep_mul, wid_mul, deep_mul = depth_dict[phi], width_dict[phi], deep_width_dict[phi]\n",
        "\n",
        "# base_channels       = int(wid_mul * 64)  # 64\n",
        "# base_depth          = max(round(dep_mul * 3), 1)\n",
        "# model = NextViT('small',base_channels, base_depth, deep_mul)\n",
        "\n",
        "# a = torch.randn(1, 3, 640, 640)\n",
        "# a,b,c = model(a)\n",
        "# print(base_channels,base_depth,deep_mul, a.shape, b.shape, c.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMB-PqKU_pbN"
      },
      "source": [
        "## Vision Mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huLMFUctAl3c"
      },
      "outputs": [],
      "source": [
        "__all__ = [\n",
        "    'vim_tiny_patch16_224', 'vim_small_patch16_224', 'vim_base_patch16_224',\n",
        "    'vim_tiny_patch16_384', 'vim_small_patch16_384', 'vim_base_patch16_384',\n",
        "]\n",
        "\n",
        "\n",
        "class PatchEmbed2(nn.Module):\n",
        "    \"\"\" 2D Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, stride=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = ((img_size[0] - patch_size[0]) // stride + 1, (img_size[1] - patch_size[1]) // stride + 1)\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "        self.flatten = flatten\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride)\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "        x = self.proj(x)\n",
        "        if self.flatten:\n",
        "            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block4(nn.Module):\n",
        "    def __init__(\n",
        "        self, dim, mixer_cls, norm_cls=nn.LayerNorm, fused_add_norm=False, residual_in_fp32=False,drop_path=0.,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection\"\n",
        "\n",
        "        This Block has a slightly different structure compared to a regular\n",
        "        prenorm Transformer block.\n",
        "        The standard block is: LN -> MHA/MLP -> Add.\n",
        "        [Ref: https://arxiv.org/abs/2002.04745]\n",
        "        Here we have: Add -> LN -> Mixer, returning both\n",
        "        the hidden_states (output of the mixer) and the residual.\n",
        "        This is purely for performance reasons, as we can fuse add and LayerNorm.\n",
        "        The residual needs to be provided (except for the very first block).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "        self.fused_add_norm = fused_add_norm\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "        self.mixer = mixer_cls(dim)\n",
        "        self.norm = norm_cls(dim)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        if self.fused_add_norm:\n",
        "            assert RMSNorm is not None, \"RMSNorm import fails\"\n",
        "            assert isinstance(\n",
        "                self.norm, (nn.LayerNorm, RMSNorm)\n",
        "            ), \"Only LayerNorm and RMSNorm are supported for fused_add_norm\"\n",
        "\n",
        "    def forward(\n",
        "        self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None\n",
        "    ):\n",
        "        r\"\"\"Pass the input through the encoder layer.\n",
        "\n",
        "        Args:\n",
        "            hidden_states: the sequence to the encoder layer (required).\n",
        "            residual: hidden_states = Mixer(LN(residual))\n",
        "        \"\"\"\n",
        "        if not self.fused_add_norm:\n",
        "            if residual is None:\n",
        "                residual = hidden_states\n",
        "            else:\n",
        "                residual = residual + self.drop_path(hidden_states)\n",
        "\n",
        "            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n",
        "            if self.residual_in_fp32:\n",
        "                residual = residual.to(torch.float32)\n",
        "        else:\n",
        "            fused_add_norm_fn = rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn\n",
        "            if residual is None:\n",
        "                hidden_states, residual = fused_add_norm_fn(\n",
        "                    hidden_states,\n",
        "                    self.norm.weight,\n",
        "                    self.norm.bias,\n",
        "                    residual=residual,\n",
        "                    prenorm=True,\n",
        "                    residual_in_fp32=self.residual_in_fp32,\n",
        "                    eps=self.norm.eps,\n",
        "                )\n",
        "            else:\n",
        "                hidden_states, residual = fused_add_norm_fn(\n",
        "                    self.drop_path(hidden_states),\n",
        "                    self.norm.weight,\n",
        "                    self.norm.bias,\n",
        "                    residual=residual,\n",
        "                    prenorm=True,\n",
        "                    residual_in_fp32=self.residual_in_fp32,\n",
        "                    eps=self.norm.eps,\n",
        "                )\n",
        "        hidden_states = self.mixer(hidden_states, inference_params=inference_params)\n",
        "        return hidden_states, residual\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
        "\n",
        "\n",
        "def create_block(\n",
        "    d_model,\n",
        "    d_state=16,\n",
        "    ssm_cfg=None,\n",
        "    norm_epsilon=1e-5,\n",
        "    drop_path=0.,\n",
        "    rms_norm=False,\n",
        "    residual_in_fp32=False,\n",
        "    fused_add_norm=False,\n",
        "    layer_idx=None,\n",
        "    device=None,\n",
        "    dtype=None,\n",
        "    # if_bimamba=False,\n",
        "    # bimamba_type=\"none\",\n",
        "    if_divide_out=False,\n",
        "    init_layer_scale=None,\n",
        "):\n",
        "    # if if_bimamba:\n",
        "    #     bimamba_type = \"v1\"\n",
        "    if ssm_cfg is None:\n",
        "        ssm_cfg = {}\n",
        "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "    # import ipdb; ipdb.set_trace()\n",
        "    mixer_cls = partial(Mamba, d_state=d_state, layer_idx=layer_idx,  **ssm_cfg, **factory_kwargs)\n",
        "    norm_cls = partial(\n",
        "        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs\n",
        "    )\n",
        "    block = Block4(\n",
        "        d_model,\n",
        "        mixer_cls,\n",
        "        norm_cls=norm_cls,\n",
        "        drop_path=drop_path,\n",
        "        fused_add_norm=fused_add_norm,\n",
        "        residual_in_fp32=residual_in_fp32,\n",
        "    )\n",
        "    block.layer_idx = layer_idx\n",
        "    return block\n",
        "\n",
        "\n",
        "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
        "def _init_weights(\n",
        "    module,\n",
        "    n_layer,\n",
        "    initializer_range=0.02,  # Now only used for embedding layer.\n",
        "    rescale_prenorm_residual=True,\n",
        "    n_residuals_per_layer=1,  # Change to 2 if we have MLP\n",
        "):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        if module.bias is not None:\n",
        "            if not getattr(module.bias, \"_no_reinit\", False):\n",
        "                nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        nn.init.normal_(module.weight, std=initializer_range)\n",
        "\n",
        "    if rescale_prenorm_residual:\n",
        "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
        "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
        "        #   > the weights of residual layers at initialization by a factor of 1/N where N is the # of residual layers.\n",
        "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
        "        #\n",
        "        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
        "        for name, p in module.named_parameters():\n",
        "            if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
        "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
        "                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n",
        "                # We need to reinit p since this code could be called multiple times\n",
        "                # Having just p *= scale would repeatedly scale it down\n",
        "                nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
        "                with torch.no_grad():\n",
        "                    p /= math.sqrt(n_residuals_per_layer * n_layer)\n",
        "\n",
        "\n",
        "def segm_init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        trunc_normal_(m.weight, std=0.02)\n",
        "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.Conv2d):\n",
        "        # NOTE conv was left to pytorch default in my original init\n",
        "        lecun_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):\n",
        "        nn.init.zeros_(m.bias)\n",
        "        nn.init.ones_(m.weight)\n",
        "\n",
        "\n",
        "class VisionMamba(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=640,\n",
        "                 patch_size=16,\n",
        "                 stride=16,\n",
        "                 depth=24,\n",
        "                 embed_dim=192,\n",
        "                 d_state=16,\n",
        "                 channels=3,\n",
        "                 num_classes=1000,\n",
        "                 ssm_cfg=None,\n",
        "                 drop_rate=0.,\n",
        "                 drop_path_rate=0.1,\n",
        "                 norm_epsilon: float = 1e-5,\n",
        "                 rms_norm: bool = True,\n",
        "                 initializer_cfg=None,\n",
        "                 fused_add_norm=True,\n",
        "                 residual_in_fp32=True,\n",
        "                 device=None,\n",
        "                 dtype=None,\n",
        "                 ft_seq_len=None,\n",
        "                 pt_hw_seq_len=14,\n",
        "                 if_bidirectional=False,\n",
        "                 final_pool_type='none',\n",
        "                 if_abs_pos_embed=True,\n",
        "                 if_rope=False,\n",
        "                 if_rope_residual=False,\n",
        "                 flip_img_sequences_ratio=-1.,\n",
        "                 if_bimamba=False,\n",
        "                #  bimamba_type=\"v2\",\n",
        "                 if_cls_token=True,\n",
        "                 if_divide_out=True,\n",
        "                 init_layer_scale=None,\n",
        "                 use_double_cls_token=False,\n",
        "                 use_middle_cls_token=True,\n",
        "                 **kwargs):\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        # add factory_kwargs into kwargs\n",
        "        kwargs.update(factory_kwargs)\n",
        "        super().__init__()\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "        self.fused_add_norm = fused_add_norm\n",
        "        self.if_bidirectional = if_bidirectional\n",
        "        self.final_pool_type = final_pool_type\n",
        "        self.if_abs_pos_embed = if_abs_pos_embed\n",
        "        self.if_rope = if_rope\n",
        "        self.if_rope_residual = if_rope_residual\n",
        "        self.flip_img_sequences_ratio = flip_img_sequences_ratio\n",
        "        self.if_cls_token = if_cls_token\n",
        "        self.use_double_cls_token = use_double_cls_token\n",
        "        self.use_middle_cls_token = use_middle_cls_token\n",
        "        self.num_tokens = 1 if if_cls_token else 0\n",
        "\n",
        "        # pretrain parameters\n",
        "        self.num_classes = num_classes\n",
        "        self.d_model = self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "        self.norm_f = nn.LayerNorm(embed_dim)\n",
        "        self.patch_embed = PatchEmbed2(img_size=img_size, patch_size=patch_size, stride=stride, in_chans=channels, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        if if_cls_token:\n",
        "            if use_double_cls_token:\n",
        "                self.cls_token_head = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
        "                self.cls_token_tail = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
        "                self.num_tokens = 2\n",
        "            else:\n",
        "                self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
        "                # self.num_tokens = 1\n",
        "\n",
        "        if if_abs_pos_embed:\n",
        "            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, self.embed_dim))\n",
        "            self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        if if_rope:\n",
        "            half_head_dim = embed_dim // 2\n",
        "            hw_seq_len = img_size // patch_size\n",
        "            self.rope = VisionRotaryEmbeddingFast(\n",
        "                dim=half_head_dim,\n",
        "                pt_seq_len=pt_hw_seq_len,\n",
        "                ft_seq_len=hw_seq_len\n",
        "            )\n",
        "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "\n",
        "        # TODO: release this comment\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # import ipdb;ipdb.set_trace()\n",
        "        inter_dpr = [0.0] + dpr\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
        "                # transformer blocks\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                create_block(\n",
        "                    embed_dim,\n",
        "                    d_state=d_state,\n",
        "                    ssm_cfg=ssm_cfg,\n",
        "                    norm_epsilon=norm_epsilon,\n",
        "                    rms_norm=rms_norm,\n",
        "                    residual_in_fp32=residual_in_fp32,\n",
        "                    fused_add_norm=fused_add_norm,\n",
        "                    layer_idx=i,\n",
        "                    # if_bimamba=if_bimamba,\n",
        "                    # bimamba_type=bimamba_type,\n",
        "                    drop_path=inter_dpr[i],\n",
        "                    if_divide_out=if_divide_out,\n",
        "                    init_layer_scale=init_layer_scale,\n",
        "                    **factory_kwargs,\n",
        "                )\n",
        "                for i in range(depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # output head\n",
        "        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(\n",
        "            embed_dim, eps=norm_epsilon, **factory_kwargs\n",
        "        )\n",
        "\n",
        "        # self.pre_logits = nn.Identity()\n",
        "\n",
        "        # original init\n",
        "        self.patch_embed.apply(segm_init_weights)\n",
        "        self.head.apply(segm_init_weights)\n",
        "        if if_abs_pos_embed:\n",
        "            trunc_normal_(self.pos_embed, std=.02)\n",
        "        if if_cls_token:\n",
        "            if use_double_cls_token:\n",
        "                trunc_normal_(self.cls_token_head, std=.02)\n",
        "                trunc_normal_(self.cls_token_tail, std=.02)\n",
        "            else:\n",
        "                trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "        # mamba init\n",
        "        self.apply(\n",
        "            partial(\n",
        "                _init_weights,\n",
        "                n_layer=depth,\n",
        "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return {\n",
        "            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
        "            for i, layer in enumerate(self.layers)\n",
        "        }\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {\"pos_embed\", \"cls_token\", \"dist_token\", \"cls_token_head\", \"cls_token_tail\"}\n",
        "\n",
        "    @torch.jit.ignore()\n",
        "    def load_pretrained(self, checkpoint_path, prefix=\"\"):\n",
        "        _load_weights(self, checkpoint_path, prefix)\n",
        "\n",
        "\n",
        "    def forward(self, x, return_features=True, inference_params=None, if_random_cls_token_position=False, if_random_token_rank=False):\n",
        "      device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      x = self.patch_embed(x.to(device))\n",
        "      low_feat = None\n",
        "      medium_feat = None\n",
        "      high_feat = None\n",
        "      residual = None\n",
        "      hidden_states = x.to(device)\n",
        "      for i, layer in enumerate(self.layers):\n",
        "          hidden_states, residual = layer(hidden_states, residual)\n",
        "\n",
        "          # Capture 7th layer output as low-level feature\n",
        "          if i == 6:\n",
        "              low_feat = hidden_states\n",
        "\n",
        "          # Capture 16th layer output as medium-level feature\n",
        "          if i == 15:\n",
        "              medium_feat = hidden_states\n",
        "\n",
        "          # Capture 24th (final) layer output as high-level feature\n",
        "          if i == 23:\n",
        "              high_feat = hidden_states\n",
        "      # low_feat = self.norm_f(low_feat)\n",
        "      # medium_feat = self.norm_f(medium_feat)\n",
        "      # high_feat = self.norm_f(high_feat)\n",
        "      return low_feat, medium_feat, high_feat\n",
        "\n",
        "def get_vision_mamba_backbone(model_size, img_size):\n",
        "    # Instantiate the Vision Mamba (ViM) model based on the size (small, base, large, etc.)\n",
        "\n",
        "    if model_size == 'tiny':\n",
        "      embed_dim = 192\n",
        "    elif model_size == 'small':\n",
        "      embed_dim = 384\n",
        "    elif model_size == 'base':\n",
        "      embed_dim = 768\n",
        "    else:\n",
        "      raise ValueError(f\"Unknown model size: {model_size}. Choose 'tiny', 'small' or 'base'.\")\n",
        "    vim_model = VisionMamba(img_size=img_size,patch_size=16, embed_dim=embed_dim, depth=24, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, final_pool_type='mean', if_abs_pos_embed=True, if_rope=False, if_rope_residual=False, if_cls_token=True, if_divide_out=True, use_middle_cls_token=True)\n",
        "\n",
        "    return vim_model, embed_dim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRd2-GzpAo69"
      },
      "source": [
        "### Vision Mamba Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OL-pVpPAqwC"
      },
      "outputs": [],
      "source": [
        "class VisionMambaBackbone(nn.Module):\n",
        "    def __init__(self, vim_model, base_channels,base_depth, deep_mul):\n",
        "        super(VisionMambaBackbone, self).__init__()\n",
        "        self.vim_model, self.embed_dim = vim_model\n",
        "\n",
        "        # Convolutions to map the channels to desired output\n",
        "        self.conv_low = nn.Conv2d(self.embed_dim, base_channels * 4, kernel_size=1)  # 192 -> 64\n",
        "        self.conv_medium = nn.Conv2d(self.embed_dim, base_channels * 8, kernel_size=1)  # 192 -> 128\n",
        "        self.conv_high = nn.Conv2d(self.embed_dim, int(base_channels * 16 * deep_mul), kernel_size=1)  # 192 -> 256\n",
        "\n",
        "        # Adjusting spatial dimensions\n",
        "        self.upsample_low = nn.Upsample(scale_factor=2, mode='nearest')  # 40x40 -> 80x80\n",
        "        self.downsample_high = nn.Conv2d(int(base_channels * 16 * deep_mul), int(base_channels * 16 * deep_mul), kernel_size=3, stride=2, padding=1)  # 40x40 -> 20x20\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features from the Vision Mamba model\n",
        "        features = self.vim_model.forward(x)\n",
        "\n",
        "        # Extract low, medium, and high-level features\n",
        "        low_feat = features[0]  # From 7th layer\n",
        "        medium_feat = features[1]  # From 16th layer\n",
        "        high_feat = features[2]  # From 24th layer\n",
        "\n",
        "        seq_len = int(low_feat.shape[1])\n",
        "        spatial_size = int(seq_len ** 0.5)\n",
        "\n",
        "        # Change the view\n",
        "        low_feat = low_feat.view(low_feat.shape[0], self.embed_dim, spatial_size, spatial_size)\n",
        "        medium_feat = medium_feat.view(medium_feat.shape[0], self.embed_dim, spatial_size, spatial_size)\n",
        "        high_feat = high_feat.view(high_feat.shape[0], self.embed_dim, spatial_size, spatial_size)\n",
        "\n",
        "        # Convolve to match the required number of channels\n",
        "        low_feat = self.conv_low(low_feat)\n",
        "        medium_feat = self.conv_medium(medium_feat)\n",
        "        high_feat = self.conv_high(high_feat)\n",
        "\n",
        "        # Up and down sampling\n",
        "        low_feat = self.upsample_low(low_feat)\n",
        "        high_feat = self.downsample_high(high_feat)\n",
        "\n",
        "        return low_feat, medium_feat, high_feat\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfTGYcA3ymHz"
      },
      "source": [
        "## DEiT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW-3cmuGykfb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "from timm.models.vision_transformer import VisionTransformer, _cfg\n",
        "from timm.models.layers import trunc_normal_\n",
        "\n",
        "\n",
        "\n",
        "class DistilledVisionTransformer(VisionTransformer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n",
        "        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if self.num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.dist_token, std=.02)\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        self.head_dist.apply(self._init_weights)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
        "        dist_token = self.dist_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
        "\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "            print(x.shape)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return x[:, 0], x[:, 1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, x_dist = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        print(\"Head shape :\", x.shape)\n",
        "        x_dist = self.head_dist(x_dist)\n",
        "        print(\"Head Dist shape :\", x_dist.shape)\n",
        "        if self.training:\n",
        "            return x, x_dist\n",
        "        else:\n",
        "            # during inference, return the average of both classifier predictions\n",
        "            return (x + x_dist) / 2\n",
        "\n",
        "\n",
        "def deit_tiny_patch16_224(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        img_size=640,patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = _cfg()\n",
        "    if pretrained:\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\",\n",
        "            map_location=\"cpu\", check_hash=True\n",
        "        )\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_small_patch16_224(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        img_size=640, patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = _cfg()\n",
        "    if pretrained:\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth\",\n",
        "            map_location=\"cpu\", check_hash=True\n",
        "        )\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_base_patch16_224(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        img_size=640, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = _cfg()\n",
        "    if pretrained:\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\",\n",
        "            map_location=\"cpu\", check_hash=True\n",
        "        )\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNJ9TK5AyvHC"
      },
      "source": [
        "### DEiT Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDVM3RiQyxNz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class deitbackbone(nn.Module):\n",
        "    def __init__(self, deit_model,base_channels,base_depth, deep_mul):\n",
        "        super(deitbackbone, self).__init__()\n",
        "        self.deit_model = deit_model\n",
        "\n",
        "        # Define convolution layers to map features to desired channel sizes\n",
        "        self.conv_low = nn.Conv2d(self.deit_model.embed_dim, base_channels * 4, kernel_size=1)  # low-level: 192 -> 64\n",
        "        self.conv_medium = nn.Conv2d(self.deit_model.embed_dim, base_channels * 8, kernel_size=1)  # medium-level: 192 -> 128\n",
        "        self.conv_high = nn.Conv2d(self.deit_model.embed_dim, int(base_channels * 16 * deep_mul), kernel_size=1)  # high-level: 192 -> 256\n",
        "\n",
        "        self.patch_size = self.deit_model.patch_embed.patch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.deit_model.patch_embed(x)\n",
        "        cls_tokens = self.deit_model.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.deit_model.pos_embed[:, :x.size(1), :]\n",
        "        x = self.deit_model.pos_drop(x)\n",
        "\n",
        "        low_feat, medium_feat, high_feat = None, None, None\n",
        "\n",
        "        for i, blk in enumerate(self.deit_model.blocks):\n",
        "            x = blk(x)\n",
        "            if i == 3:\n",
        "                low_feat = x[:, 1:]\n",
        "            if i == 7:\n",
        "                medium_feat = x[:, 1:]\n",
        "            if i == 11:\n",
        "                high_feat = x[:, 1:]\n",
        "        patch_dim = 40  # sqrt(1600) = 40\n",
        "        low_feat = low_feat.view(B, patch_dim, patch_dim, self.deit_model.embed_dim).permute(0, 3, 1, 2)\n",
        "        medium_feat = medium_feat.view(B, patch_dim, patch_dim, self.deit_model.embed_dim).permute(0, 3, 1, 2)\n",
        "        high_feat = high_feat.view(B, patch_dim, patch_dim, self.deit_model.embed_dim).permute(0, 3, 1, 2)\n",
        "        # print(f\"Low feat : {low_feat.shape}\")\n",
        "        # print(f\"Medium feat : {medium_feat.shape}\")\n",
        "        # print(f\"High feat : {high_feat.shape}\")\n",
        "        low_feat = F.interpolate(low_feat, size=(80, 80), mode='bilinear', align_corners=False)\n",
        "        medium_feat = F.interpolate(medium_feat, size=(40, 40), mode='bilinear', align_corners=False)\n",
        "        high_feat = F.interpolate(high_feat, size=(20, 20), mode='bilinear', align_corners=False)\n",
        "        # print(f\"Low feat : {low_feat.shape}\")\n",
        "        # print(f\"Medium feat : {medium_feat.shape}\")\n",
        "        # print(f\"High feat : {high_feat.shape}\")\n",
        "        low_feat = self.conv_low(low_feat)\n",
        "        medium_feat = self.conv_medium(medium_feat)\n",
        "        high_feat = self.conv_high(high_feat)\n",
        "        # print(f\"Low feat : {low_feat.shape}\")\n",
        "        # print(f\"Medium feat : {medium_feat.shape}\")\n",
        "        # print(f\"High feat : {high_feat.shape}\")\n",
        "        return low_feat, medium_feat, high_feat\n",
        "\n",
        "\n",
        "def DEIT_Backbone(model_size,base_channels, base_depth, deep_mul):\n",
        "    if model_size == 'tiny':\n",
        "        deit_model = deit_tiny_patch16_224(pretrained=False)\n",
        "    elif model_size == 'small':\n",
        "        deit_model = deit_small_patch16_224(pretrained=False)\n",
        "    elif model_size == 'base':\n",
        "        deit_model = deit_base_patch16_224(pretrained=False)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model size: {model_size}. Choose 'tiny', 'small' or 'base'.\")\n",
        "    return deitbackbone(deit_model,base_channels, base_depth, deep_mul)\n",
        "\n",
        "\n",
        "# Testing the backbone\n",
        "# phi='x'\n",
        "# depth_dict          = {'n' : 0.33, 's' : 0.33, 'm' : 0.67, 'l' : 1.00, 'x' : 1.00,}\n",
        "# width_dict          = {'n' : 0.25, 's' : 0.50, 'm' : 0.75, 'l' : 1.00, 'x' : 1.25,}\n",
        "# deep_width_dict     = {'n' : 1.00, 's' : 1.00, 'm' : 0.75, 'l' : 0.50, 'x' : 0.50,}\n",
        "# dep_mul, wid_mul, deep_mul = depth_dict[phi], width_dict[phi], deep_width_dict[phi]\n",
        "\n",
        "# base_channels       = int(wid_mul * 64)  # 64\n",
        "# base_depth          = max(round(dep_mul * 3), 1)\n",
        "# model = DEIT_Backbone('tiny',base_channels, base_depth, deep_mul)\n",
        "# a = torch.randn(1, 3, 640, 640)\n",
        "# a,b,c = model(a)\n",
        "# print(base_channels,base_depth,deep_mul, a.shape, b.shape, c.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WopG2Rk_saG"
      },
      "source": [
        "## Mamba Vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3sMGsPU_lcS"
      },
      "outputs": [],
      "source": [
        "def _cfg(url='', **kwargs):\n",
        "    return {'url': url,\n",
        "            'num_classes': 1000,\n",
        "            'input_size': (3, 640, 640),\n",
        "            'pool_size': None,\n",
        "            'crop_pct': 0.875,\n",
        "            'interpolation': 'bicubic',\n",
        "            'fixed_input_size': True,\n",
        "            'mean': (0.485, 0.456, 0.406),\n",
        "            'std': (0.229, 0.224, 0.225),\n",
        "            **kwargs\n",
        "            }\n",
        "\n",
        "\n",
        "default_cfgs = {\n",
        "    'mamba_vision_T': _cfg(url='https://huggingface.co/nvidia/MambaVision-T-1K/resolve/main/mambavision_tiny_1k.pth.tar',\n",
        "                           crop_pct=1.0,\n",
        "                           input_size=(3, 640, 640),\n",
        "                           crop_mode='center'),\n",
        "    'mamba_vision_T2': _cfg(url='https://huggingface.co/nvidia/MambaVision-T2-1K/resolve/main/mambavision_tiny2_1k.pth.tar',\n",
        "                            crop_pct=0.98,\n",
        "                            input_size=(3, 640, 640),\n",
        "                            crop_mode='center'),\n",
        "    'mamba_vision_S': _cfg(url='https://huggingface.co/nvidia/MambaVision-S-1K/resolve/main/mambavision_small_1k.pth.tar',\n",
        "                           crop_pct=0.93,\n",
        "                           input_size=(3, 640, 640),\n",
        "                           crop_mode='center'),\n",
        "    'mamba_vision_B': _cfg(url='https://huggingface.co/nvidia/MambaVision-B-1K/resolve/main/mambavision_base_1k.pth.tar',\n",
        "                           crop_pct=1.0,\n",
        "                           input_size=(3, 640, 640),\n",
        "                           crop_mode='center'),\n",
        "    'mamba_vision_L': _cfg(url='https://huggingface.co/nvidia/MambaVision-L-1K/resolve/main/mambavision_large_1k.pth.tar',\n",
        "                           crop_pct=1.0,\n",
        "                           input_size=(3, 640, 640),\n",
        "                           crop_mode='center'),\n",
        "    'mamba_vision_L2': _cfg(url='https://huggingface.co/nvidia/MambaVision-L2-1K/resolve/main/mambavision_large2_1k.pth.tar',\n",
        "                            crop_pct=1.0,\n",
        "                            input_size=(3, 640, 640),\n",
        "                            crop_mode='center')\n",
        "}\n",
        "\n",
        "\n",
        "__all__ = ['list_models', 'is_model', 'model_entrypoint', 'list_modules', 'is_model_in_modules',\n",
        "           'is_model_default_key', 'has_model_default_key', 'get_model_default_value', 'is_model_pretrained']\n",
        "\n",
        "_module_to_models = defaultdict(set)  # dict of sets to check membership of model in module\n",
        "_model_to_module = {}  # mapping of model names to module names\n",
        "_model_entrypoints = {}  # mapping of model names to entrypoint fns\n",
        "_model_has_pretrained = set()  # set of model names that have pretrained weight url present\n",
        "_model_default_cfgs = dict()  # central repo for model default_cfgs\n",
        "def register_pip_model(fn):\n",
        "    # lookup containing module\n",
        "    mod = sys.modules[fn.__module__]\n",
        "    module_name_split = fn.__module__.split('.')\n",
        "    module_name = module_name_split[-1] if len(module_name_split) else ''\n",
        "\n",
        "    # add model to __all__ in module\n",
        "    model_name = fn.__name__\n",
        "    if hasattr(mod, '__all__'):\n",
        "        mod.__all__.append(model_name)\n",
        "    else:\n",
        "        mod.__all__ = [model_name]\n",
        "\n",
        "    # add entries to registry dict/sets\n",
        "    _model_entrypoints[model_name] = fn\n",
        "    _model_to_module[model_name] = module_name\n",
        "    _module_to_models[module_name].add(model_name)\n",
        "    has_pretrained = False  # check if model has a pretrained url to allow filtering on this\n",
        "    if hasattr(mod, 'default_cfgs') and model_name in mod.default_cfgs:\n",
        "        # this will catch all models that have entrypoint matching cfg key, but miss any aliasing\n",
        "        # entrypoints or non-matching combos\n",
        "        has_pretrained = 'url' in mod.default_cfgs[model_name] and 'http' in mod.default_cfgs[model_name]['url']\n",
        "        _model_default_cfgs[model_name] = deepcopy(mod.default_cfgs[model_name])\n",
        "    if has_pretrained:\n",
        "        _model_has_pretrained.add(model_name)\n",
        "    return fn\n",
        "\n",
        "\n",
        "def window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, C, H, W)\n",
        "        window_size: window size\n",
        "        h_w: Height of window\n",
        "        w_w: Width of window\n",
        "    Returns:\n",
        "        local window features (num_windows*B, window_size*window_size, C)\n",
        "    \"\"\"\n",
        "    B, C, H, W = x.shape\n",
        "    x = x.view(B, C, H // window_size, window_size, W // window_size, window_size)\n",
        "    windows = x.permute(0, 2, 4, 3, 5, 1).reshape(-1, window_size*window_size, C)\n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: local window features (num_windows*B, window_size, window_size, C)\n",
        "        window_size: Window size\n",
        "        H: Height of image\n",
        "        W: Width of image\n",
        "    Returns:\n",
        "        x: (B, C, H, W)\n",
        "    \"\"\"\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.reshape(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 5, 1, 3, 2, 4).reshape(B,windows.shape[2], H, W)\n",
        "    return x\n",
        "\n",
        "\n",
        "def _load_state_dict(module, state_dict, strict=False, logger=None):\n",
        "    \"\"\"Load state_dict to a module.\n",
        "\n",
        "    This method is modified from :meth:`torch.nn.Module.load_state_dict`.\n",
        "    Default value for ``strict`` is set to ``False`` and the message for\n",
        "    param mismatch will be shown even if strict is False.\n",
        "\n",
        "    Args:\n",
        "        module (Module): Module that receives the state_dict.\n",
        "        state_dict (OrderedDict): Weights.\n",
        "        strict (bool): whether to strictly enforce that the keys\n",
        "            in :attr:`state_dict` match the keys returned by this module's\n",
        "            :meth:`~torch.nn.Module.state_dict` function. Default: ``False``.\n",
        "        logger (:obj:`logging.Logger`, optional): Logger to log the error\n",
        "            message. If not specified, print function will be used.\n",
        "    \"\"\"\n",
        "    unexpected_keys = []\n",
        "    all_missing_keys = []\n",
        "    err_msg = []\n",
        "\n",
        "    metadata = getattr(state_dict, '_metadata', None)\n",
        "    state_dict = state_dict.copy()\n",
        "    if metadata is not None:\n",
        "        state_dict._metadata = metadata\n",
        "\n",
        "    def load(module, prefix=''):\n",
        "        local_metadata = {} if metadata is None else metadata.get(\n",
        "            prefix[:-1], {})\n",
        "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
        "                                     all_missing_keys, unexpected_keys,\n",
        "                                     err_msg)\n",
        "        for name, child in module._modules.items():\n",
        "            if child is not None:\n",
        "                load(child, prefix + name + '.')\n",
        "\n",
        "    load(module)\n",
        "    load = None\n",
        "    missing_keys = [\n",
        "        key for key in all_missing_keys if 'num_batches_tracked' not in key\n",
        "    ]\n",
        "\n",
        "    if unexpected_keys:\n",
        "        err_msg.append('unexpected key in source '\n",
        "                       f'state_dict: {\", \".join(unexpected_keys)}\\n')\n",
        "    if missing_keys:\n",
        "        err_msg.append(\n",
        "            f'missing keys in source state_dict: {\", \".join(missing_keys)}\\n')\n",
        "\n",
        "\n",
        "    if len(err_msg) > 0:\n",
        "        err_msg.insert(\n",
        "            0, 'The model and loaded state dict do not match exactly\\n')\n",
        "        err_msg = '\\n'.join(err_msg)\n",
        "        if strict:\n",
        "            raise RuntimeError(err_msg)\n",
        "        elif logger is not None:\n",
        "            logger.warning(err_msg)\n",
        "        else:\n",
        "            print(err_msg)\n",
        "\n",
        "\n",
        "def _load_checkpoint(model,\n",
        "                    filename,\n",
        "                    map_location='cpu',\n",
        "                    strict=False,\n",
        "                    logger=None):\n",
        "    \"\"\"Load checkpoint from a file or URI.\n",
        "\n",
        "    Args:\n",
        "        model (Module): Module to load checkpoint.\n",
        "        filename (str): Accept local filepath, URL, ``torchvision://xxx``,\n",
        "            ``open-mmlab://xxx``. Please refer to ``docs/model_zoo.md`` for\n",
        "            details.\n",
        "        map_location (str): Same as :func:`torch.load`.\n",
        "        strict (bool): Whether to allow different params for the model and\n",
        "            checkpoint.\n",
        "        logger (:mod:`logging.Logger` or None): The logger for error message.\n",
        "\n",
        "    Returns:\n",
        "        dict or OrderedDict: The loaded checkpoint.\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(filename, map_location=map_location)\n",
        "    if not isinstance(checkpoint, dict):\n",
        "        raise RuntimeError(\n",
        "            f'No state_dict found in checkpoint file {filename}')\n",
        "    if 'state_dict' in checkpoint:\n",
        "        state_dict = checkpoint['state_dict']\n",
        "    elif 'model' in checkpoint:\n",
        "        state_dict = checkpoint['model']\n",
        "    else:\n",
        "        state_dict = checkpoint\n",
        "    if list(state_dict.keys())[0].startswith('module.'):\n",
        "        state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
        "\n",
        "    if sorted(list(state_dict.keys()))[0].startswith('encoder'):\n",
        "        state_dict = {k.replace('encoder.', ''): v for k, v in state_dict.items() if k.startswith('encoder.')}\n",
        "\n",
        "    _load_state_dict(model, state_dict, strict, logger)\n",
        "    return checkpoint\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    \"\"\"\n",
        "    Down-sampling block\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim,\n",
        "                 keep_dim=False,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim: feature size dimension.\n",
        "            norm_layer: normalization layer.\n",
        "            keep_dim: bool argument for maintaining the resolution.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        if keep_dim:\n",
        "            dim_out = dim\n",
        "        else:\n",
        "            dim_out = 2 * dim\n",
        "        self.reduction = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim_out, 3, 2, 1, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reduction(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed3(nn.Module):\n",
        "    \"\"\"\n",
        "    Patch embedding block\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_chans=3, in_dim=64, dim=96):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_chans: number of input channels.\n",
        "            dim: feature size dimension.\n",
        "        \"\"\"\n",
        "        # in_dim = 1\n",
        "        super().__init__()\n",
        "        self.proj = nn.Identity()\n",
        "        self.conv_down = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, in_dim, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(in_dim, eps=1e-4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_dim, dim, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(dim, eps=1e-4),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "\n",
        "    def forward(self, x, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
        "        x.to(device)\n",
        "        x = self.proj(x)\n",
        "        x = self.conv_down(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim,\n",
        "                 drop_path=0.,\n",
        "                 layer_scale=None,\n",
        "                 kernel_size=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(dim, dim, kernel_size=kernel_size, stride=1, padding=1)\n",
        "        self.norm1 = nn.BatchNorm2d(dim, eps=1e-5)\n",
        "        self.act1 = nn.GELU(approximate= 'tanh')\n",
        "        self.conv2 = nn.Conv2d(dim, dim, kernel_size=kernel_size, stride=1, padding=1)\n",
        "        self.norm2 = nn.BatchNorm2d(dim, eps=1e-5)\n",
        "        self.layer_scale = layer_scale\n",
        "        if layer_scale is not None and type(layer_scale) in [int, float]:\n",
        "            self.gamma = nn.Parameter(layer_scale * torch.ones(dim))\n",
        "            self.layer_scale = True\n",
        "        else:\n",
        "            self.layer_scale = False\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        if self.layer_scale:\n",
        "            x = x * self.gamma.view(1, -1, 1, 1)\n",
        "        x = input + self.drop_path(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MambaVisionMixer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        d_state=16,\n",
        "        d_conv=4,\n",
        "        expand=2,\n",
        "        dt_rank=\"auto\",\n",
        "        dt_min=0.001,\n",
        "        dt_max=0.1,\n",
        "        dt_init=\"random\",\n",
        "        dt_scale=1.0,\n",
        "        dt_init_floor=1e-4,\n",
        "        conv_bias=True,\n",
        "        bias=False,\n",
        "        use_fast_path=True,\n",
        "        layer_idx=None,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ):\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_conv = d_conv\n",
        "        self.expand = expand\n",
        "        self.d_inner = int(self.expand * self.d_model)\n",
        "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
        "        self.use_fast_path = use_fast_path\n",
        "        self.layer_idx = layer_idx\n",
        "        self.in_proj = nn.Linear(self.d_model, self.d_inner, bias=bias, **factory_kwargs)\n",
        "        self.x_proj = nn.Linear(\n",
        "            self.d_inner//2, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs\n",
        "        )\n",
        "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner//2, bias=True, **factory_kwargs)\n",
        "        dt_init_std = self.dt_rank**-0.5 * dt_scale\n",
        "        if dt_init == \"constant\":\n",
        "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
        "        elif dt_init == \"random\":\n",
        "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        dt = torch.exp(\n",
        "            torch.rand(self.d_inner//2, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
        "            + math.log(dt_min)\n",
        "        ).clamp(min=dt_init_floor)\n",
        "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
        "        with torch.no_grad():\n",
        "            self.dt_proj.bias.copy_(inv_dt)\n",
        "        self.dt_proj.bias._no_reinit = True\n",
        "        A = repeat(\n",
        "            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),\n",
        "            \"n -> d n\",\n",
        "            d=self.d_inner//2,\n",
        "        ).contiguous()\n",
        "        A_log = torch.log(A)\n",
        "        self.A_log = nn.Parameter(A_log)\n",
        "        self.A_log._no_weight_decay = True\n",
        "        self.D = nn.Parameter(torch.ones(self.d_inner//2, device=device))\n",
        "        self.D._no_weight_decay = True\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
        "        self.conv1d_x = nn.Conv1d(\n",
        "            in_channels=self.d_inner//2,\n",
        "            out_channels=self.d_inner//2,\n",
        "            bias=conv_bias//2,\n",
        "            kernel_size=d_conv,\n",
        "            groups=self.d_inner//2,\n",
        "            **factory_kwargs,\n",
        "        )\n",
        "        self.conv1d_z = nn.Conv1d(\n",
        "            in_channels=self.d_inner//2,\n",
        "            out_channels=self.d_inner//2,\n",
        "            bias=conv_bias//2,\n",
        "            kernel_size=d_conv,\n",
        "            groups=self.d_inner//2,\n",
        "            **factory_kwargs,\n",
        "        )\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        \"\"\"\n",
        "        hidden_states: (B, L, D)\n",
        "        Returns: same shape as hidden_states\n",
        "        \"\"\"\n",
        "        _, seqlen, _ = hidden_states.shape\n",
        "        xz = self.in_proj(hidden_states)\n",
        "        xz = rearrange(xz, \"b l d -> b d l\")\n",
        "        x, z = xz.chunk(2, dim=1)\n",
        "        A = -torch.exp(self.A_log.float())\n",
        "        x = F.silu(F.conv1d(input=x, weight=self.conv1d_x.weight, bias=self.conv1d_x.bias, padding='same', groups=self.d_inner//2))\n",
        "        z = F.silu(F.conv1d(input=z, weight=self.conv1d_z.weight, bias=self.conv1d_z.bias, padding='same', groups=self.d_inner//2))\n",
        "        x_dbl = self.x_proj(rearrange(x, \"b d l -> (b l) d\"))\n",
        "        dt, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
        "        dt = rearrange(self.dt_proj(dt), \"(b l) d -> b d l\", l=seqlen)\n",
        "        B = rearrange(B, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n",
        "        C = rearrange(C, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n",
        "        y = selective_scan_fn(x,\n",
        "                              dt,\n",
        "                              A,\n",
        "                              B,\n",
        "                              C,\n",
        "                              self.D.float(),\n",
        "                              z=None,\n",
        "                              delta_bias=self.dt_proj.bias.float(),\n",
        "                              delta_softplus=True,\n",
        "                              return_last_state=None)\n",
        "\n",
        "        y = torch.cat([y, z], dim=1)\n",
        "        y = rearrange(y, \"b d l -> b l d\")\n",
        "        out = self.out_proj(y)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim,\n",
        "            num_heads=8,\n",
        "            qkv_bias=False,\n",
        "            qk_norm=False,\n",
        "            attn_drop=0.,\n",
        "            proj_drop=0.,\n",
        "            norm_layer=nn.LayerNorm,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.fused_attn = True\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
        "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv.unbind(0)\n",
        "        q, k = self.q_norm(q), self.k_norm(k)\n",
        "\n",
        "        if self.fused_attn:\n",
        "            x = F.scaled_dot_product_attention(\n",
        "             q, k, v,\n",
        "                dropout_p=self.attn_drop.p,\n",
        "            )\n",
        "        else:\n",
        "            q = q * self.scale\n",
        "            attn = q @ k.transpose(-2, -1)\n",
        "            attn = attn.softmax(dim=-1)\n",
        "            attn = self.attn_drop(attn)\n",
        "            x = attn @ v\n",
        "\n",
        "        x = x.transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block2(nn.Module):\n",
        "    def __init__(self,\n",
        "                 dim,\n",
        "                 num_heads,\n",
        "                 counter,\n",
        "                 transformer_blocks,\n",
        "                 mlp_ratio=4.,\n",
        "                 qkv_bias=False,\n",
        "                 qk_scale=False,\n",
        "                 drop=0.,\n",
        "                 attn_drop=0.,\n",
        "                 drop_path=0.,\n",
        "                 act_layer=nn.GELU,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 Mlp_block=Mlp,\n",
        "                 layer_scale=None,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        if counter in transformer_blocks:\n",
        "            self.mixer = Attention(\n",
        "            dim,\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_norm=qk_scale,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop,\n",
        "            norm_layer=norm_layer,\n",
        "        )\n",
        "        else:\n",
        "            self.mixer = MambaVisionMixer(d_model=dim,\n",
        "                                          d_state=8,\n",
        "                                          d_conv=3,\n",
        "                                          expand=1\n",
        "                                          )\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "        use_layer_scale = layer_scale is not None and type(layer_scale) in [int, float]\n",
        "        self.gamma_1 = nn.Parameter(layer_scale * torch.ones(dim))  if use_layer_scale else 1\n",
        "        self.gamma_2 = nn.Parameter(layer_scale * torch.ones(dim))  if use_layer_scale else 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.gamma_1 * self.mixer(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class MambaVisionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    MambaVision layer\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim,\n",
        "                 depth,\n",
        "                 num_heads,\n",
        "                 window_size,\n",
        "                 conv=False,\n",
        "                 downsample=True,\n",
        "                 mlp_ratio=4.,\n",
        "                 qkv_bias=True,\n",
        "                 qk_scale=None,\n",
        "                 drop=0.,\n",
        "                 attn_drop=0.,\n",
        "                 drop_path=0.,\n",
        "                 layer_scale=None,\n",
        "                 layer_scale_conv=None,\n",
        "                 transformer_blocks = [],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim: feature size dimension.\n",
        "            depth: number of layers in each stage.\n",
        "            window_size: window size in each stage.\n",
        "            conv: bool argument for conv stage flag.\n",
        "            downsample: bool argument for down-sampling.\n",
        "            mlp_ratio: MLP ratio.\n",
        "            num_heads: number of heads in each stage.\n",
        "            qkv_bias: bool argument for query, key, value learnable bias.\n",
        "            qk_scale: bool argument to scaling query, key.\n",
        "            drop: dropout rate.\n",
        "            attn_drop: attention dropout rate.\n",
        "            drop_path: drop path rate.\n",
        "            norm_layer: normalization layer.\n",
        "            layer_scale: layer scaling coefficient.\n",
        "            layer_scale_conv: conv layer scaling coefficient.\n",
        "            transformer_blocks: list of transformer blocks.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.conv = conv\n",
        "        self.transformer_block = False\n",
        "        if conv:\n",
        "            self.blocks = nn.ModuleList([ConvBlock(dim=dim,\n",
        "                                                   drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                                                   layer_scale=layer_scale_conv)\n",
        "                                                   for i in range(depth)])\n",
        "            self.transformer_block = False\n",
        "        else:\n",
        "            self.blocks = nn.ModuleList([Block2(dim=dim,\n",
        "                                               counter=i,\n",
        "                                               transformer_blocks=transformer_blocks,\n",
        "                                               num_heads=num_heads,\n",
        "                                               mlp_ratio=mlp_ratio,\n",
        "                                               qkv_bias=qkv_bias,\n",
        "                                               qk_scale=qk_scale,\n",
        "                                               drop=drop,\n",
        "                                               attn_drop=attn_drop,\n",
        "                                               drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                                               layer_scale=layer_scale)\n",
        "                                               for i in range(depth)])\n",
        "            self.transformer_block = True\n",
        "\n",
        "        self.downsample = None if not downsample else Downsample(dim=dim)\n",
        "        self.do_gt = False\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, H, W = x.shape\n",
        "\n",
        "        if self.transformer_block:\n",
        "            pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
        "            pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
        "            if pad_r > 0 or pad_b > 0:\n",
        "                x = torch.nn.functional.pad(x, (0,pad_r,0,pad_b))\n",
        "                _, _, Hp, Wp = x.shape\n",
        "            else:\n",
        "                Hp, Wp = H, W\n",
        "            x = window_partition(x, self.window_size)\n",
        "\n",
        "        for _, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "        if self.transformer_block:\n",
        "            x = window_reverse(x, self.window_size, Hp, Wp)\n",
        "            if pad_r > 0 or pad_b > 0:\n",
        "                x = x[:, :, :H, :W].contiguous()\n",
        "        if self.downsample is None:\n",
        "            return x\n",
        "        return self.downsample(x)\n",
        "\n",
        "\n",
        "class MambaVision(nn.Module):\n",
        "    \"\"\"\n",
        "    MambaVision,\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim,\n",
        "                 in_dim,\n",
        "                 depths,\n",
        "                 window_size,\n",
        "                 mlp_ratio,\n",
        "                 num_heads,\n",
        "                 drop_path_rate=0.2,\n",
        "                 in_chans=3,\n",
        "                 num_classes=1000,\n",
        "                 qkv_bias=True,\n",
        "                 qk_scale=None,\n",
        "                 drop_rate=0.,\n",
        "                 attn_drop_rate=0.,\n",
        "                 layer_scale=None,\n",
        "                 layer_scale_conv=None,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim: feature size dimension.\n",
        "            depths: number of layers in each stage.\n",
        "            window_size: window size in each stage.\n",
        "            mlp_ratio: MLP ratio.\n",
        "            num_heads: number of heads in each stage.\n",
        "            drop_path_rate: drop path rate.\n",
        "            in_chans: number of input channels.\n",
        "            num_classes: number of classes.\n",
        "            qkv_bias: bool argument for query, key, value learnable bias.\n",
        "            qk_scale: bool argument to scaling query, key.\n",
        "            drop_rate: dropout rate.\n",
        "            attn_drop_rate: attention dropout rate.\n",
        "            norm_layer: normalization layer.\n",
        "            layer_scale: layer scaling coefficient.\n",
        "            layer_scale_conv: conv layer scaling coefficient.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        num_features = int(dim * 2 ** (len(depths) - 1))\n",
        "        self.num_classes = num_classes\n",
        "        self.patch_embed = PatchEmbed3(in_chans=in_chans, in_dim=in_dim, dim=dim)\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "        self.levels = nn.ModuleList()\n",
        "        for i in range(len(depths)):\n",
        "            conv = True if (i == 0 or i == 1) else False\n",
        "            level = MambaVisionLayer(dim=int(dim * 2 ** i),\n",
        "                                     depth=depths[i],\n",
        "                                     num_heads=num_heads[i],\n",
        "                                     window_size=window_size[i],\n",
        "                                     mlp_ratio=mlp_ratio,\n",
        "                                     qkv_bias=qkv_bias,\n",
        "                                     qk_scale=qk_scale,\n",
        "                                     conv=conv,\n",
        "                                     drop=drop_rate,\n",
        "                                     attn_drop=attn_drop_rate,\n",
        "                                     drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n",
        "                                     downsample=(i < 3),\n",
        "                                     layer_scale=layer_scale,\n",
        "                                     layer_scale_conv=layer_scale_conv,\n",
        "                                     transformer_blocks=list(range(depths[i]//2+1, depths[i])) if depths[i]%2!=0 else list(range(depths[i]//2, depths[i])),\n",
        "                                     )\n",
        "            self.levels.append(level)\n",
        "        self.norm = nn.BatchNorm2d(num_features)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.head = nn.Linear(num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, LayerNorm2d):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.ones_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        return {'rpb'}\n",
        "\n",
        "    # def forward_features(self, x):\n",
        "    #     x = self.patch_embed(x)\n",
        "    #     for level in self.levels:\n",
        "    #         x = level(x)\n",
        "    #     x = self.norm(x)\n",
        "    #     x = self.avgpool(x)\n",
        "    #     x = torch.flatten(x, 1)\n",
        "    #     return x\n",
        "    def forward_features(self, x, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
        "        x = self.patch_embed(x.to(device))\n",
        "        low_features = self.levels[0](x.to(device))\n",
        "        medium_features = self.levels[1](low_features.to(device))\n",
        "        x = self.levels[2](medium_features.to(device))\n",
        "        high_features = self.levels[3](x.to(device))\n",
        "        return low_features, medium_features, high_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        low, medium, high = self.forward_features(x)\n",
        "        return low, medium, high\n",
        "\n",
        "    def _load_state_dict(self,\n",
        "                         pretrained,\n",
        "                         strict: bool = False):\n",
        "        _load_checkpoint(self,\n",
        "                         pretrained,\n",
        "                         strict=strict)\n",
        "\n",
        "\n",
        "\n",
        "def mamba_vision_T(pretrained=False, **kwargs):\n",
        "    model_path = kwargs.pop(\"model_path\", \"/tmp/mamba_vision_T.pth.tar\")\n",
        "    depths = kwargs.pop(\"depths\", [1, 3, 8, 4])\n",
        "    num_heads = kwargs.pop(\"num_heads\", [2, 4, 8, 16])\n",
        "    window_size = kwargs.pop(\"window_size\", [8, 8, 14, 7])\n",
        "    dim = kwargs.pop(\"dim\", 80)\n",
        "    in_dim = kwargs.pop(\"in_dim\", 32)\n",
        "    mlp_ratio = kwargs.pop(\"mlp_ratio\", 4)\n",
        "    resolution = kwargs.pop(\"resolution\", 224)\n",
        "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.2)\n",
        "    pretrained_cfg = resolve_pretrained_cfg('mamba_vision_T').to_dict()\n",
        "    update_args(pretrained_cfg, kwargs, kwargs_filter=None)\n",
        "    model = MambaVision(depths=[1, 3, 8, 4],\n",
        "                        num_heads=[2, 4, 8, 16],\n",
        "                        window_size=[8, 8, 14, 7],\n",
        "                        dim=80,\n",
        "                        in_dim=32,\n",
        "                        mlp_ratio=4,\n",
        "                        resolution=224,\n",
        "                        drop_path_rate=0.2,\n",
        "                        **kwargs)\n",
        "    model.pretrained_cfg = pretrained_cfg\n",
        "    model.default_cfg = model.pretrained_cfg\n",
        "    if pretrained:\n",
        "        if not Path(model_path).is_file():\n",
        "            url = model.default_cfg['url']\n",
        "            torch.hub.download_url_to_file(url=url, dst=model_path)\n",
        "        model._load_state_dict(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def mamba_vision_T2(pretrained=False, **kwargs):\n",
        "    model_path = kwargs.pop(\"model_path\", \"/tmp/mamba_vision_T2.pth.tar\")\n",
        "    depths = kwargs.pop(\"depths\", [1, 3, 11, 4])\n",
        "    num_heads = kwargs.pop(\"num_heads\", [2, 4, 8, 16])\n",
        "    window_size = kwargs.pop(\"window_size\", [8, 8, 14, 7])\n",
        "    dim = kwargs.pop(\"dim\", 80)\n",
        "    in_dim = kwargs.pop(\"in_dim\", 32)\n",
        "    mlp_ratio = kwargs.pop(\"mlp_ratio\", 4)\n",
        "    resolution = kwargs.pop(\"resolution\", 224)\n",
        "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.2)\n",
        "    pretrained_cfg = resolve_pretrained_cfg('mamba_vision_T2').to_dict()\n",
        "    update_args(pretrained_cfg, kwargs, kwargs_filter=None)\n",
        "    model = MambaVision(depths=[1, 3, 11, 4],\n",
        "                        num_heads=[2, 4, 8, 16],\n",
        "                        window_size=[8, 8, 14, 7],\n",
        "                        dim=80,\n",
        "                        in_dim=32,\n",
        "                        mlp_ratio=4,\n",
        "                        resolution=224,\n",
        "                        drop_path_rate=0.2,\n",
        "                        **kwargs)\n",
        "    model.pretrained_cfg = pretrained_cfg\n",
        "    model.default_cfg = model.pretrained_cfg\n",
        "    if pretrained:\n",
        "        if not Path(model_path).is_file():\n",
        "            url = model.default_cfg['url']\n",
        "            torch.hub.download_url_to_file(url=url, dst=model_path)\n",
        "        model._load_state_dict(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def mamba_vision_S(pretrained=False, **kwargs):\n",
        "    model_path = kwargs.pop(\"model_path\", \"/tmp/mamba_vision_S.pth.tar\")\n",
        "    depths = kwargs.pop(\"depths\", [3, 3, 7, 5])\n",
        "    num_heads = kwargs.pop(\"num_heads\", [2, 4, 8, 16])\n",
        "    window_size = kwargs.pop(\"window_size\", [8, 8, 14, 7])\n",
        "    dim = kwargs.pop(\"dim\", 96)\n",
        "    in_dim = kwargs.pop(\"in_dim\", 64)\n",
        "    mlp_ratio = kwargs.pop(\"mlp_ratio\", 4)\n",
        "    resolution = kwargs.pop(\"resolution\", 224)\n",
        "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.2)\n",
        "    pretrained_cfg = resolve_pretrained_cfg('mamba_vision_S').to_dict()\n",
        "    update_args(pretrained_cfg, kwargs, kwargs_filter=None)\n",
        "    model = MambaVision(depths=[3, 3, 7, 5],\n",
        "                        num_heads=[2, 4, 8, 16],\n",
        "                        window_size=[8, 8, 14, 7],\n",
        "                        dim=96,\n",
        "                        in_dim=64,\n",
        "                        mlp_ratio=4,\n",
        "                        resolution=224,\n",
        "                        drop_path_rate=0.2,\n",
        "                        **kwargs)\n",
        "    model.pretrained_cfg = pretrained_cfg\n",
        "    model.default_cfg = model.pretrained_cfg\n",
        "    if pretrained:\n",
        "        if not Path(model_path).is_file():\n",
        "            url = model.default_cfg['url']\n",
        "            torch.hub.download_url_to_file(url=url, dst=model_path)\n",
        "        model._load_state_dict(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def mamba_vision_B(pretrained=False, **kwargs):\n",
        "    model_path = kwargs.pop(\"model_path\", \"/tmp/mamba_vision_B.pth.tar\")\n",
        "    depths = kwargs.pop(\"depths\", [3, 3, 10, 5])\n",
        "    num_heads = kwargs.pop(\"num_heads\", [2, 4, 8, 16])\n",
        "    window_size = kwargs.pop(\"window_size\", [8, 8, 14, 7])\n",
        "    dim = kwargs.pop(\"dim\", 128)\n",
        "    in_dim = kwargs.pop(\"in_dim\", 64)\n",
        "    mlp_ratio = kwargs.pop(\"mlp_ratio\", 4)\n",
        "    resolution = kwargs.pop(\"resolution\", 224)\n",
        "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.3)\n",
        "    layer_scale = kwargs.pop(\"layer_scale\", 1e-5)\n",
        "    pretrained_cfg = resolve_pretrained_cfg('mamba_vision_B').to_dict()\n",
        "    update_args(pretrained_cfg, kwargs, kwargs_filter=None)\n",
        "    model = MambaVision(depths=[3, 3, 10, 5],\n",
        "                        num_heads=[2, 4, 8, 16],\n",
        "                        window_size=[8, 8, 14, 7],\n",
        "                        dim=128,\n",
        "                        in_dim=64,\n",
        "                        mlp_ratio=4,\n",
        "                        resolution=224,\n",
        "                        drop_path_rate=0.3,\n",
        "                        layer_scale=1e-5,\n",
        "                        layer_scale_conv=None,\n",
        "                        **kwargs)\n",
        "    model.pretrained_cfg = pretrained_cfg\n",
        "    model.default_cfg = model.pretrained_cfg\n",
        "    if pretrained:\n",
        "        if not Path(model_path).is_file():\n",
        "            url = model.default_cfg['url']\n",
        "            torch.hub.download_url_to_file(url=url, dst=model_path)\n",
        "        model._load_state_dict(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def mamba_vision_L(pretrained=False, **kwargs):\n",
        "    model_path = kwargs.pop(\"model_path\", \"/tmp/mamba_vision_L.pth.tar\")\n",
        "    depths = kwargs.pop(\"depths\", [3, 3, 10, 5])\n",
        "    num_heads = kwargs.pop(\"num_heads\", [4, 8, 16, 32])\n",
        "    window_size = kwargs.pop(\"window_size\", [8, 8, 14, 7])\n",
        "    dim = kwargs.pop(\"dim\", 196)\n",
        "    in_dim = kwargs.pop(\"in_dim\", 64)\n",
        "    mlp_ratio = kwargs.pop(\"mlp_ratio\", 4)\n",
        "    resolution = kwargs.pop(\"resolution\", 224)\n",
        "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.3)\n",
        "    layer_scale = kwargs.pop(\"layer_scale\", 1e-5)\n",
        "    pretrained_cfg = resolve_pretrained_cfg('mamba_vision_L').to_dict()\n",
        "    update_args(pretrained_cfg, kwargs, kwargs_filter=None)\n",
        "    model = MambaVision(depths=[3, 3, 10, 5],\n",
        "                        num_heads=[4, 8, 16, 32],\n",
        "                        window_size=[8, 8, 14, 7],\n",
        "                        dim=196,\n",
        "                        in_dim=64,\n",
        "                        mlp_ratio=4,\n",
        "                        resolution=224,\n",
        "                        drop_path_rate=0.3,\n",
        "                        layer_scale=1e-5,\n",
        "                        layer_scale_conv=None,\n",
        "                        **kwargs)\n",
        "    model.pretrained_cfg = pretrained_cfg\n",
        "    model.default_cfg = model.pretrained_cfg\n",
        "    if pretrained:\n",
        "        if not Path(model_path).is_file():\n",
        "            url = model.default_cfg['url']\n",
        "            torch.hub.download_url_to_file(url=url, dst=model_path)\n",
        "        model._load_state_dict(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def mamba_vision_L2(pretrained=False, **kwargs):\n",
        "    model_path = kwargs.pop(\"model_path\", \"/tmp/mamba_vision_L2.pth.tar\")\n",
        "    depths = kwargs.pop(\"depths\", [3, 3, 12, 5])\n",
        "    num_heads = kwargs.pop(\"num_heads\", [4, 8, 16, 32])\n",
        "    window_size = kwargs.pop(\"window_size\", [8, 8, 14, 7])\n",
        "    dim = kwargs.pop(\"dim\", 196)\n",
        "    in_dim = kwargs.pop(\"in_dim\", 64)\n",
        "    mlp_ratio = kwargs.pop(\"mlp_ratio\", 4)\n",
        "    resolution = kwargs.pop(\"resolution\", 224)\n",
        "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.3)\n",
        "    layer_scale = kwargs.pop(\"layer_scale\", 1e-5)\n",
        "    pretrained_cfg = resolve_pretrained_cfg('mamba_vision_L2').to_dict()\n",
        "    update_args(pretrained_cfg, kwargs, kwargs_filter=None)\n",
        "    model = MambaVision(depths=[3, 3, 12, 5],\n",
        "                        num_heads=[4, 8, 16, 32],\n",
        "                        window_size=[8, 8, 14, 7],\n",
        "                        dim=196,\n",
        "                        in_dim=64,\n",
        "                        mlp_ratio=4,\n",
        "                        resolution=224,\n",
        "                        drop_path_rate=0.3,\n",
        "                        layer_scale=1e-5,\n",
        "                        layer_scale_conv=None,\n",
        "                        **kwargs)\n",
        "    model.pretrained_cfg = pretrained_cfg\n",
        "    model.default_cfg = model.pretrained_cfg\n",
        "    if pretrained:\n",
        "        if not Path(model_path).is_file():\n",
        "            url = model.default_cfg['url']\n",
        "            torch.hub.download_url_to_file(url=url, dst=model_path)\n",
        "        model._load_state_dict(model_path)\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7huQVmEAYLp"
      },
      "source": [
        "### Mamba Vision Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmGvOZShAZ92"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MambaVision_Backbone(nn.Module):\n",
        "    def __init__(self, model_name, base_channels, base_depth, deep_mul, pretrained=False):\n",
        "        super(MambaVision_Backbone, self).__init__()\n",
        "\n",
        "        # Define MambaVision model as the backbone\n",
        "        if model_name == 'tiny':\n",
        "            self.mamba_vision = mamba_vision_T()\n",
        "        elif model_name == 'tiny2':\n",
        "            self.mamba_vision = mamba_vision_T2()\n",
        "        elif model_name == 'small':\n",
        "            self.mamba_vision = mamba_vision_S()\n",
        "        elif model_name == 'base':\n",
        "            self.mamba_vision = mamba_vision_B()\n",
        "        elif model_name == 'large':\n",
        "            self.mamba_vision = mamba_vision_L()\n",
        "        elif model_name == 'large2':\n",
        "          self.mamba_vision = mamba_vision_L2()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model name: {model_name}, Please select one from : tiny, tiny2, small, base, large, large2\")\n",
        "\n",
        "        # Reduction layers for feature maps from different stages\n",
        "        self.conv3_reduction = nn.Conv2d(160, base_channels * 4, kernel_size=1)  # Stage 2 output\n",
        "        self.conv4_reduction = nn.Conv2d(320, base_channels * 8, kernel_size=1)  # Stage 3 output\n",
        "        self.conv5_reduction = nn.Conv2d(640, int(base_channels * 16 * deep_mul), kernel_size=1)  # Stage 4 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat3, feat4, feat5 = self.mamba_vision(x)\n",
        "        feat3 = self.conv3_reduction(feat3)\n",
        "        feat4 = self.conv4_reduction(feat4)\n",
        "        feat5 = self.conv5_reduction(feat5)\n",
        "        # Return the reduced feature maps as low, medium, and high-level features\n",
        "        return feat3, feat4, feat5\n",
        "\n",
        "# Instantiate and test the backbone\n",
        "# if __name__ == \"__main__\":\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model = MambaVision_Backbone(\"tiny\",base_channels=64, base_depth=3, deep_mul=0.5,pretrained=False).to(device)\n",
        "#     x = torch.randn(1, 3, 640, 640) # Example input tensor\n",
        "#     low_feat, medium_feat, high_feat = model(x)\n",
        "#     print(f\"Low-level features shape: {low_feat.shape}\")\n",
        "#     print(f\"Medium-level features shape: {medium_feat.shape}\")\n",
        "#     print(f\"High-level features shape: {high_feat.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbA8dCuo4HX5"
      },
      "source": [
        "## VMamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiXfPxLN4Pyo",
        "outputId": "735f771d-1a27-46a2-a6aa-070356114ef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Can not import selective_scan_cuda_oflex. This affects speed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import copy\n",
        "from functools import partial\n",
        "from typing import Optional, Callable, Any\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from timm.models.layers import DropPath, trunc_normal_\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_str, flop_count, parameter_count\n",
        "\n",
        "DropPath.__repr__ = lambda self: f\"timm.DropPath({self.drop_prob})\"\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from functools import cached_property\n",
        "from mamba_ssm import selective_scan_fn\n",
        "\n",
        "# torch implementation ========================================\n",
        "def cross_scan_fwd(x: torch.Tensor, in_channel_first=True, out_channel_first=True, scans=0):\n",
        "    if in_channel_first:\n",
        "        B, C, H, W = x.shape\n",
        "        if scans == 0:\n",
        "            y = x.new_empty((B, 4, C, H * W))\n",
        "            y[:, 0, :, :] = x.flatten(2, 3)\n",
        "            y[:, 1, :, :] = x.transpose(dim0=2, dim1=3).flatten(2, 3)\n",
        "            y[:, 2:4, :, :] = torch.flip(y[:, 0:2, :, :], dims=[-1])\n",
        "        elif scans == 1:\n",
        "            y = x.view(B, 1, C, H * W).repeat(1, 4, 1, 1)\n",
        "        elif scans == 2:\n",
        "            y = x.view(B, 1, C, H * W).repeat(1, 2, 1, 1)\n",
        "            y = torch.cat([y, y.flip(dims=[-1])], dim=1)\n",
        "    else:\n",
        "        B, H, W, C = x.shape\n",
        "        if scans == 0:\n",
        "            y = x.new_empty((B, H * W, 4, C))\n",
        "            y[:, :, 0, :] = x.flatten(1, 2)\n",
        "            y[:, :, 1, :] = x.transpose(dim0=1, dim1=2).flatten(1, 2)\n",
        "            y[:, :, 2:4, :] = torch.flip(y[:, :, 0:2, :], dims=[1])\n",
        "        elif scans == 1:\n",
        "            y = x.view(B, H * W, 1, C).repeat(1, 1, 4, 1)\n",
        "        elif scans == 2:\n",
        "            y = x.view(B, H * W, 1, C).repeat(1, 1, 2, 1)\n",
        "            y = torch.cat([y, y.flip(dims=[1])], dim=2)\n",
        "\n",
        "    if in_channel_first and (not out_channel_first):\n",
        "        y = y.permute(0, 3, 1, 2).contiguous()\n",
        "    elif (not in_channel_first) and out_channel_first:\n",
        "        y = y.permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def cross_merge_fwd(y: torch.Tensor, in_channel_first=True, out_channel_first=True, scans=0):\n",
        "    if out_channel_first:\n",
        "        B, K, D, H, W = y.shape\n",
        "        y = y.view(B, K, D, -1)\n",
        "        if scans == 0:\n",
        "            y = y[:, 0:2] + y[:, 2:4].flip(dims=[-1]).view(B, 2, D, -1)\n",
        "            y = y[:, 0] + y[:, 1].view(B, -1, W, H).transpose(dim0=2, dim1=3).contiguous().view(B, D, -1)\n",
        "        elif scans == 1:\n",
        "            y = y.sum(1)\n",
        "        elif scans == 2:\n",
        "            y = y[:, 0:2] + y[:, 2:4].flip(dims=[-1]).view(B, 2, D, -1)\n",
        "            y = y.sum(1)\n",
        "    else:\n",
        "        B, H, W, K, D = y.shape\n",
        "        y = y.view(B, -1, K, D)\n",
        "        if scans == 0:\n",
        "            y = y[:, :, 0:2] + y[:, :, 2:4].flip(dims=[1]).view(B, -1, 2, D)\n",
        "            y = y[:, :, 0] + y[:, :, 1].view(B, W, H, -1).transpose(dim0=1, dim1=2).contiguous().view(B, -1, D)\n",
        "        elif scans == 1:\n",
        "            y = y.sum(2)\n",
        "        elif scans == 2:\n",
        "            y = y[:, :, 0:2] + y[:, :, 2:4].flip(dims=[1]).view(B, -1, 2, D)\n",
        "            y = y.sum(2)\n",
        "\n",
        "    if in_channel_first and (not out_channel_first):\n",
        "        y = y.permute(0, 2, 1).contiguous()\n",
        "    elif (not in_channel_first) and out_channel_first:\n",
        "        y = y.permute(0, 2, 1).contiguous()\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def cross_scan1b1_fwd(x: torch.Tensor, in_channel_first=True, out_channel_first=True, scans=0):\n",
        "    if in_channel_first:\n",
        "        B, _, C, H, W = x.shape\n",
        "        if scans == 0:\n",
        "            y = torch.stack([\n",
        "                x[:, 0].flatten(2, 3),\n",
        "                x[:, 1].transpose(dim0=2, dim1=3).flatten(2, 3),\n",
        "                torch.flip(x[:, 2].flatten(2, 3), dims=[-1]),\n",
        "                torch.flip(x[:, 3].transpose(dim0=2, dim1=3).flatten(2, 3), dims=[-1]),\n",
        "            ], dim=1)\n",
        "        elif scans == 1:\n",
        "            y = x.flatten(2, 3)\n",
        "        elif scans == 2:\n",
        "            y = torch.stack([\n",
        "                x[:, 0].flatten(2, 3),\n",
        "                x[:, 1].flatten(2, 3),\n",
        "                torch.flip(x[:, 2].flatten(2, 3), dims=[-1]),\n",
        "                torch.flip(x[:, 3].flatten(2, 3), dims=[-1]),\n",
        "            ], dim=1)\n",
        "    else:\n",
        "        B, H, W, _, C = x.shape\n",
        "        if scans == 0:\n",
        "            y = torch.stack([\n",
        "                x[:, :, :, 0].flatten(1, 2),\n",
        "                x[:, :, :, 1].transpose(dim0=1, dim1=2).flatten(1, 2),\n",
        "                torch.flip(x[:, :, :, 2].flatten(1, 2), dims=[1]),\n",
        "                torch.flip(x[:, :, :, 3].transpose(dim0=1, dim1=2).flatten(1, 2), dims=[1]),\n",
        "            ], dim=2)\n",
        "        elif scans == 1:\n",
        "            y = x.flatten(1, 2)\n",
        "        elif scans == 2:\n",
        "            y = torch.stack([\n",
        "                x[:, 0].flatten(1, 2),\n",
        "                x[:, 1].flatten(1, 2),\n",
        "                torch.flip(x[:, 2].flatten(1, 2), dims=[-1]),\n",
        "                torch.flip(x[:, 3].flatten(1, 2), dims=[-1]),\n",
        "            ], dim=2)\n",
        "\n",
        "    if in_channel_first and (not out_channel_first):\n",
        "        y = y.permute(0, 3, 1, 2).contiguous()\n",
        "    elif (not in_channel_first) and out_channel_first:\n",
        "        y = y.permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def cross_merge1b1_fwd(y: torch.Tensor, in_channel_first=True, out_channel_first=True, scans=0):\n",
        "    if out_channel_first:\n",
        "        B, K, D, H, W = y.shape\n",
        "        y = y.view(B, K, D, -1)\n",
        "        if scans == 0:\n",
        "            y = torch.stack([\n",
        "                y[:, 0],\n",
        "                y[:, 1].view(B, -1, W, H).transpose(dim0=2, dim1=3).flatten(2, 3),\n",
        "                torch.flip(y[:, 2], dims=[-1]),\n",
        "                torch.flip(y[:, 3].view(B, -1, W, H).transpose(dim0=2, dim1=3).flatten(2, 3), dims=[-1]),\n",
        "            ], dim=1)\n",
        "        elif scans == 1:\n",
        "            y = y\n",
        "        elif scans == 2:\n",
        "            y = torch.stack([\n",
        "                y[:, 0],\n",
        "                y[:, 1],\n",
        "                torch.flip(y[:, 2], dims=[-1]),\n",
        "                torch.flip(y[:, 3], dims=[-1]),\n",
        "            ], dim=1)\n",
        "    else:\n",
        "        B, H, W, K, D = y.shape\n",
        "        y = y.view(B, -1, K, D)\n",
        "        if scans == 0:\n",
        "            y = torch.stack([\n",
        "                y[:, :, 0],\n",
        "                y[:, :, 1].view(B, W, H, -1).transpose(dim0=1, dim1=2).flatten(1, 2),\n",
        "                torch.flip(y[:, :, 2], dims=[1]),\n",
        "                torch.flip(y[:, :, 3].view(B, W, H, -1).transpose(dim0=1, dim1=2).flatten(1, 2), dims=[1]),\n",
        "            ], dim=2)\n",
        "        elif scans == 1:\n",
        "            y = y\n",
        "        elif scans == 2:\n",
        "            y = torch.stack([\n",
        "                y[:, :, 0],\n",
        "                y[:, :, 1],\n",
        "                torch.flip(y[:, :, 2], dims=[1]),\n",
        "                torch.flip(y[:, :, 3], dims=[1]),\n",
        "            ], dim=2)\n",
        "\n",
        "    if out_channel_first and (not in_channel_first):\n",
        "        y = y.permute(0, 3, 1, 2).contiguous()\n",
        "    elif (not out_channel_first) and in_channel_first:\n",
        "        y = y.permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "class CrossScanF(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x: torch.Tensor, in_channel_first=True, out_channel_first=True, one_by_one=False, scans=0):\n",
        "        # x: (B, C, H, W) | (B, H, W, C) | (B, 4, C, H, W) | (B, H, W, 4, C)\n",
        "        # y: (B, 4, C, H * W) | (B, H * W, 4, C)\n",
        "        ctx.in_channel_first = in_channel_first\n",
        "        ctx.out_channel_first = out_channel_first\n",
        "        ctx.one_by_one = one_by_one\n",
        "        ctx.scans = scans\n",
        "\n",
        "        if one_by_one:\n",
        "            B, K, C, H, W = x.shape\n",
        "            if not in_channel_first:\n",
        "                B, H, W, K, C = x.shape\n",
        "        else:\n",
        "            B, C, H, W = x.shape\n",
        "            if not in_channel_first:\n",
        "                B, H, W, C = x.shape\n",
        "        ctx.shape = (B, C, H, W)\n",
        "\n",
        "        _fn = cross_scan1b1_fwd if one_by_one else cross_scan_fwd\n",
        "        y = _fn(x, in_channel_first, out_channel_first, scans)\n",
        "\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, ys: torch.Tensor):\n",
        "        # out: (b, k, d, l)\n",
        "        in_channel_first = ctx.in_channel_first\n",
        "        out_channel_first = ctx.out_channel_first\n",
        "        one_by_one = ctx.one_by_one\n",
        "        scans = ctx.scans\n",
        "        B, C, H, W = ctx.shape\n",
        "\n",
        "        ys = ys.view(B, -1, C, H, W) if out_channel_first else ys.view(B, H, W, -1, C)\n",
        "        _fn = cross_merge1b1_fwd if one_by_one else cross_merge_fwd\n",
        "        y = _fn(ys, in_channel_first, out_channel_first, scans)\n",
        "\n",
        "        if one_by_one:\n",
        "            y = y.view(B, 4, -1, H, W) if in_channel_first else y.view(B, H, W, 4, -1)\n",
        "        else:\n",
        "            y = y.view(B, -1, H, W) if in_channel_first else y.view(B, H, W, -1)\n",
        "\n",
        "        return y, None, None, None, None\n",
        "\n",
        "\n",
        "class CrossMergeF(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, ys: torch.Tensor, in_channel_first=True, out_channel_first=True, one_by_one=False, scans=0):\n",
        "        # x: (B, C, H, W) | (B, H, W, C) | (B, 4, C, H, W) | (B, H, W, 4, C)\n",
        "        # y: (B, 4, C, H * W) | (B, H * W, 4, C)\n",
        "        ctx.in_channel_first = in_channel_first\n",
        "        ctx.out_channel_first = out_channel_first\n",
        "        ctx.one_by_one = one_by_one\n",
        "        ctx.scans = scans\n",
        "\n",
        "        B, K, C, H, W = ys.shape\n",
        "        if not out_channel_first:\n",
        "            B, H, W, K, C = ys.shape\n",
        "        ctx.shape = (B, C, H, W)\n",
        "\n",
        "        _fn = cross_merge1b1_fwd if one_by_one else cross_merge_fwd\n",
        "        y = _fn(ys, in_channel_first, out_channel_first, scans)\n",
        "\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, x: torch.Tensor):\n",
        "        # B, D, L = x.shape\n",
        "        # out: (b, k, d, h, w)\n",
        "        in_channel_first = ctx.in_channel_first\n",
        "        out_channel_first = ctx.out_channel_first\n",
        "        one_by_one = ctx.one_by_one\n",
        "        scans = ctx.scans\n",
        "        B, C, H, W = ctx.shape\n",
        "\n",
        "        if not one_by_one:\n",
        "            if in_channel_first:\n",
        "                x = x.view(B, C, H, W)\n",
        "            else:\n",
        "                x = x.view(B, H, W, C)\n",
        "        else:\n",
        "            if in_channel_first:\n",
        "                x = x.view(B, 4, C, H, W)\n",
        "            else:\n",
        "                x = x.view(B, H, W, 4, C)\n",
        "\n",
        "        _fn = cross_scan1b1_fwd if one_by_one else cross_scan_fwd\n",
        "        x = _fn(x, in_channel_first, out_channel_first, scans)\n",
        "        x = x.view(B, 4, C, H, W) if out_channel_first else x.view(B, H, W, 4, C)\n",
        "\n",
        "        return x, None, None, None, None\n",
        "\n",
        "\n",
        "# triton implements ========================================\n",
        "\n",
        "@triton.jit\n",
        "def triton_cross_scan_flex(\n",
        "    x: tl.tensor, # (B, C, H, W) | (B, H, W, C) | (B, 4, C, H, W) | (B, H, W, 4, C)\n",
        "    y: tl.tensor, # (B, 4, C, H, W) | (B, H, W, 4, C)\n",
        "    x_layout: tl.constexpr,\n",
        "    y_layout: tl.constexpr,\n",
        "    operation: tl.constexpr,\n",
        "    onebyone: tl.constexpr,\n",
        "    scans: tl.constexpr,\n",
        "    BC: tl.constexpr,\n",
        "    BH: tl.constexpr,\n",
        "    BW: tl.constexpr,\n",
        "    DC: tl.constexpr,\n",
        "    DH: tl.constexpr,\n",
        "    DW: tl.constexpr,\n",
        "    NH: tl.constexpr,\n",
        "    NW: tl.constexpr,\n",
        "):\n",
        "    # x_layout = 0\n",
        "    # y_layout = 1 # 0 BCHW, 1 BHWC\n",
        "    # operation = 0 # 0 scan, 1 merge\n",
        "    # onebyone = 0 # 0 false, 1 true\n",
        "    # scans = 0 # 0 cross scan, 1 unidirectional, 2 bidirectional\n",
        "\n",
        "    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n",
        "    i_h, i_w = (i_hw // NW), (i_hw % NW)\n",
        "    _mask_h = (i_h * BH + tl.arange(0, BH)) < DH\n",
        "    _mask_w = (i_w * BW + tl.arange(0, BW)) < DW\n",
        "    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n",
        "    _for_C = min(DC - i_c * BC, BC)\n",
        "\n",
        "    pos_h = (i_h * BH + tl.arange(0, BH)[:, None])\n",
        "    pos_w = (i_w * BW + tl.arange(0, BW)[None, :])\n",
        "    neg_h = (DH - i_h * BH - 1 - tl.arange(0, BH)[:, None])\n",
        "    neg_w = (DW - i_w * BW - 1 - tl.arange(0, BW)[None, :])\n",
        "    if scans == 0:\n",
        "        # none; trans; flip; trans + flip;\n",
        "        HWRoute0 = pos_h * DW + pos_w\n",
        "        HWRoute1 = pos_w * DH + pos_h # trans\n",
        "        HWRoute2 = neg_h * DW + neg_w # flip\n",
        "        HWRoute3 = neg_w * DH + neg_h # trans + flip\n",
        "    elif scans == 1:\n",
        "        # none; none; none; none;\n",
        "        HWRoute0 = pos_h * DW + pos_w\n",
        "        HWRoute1 = HWRoute0\n",
        "        HWRoute2 = HWRoute0\n",
        "        HWRoute3 = HWRoute0\n",
        "    elif scans == 2:\n",
        "        # none; none; flip; flip;\n",
        "        HWRoute0 = pos_h * DW + pos_w\n",
        "        HWRoute1 = HWRoute0\n",
        "        HWRoute2 = neg_h * DW + neg_w # flip\n",
        "        HWRoute3 = HWRoute2\n",
        "\n",
        "    _tmp1 = DC * DH * DW\n",
        "\n",
        "    y_ptr_base = y + i_b * 4 * _tmp1 + (i_c * BC * DH * DW if y_layout == 0 else i_c * BC)\n",
        "    if y_layout == 0:\n",
        "        p_y1 = y_ptr_base + HWRoute0\n",
        "        p_y2 = y_ptr_base + _tmp1 + HWRoute1\n",
        "        p_y3 = y_ptr_base + 2 * _tmp1 + HWRoute2\n",
        "        p_y4 = y_ptr_base + 3 * _tmp1 + HWRoute3\n",
        "    else:\n",
        "        p_y1 = y_ptr_base + HWRoute0 * 4 * DC\n",
        "        p_y2 = y_ptr_base + DC + HWRoute1 * 4 * DC\n",
        "        p_y3 = y_ptr_base + 2 * DC + HWRoute2 * 4 * DC\n",
        "        p_y4 = y_ptr_base + 3 * DC + HWRoute3 * 4 * DC\n",
        "\n",
        "    if onebyone == 0:\n",
        "        x_ptr_base = x + i_b * _tmp1 + (i_c * BC * DH * DW if x_layout == 0 else i_c * BC)\n",
        "        if x_layout == 0:\n",
        "            p_x = x_ptr_base + HWRoute0\n",
        "        else:\n",
        "            p_x = x_ptr_base + HWRoute0 * DC\n",
        "\n",
        "        if operation == 0:\n",
        "            for idxc in range(_for_C):\n",
        "                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n",
        "                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n",
        "                _x = tl.load(p_x + _idx_x, mask=_mask_hw)\n",
        "                tl.store(p_y1 + _idx_y, _x, mask=_mask_hw)\n",
        "                tl.store(p_y2 + _idx_y, _x, mask=_mask_hw)\n",
        "                tl.store(p_y3 + _idx_y, _x, mask=_mask_hw)\n",
        "                tl.store(p_y4 + _idx_y, _x, mask=_mask_hw)\n",
        "        elif operation == 1:\n",
        "            for idxc in range(_for_C):\n",
        "                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n",
        "                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n",
        "                _y1 = tl.load(p_y1 + _idx_y, mask=_mask_hw)\n",
        "                _y2 = tl.load(p_y2 + _idx_y, mask=_mask_hw)\n",
        "                _y3 = tl.load(p_y3 + _idx_y, mask=_mask_hw)\n",
        "                _y4 = tl.load(p_y4 + _idx_y, mask=_mask_hw)\n",
        "                tl.store(p_x + _idx_x, _y1 + _y2 + _y3 + _y4, mask=_mask_hw)\n",
        "\n",
        "    else:\n",
        "        x_ptr_base = x + i_b * 4 * _tmp1 + (i_c * BC * DH * DW if x_layout == 0 else i_c * BC)\n",
        "        if x_layout == 0:\n",
        "            p_x1 = x_ptr_base + HWRoute0\n",
        "            p_x2 = p_x1 + _tmp1\n",
        "            p_x3 = p_x2 + _tmp1\n",
        "            p_x4 = p_x3 + _tmp1\n",
        "        else:\n",
        "            p_x1 = x_ptr_base + HWRoute0 * 4 * DC\n",
        "            p_x2 = p_x1 + DC\n",
        "            p_x3 = p_x2 + DC\n",
        "            p_x4 = p_x3 + DC\n",
        "\n",
        "        if operation == 0:\n",
        "            for idxc in range(_for_C):\n",
        "                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n",
        "                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n",
        "                tl.store(p_y1 + _idx_y, tl.load(p_x1 + _idx_x, mask=_mask_hw), mask=_mask_hw)\n",
        "                tl.store(p_y2 + _idx_y, tl.load(p_x2 + _idx_x, mask=_mask_hw), mask=_mask_hw)\n",
        "                tl.store(p_y3 + _idx_y, tl.load(p_x3 + _idx_x, mask=_mask_hw), mask=_mask_hw)\n",
        "                tl.store(p_y4 + _idx_y, tl.load(p_x4 + _idx_x, mask=_mask_hw), mask=_mask_hw)\n",
        "        else:\n",
        "            for idxc in range(_for_C):\n",
        "                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n",
        "                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n",
        "                tl.store(p_x1 + _idx_x, tl.load(p_y1 + _idx_y), mask=_mask_hw)\n",
        "                tl.store(p_x2 + _idx_x, tl.load(p_y2 + _idx_y), mask=_mask_hw)\n",
        "                tl.store(p_x3 + _idx_x, tl.load(p_y3 + _idx_y), mask=_mask_hw)\n",
        "                tl.store(p_x4 + _idx_x, tl.load(p_y4 + _idx_y), mask=_mask_hw)\n",
        "\n",
        "\n",
        "class CrossScanTritonF(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x: torch.Tensor, in_channel_first=True, out_channel_first=True, one_by_one=False, scans=0):\n",
        "        if one_by_one:\n",
        "            if in_channel_first:\n",
        "                B, _, C, H, W = x.shape\n",
        "            else:\n",
        "                B, H, W, _, C = x.shape\n",
        "        else:\n",
        "            if in_channel_first:\n",
        "                B, C, H, W = x.shape\n",
        "            else:\n",
        "                B, H, W, C = x.shape\n",
        "        B, C, H, W = int(B), int(C), int(H), int(W)\n",
        "        BC, BH, BW = 1, 32, 32\n",
        "        NH, NW, NC = triton.cdiv(H, BH), triton.cdiv(W, BW), triton.cdiv(C, BC)\n",
        "\n",
        "        ctx.in_channel_first = in_channel_first\n",
        "        ctx.out_channel_first = out_channel_first\n",
        "        ctx.one_by_one = one_by_one\n",
        "        ctx.scans = scans\n",
        "        ctx.shape = (B, C, H, W)\n",
        "        ctx.triton_shape = (BC, BH, BW, NC, NH, NW)\n",
        "\n",
        "        y = x.new_empty((B, 4, C, H * W)) if out_channel_first else x.new_empty((B, H * W, 4, C))\n",
        "        triton_cross_scan_flex[(NH * NW, NC, B)](\n",
        "            x.contiguous(), y,\n",
        "            (0 if in_channel_first else 1), (0 if out_channel_first else 1), 0, (0 if not one_by_one else 1), scans,\n",
        "            BC, BH, BW, C, H, W, NH, NW\n",
        "        )\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, y: torch.Tensor):\n",
        "        in_channel_first = ctx.in_channel_first\n",
        "        out_channel_first = ctx.out_channel_first\n",
        "        one_by_one = ctx.one_by_one\n",
        "        scans = ctx.scans\n",
        "        B, C, H, W = ctx.shape\n",
        "        BC, BH, BW, NC, NH, NW = ctx.triton_shape\n",
        "        if one_by_one:\n",
        "            x = y.new_empty((B, 4, C, H, W)) if in_channel_first else y.new_empty((B, H, W, 4, C))\n",
        "        else:\n",
        "            x = y.new_empty((B, C, H, W)) if in_channel_first else y.new_empty((B, H, W, C))\n",
        "\n",
        "        triton_cross_scan_flex[(NH * NW, NC, B)](\n",
        "            x, y.contiguous(),\n",
        "            (0 if in_channel_first else 1), (0 if out_channel_first else 1), 1, (0 if not one_by_one else 1), scans,\n",
        "            BC, BH, BW, C, H, W, NH, NW\n",
        "        )\n",
        "        return x, None, None, None, None\n",
        "\n",
        "\n",
        "class CrossMergeTritonF(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, y: torch.Tensor, in_channel_first=True, out_channel_first=True, one_by_one=False, scans=0):\n",
        "        if out_channel_first:\n",
        "            B, _, C, H, W = y.shape\n",
        "        else:\n",
        "            B, H, W, _, C = y.shape\n",
        "        B, C, H, W = int(B), int(C), int(H), int(W)\n",
        "        BC, BH, BW = 1, 32, 32\n",
        "        NH, NW, NC = triton.cdiv(H, BH), triton.cdiv(W, BW), triton.cdiv(C, BC)\n",
        "        ctx.in_channel_first = in_channel_first\n",
        "        ctx.out_channel_first = out_channel_first\n",
        "        ctx.one_by_one = one_by_one\n",
        "        ctx.scans = scans\n",
        "        ctx.shape = (B, C, H, W)\n",
        "        ctx.triton_shape = (BC, BH, BW, NC, NH, NW)\n",
        "        if one_by_one:\n",
        "            x = y.new_empty((B, 4, C, H * W)) if in_channel_first else y.new_empty((B, H * W, 4, C))\n",
        "        else:\n",
        "            x = y.new_empty((B, C, H * W)) if in_channel_first else y.new_empty((B, H * W, C))\n",
        "        triton_cross_scan_flex[(NH * NW, NC, B)](\n",
        "            x, y.contiguous(),\n",
        "            (0 if in_channel_first else 1), (0 if out_channel_first else 1), 1, (0 if not one_by_one else 1), scans,\n",
        "            BC, BH, BW, C, H, W, NH, NW\n",
        "        )\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, x: torch.Tensor):\n",
        "        in_channel_first = ctx.in_channel_first\n",
        "        out_channel_first = ctx.out_channel_first\n",
        "        one_by_one = ctx.one_by_one\n",
        "        scans = ctx.scans\n",
        "        B, C, H, W = ctx.shape\n",
        "        BC, BH, BW, NC, NH, NW = ctx.triton_shape\n",
        "        y = x.new_empty((B, 4, C, H, W)) if out_channel_first else x.new_empty((B, H, W, 4, C))\n",
        "        triton_cross_scan_flex[(NH * NW, NC, B)](\n",
        "            x.contiguous(), y,\n",
        "            (0 if in_channel_first else 1), (0 if out_channel_first else 1), 0, (0 if not one_by_one else 1), scans,\n",
        "            BC, BH, BW, C, H, W, NH, NW\n",
        "        )\n",
        "        return y, None, None, None, None, None\n",
        "\n",
        "\n",
        "# @torch.compile(options={\"triton.cudagraphs\": True}, fullgraph=True)\n",
        "def cross_scan_fn(x: torch.Tensor, in_channel_first=True, out_channel_first=True, one_by_one=False, scans=0, force_torch=False):\n",
        "    # x: (B, C, H, W) | (B, H, W, C) | (B, 4, C, H, W) | (B, H, W, 4, C)\n",
        "    # y: (B, 4, C, L) | (B, L, 4, C)\n",
        "    # scans: 0: cross scan; 1 unidirectional; 2: bidirectional;\n",
        "    CSF = CrossScanTritonF if WITH_TRITON and x.is_cuda and (not force_torch) else CrossScanF\n",
        "    with torch.cuda.device(x.device):\n",
        "        return CSF.apply(x, in_channel_first, out_channel_first, one_by_one, scans)\n",
        "\n",
        "\n",
        "# @torch.compile(options={\"triton.cudagraphs\": True}, fullgraph=True)\n",
        "def cross_merge_fn(y: torch.Tensor, in_channel_first=True, out_channel_first=True, one_by_one=False, scans=0, force_torch=False):\n",
        "    # y: (B, 4, C, L) | (B, L, 4, C)\n",
        "    # x: (B, C, H * W) | (B, H * W, C) | (B, 4, C, H * W) | (B, H * W, 4, C)\n",
        "    # scans: 0: cross scan; 1 unidirectional; 2: bidirectional;\n",
        "    CMF = CrossMergeTritonF if WITH_TRITON and y.is_cuda and (not force_torch) else CrossMergeF\n",
        "    with torch.cuda.device(y.device):\n",
        "        return CMF.apply(y, in_channel_first, out_channel_first, one_by_one, scans)\n",
        "\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import warnings\n",
        "\n",
        "\n",
        "WITH_SELECTIVESCAN_OFLEX = True\n",
        "WITH_SELECTIVESCAN_CORE = False\n",
        "WITH_SELECTIVESCAN_MAMBA = True\n",
        "try:\n",
        "    import selective_scan_cuda_oflex\n",
        "except ImportError:\n",
        "    WITH_SELECTIVESCAN_OFLEX = False\n",
        "    warnings.warn(\"Can not import selective_scan_cuda_oflex. This affects speed.\")\n",
        "    print(\"Can not import selective_scan_cuda_oflex. This affects speed.\", flush=True)\n",
        "try:\n",
        "    import selective_scan_cuda_core\n",
        "except ImportError:\n",
        "    WITH_SELECTIVESCAN_CORE = False\n",
        "try:\n",
        "    import selective_scan_cuda\n",
        "except ImportError:\n",
        "    WITH_SELECTIVESCAN_MAMBA = False\n",
        "\n",
        "\n",
        "def selective_scan_torch(\n",
        "    u: torch.Tensor, # (B, K * C, L)\n",
        "    delta: torch.Tensor, # (B, K * C, L)\n",
        "    A: torch.Tensor, # (K * C, N)\n",
        "    B: torch.Tensor, # (B, K, N, L)\n",
        "    C: torch.Tensor, # (B, K, N, L)\n",
        "    D: torch.Tensor = None, # (K * C)\n",
        "    delta_bias: torch.Tensor = None, # (K * C)\n",
        "    delta_softplus=True,\n",
        "    oflex=True,\n",
        "    *args,\n",
        "    **kwargs\n",
        "):\n",
        "    dtype_in = u.dtype\n",
        "    Batch, K, N, L = B.shape\n",
        "    KCdim = u.shape[1]\n",
        "    Cdim = int(KCdim / K)\n",
        "    assert u.shape == (Batch, KCdim, L)\n",
        "    assert delta.shape == (Batch, KCdim, L)\n",
        "    assert A.shape == (KCdim, N)\n",
        "    assert C.shape == B.shape\n",
        "\n",
        "    if delta_bias is not None:\n",
        "        delta = delta + delta_bias[..., None]\n",
        "    if delta_softplus:\n",
        "        delta = torch.nn.functional.softplus(delta)\n",
        "\n",
        "    u, delta, A, B, C = u.float(), delta.float(), A.float(), B.float(), C.float()\n",
        "    B = B.view(Batch, K, 1, N, L).repeat(1, 1, Cdim, 1, 1).view(Batch, KCdim, N, L)\n",
        "    C = C.view(Batch, K, 1, N, L).repeat(1, 1, Cdim, 1, 1).view(Batch, KCdim, N, L)\n",
        "    deltaA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))\n",
        "    deltaB_u = torch.einsum('bdl,bdnl,bdl->bdln', delta, B, u)\n",
        "\n",
        "    if True:\n",
        "        x = A.new_zeros((Batch, KCdim, N))\n",
        "        ys = []\n",
        "        for i in range(L):\n",
        "            x = deltaA[:, :, i, :] * x + deltaB_u[:, :, i, :]\n",
        "            y = torch.einsum('bdn,bdn->bd', x, C[:, :, :, i])\n",
        "            ys.append(y)\n",
        "        y = torch.stack(ys, dim=2) # (B, C, L)\n",
        "\n",
        "    out = y if D is None else y + u * D.unsqueeze(-1)\n",
        "    return out if oflex else out.to(dtype=dtype_in)\n",
        "\n",
        "\n",
        "class SelectiveScanCuda(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    @torch.cuda.amp.custom_fwd\n",
        "    def forward(ctx, u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=False, oflex=True, backend=None):\n",
        "        ctx.delta_softplus = delta_softplus\n",
        "        backend = \"oflex\" if WITH_SELECTIVESCAN_OFLEX and (backend is None) else backend\n",
        "        backend = \"core\" if WITH_SELECTIVESCAN_CORE and (backend is None) else backend\n",
        "        backend = \"mamba\" if WITH_SELECTIVESCAN_MAMBA and (backend is None) else backend\n",
        "        ctx.backend = backend\n",
        "        if backend == \"oflex\":\n",
        "            out, x, *rest = selective_scan_cuda_oflex.fwd(u, delta, A, B, C, D, delta_bias, delta_softplus, 1, oflex)\n",
        "        elif backend == \"core\":\n",
        "            out, x, *rest = selective_scan_cuda_core.fwd(u, delta, A, B, C, D, delta_bias, delta_softplus, 1)\n",
        "        elif backend == \"mamba\":\n",
        "            out, x, *rest = selective_scan_cuda.fwd(u, delta, A, B, C, D, None, delta_bias, delta_softplus)\n",
        "        ctx.save_for_backward(u, delta, A, B, C, D, delta_bias, x)\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    @torch.cuda.amp.custom_bwd\n",
        "    def backward(ctx, dout, *args):\n",
        "        u, delta, A, B, C, D, delta_bias, x = ctx.saved_tensors\n",
        "        backend = ctx.backend\n",
        "        if dout.stride(-1) != 1:\n",
        "            dout = dout.contiguous()\n",
        "        if backend == \"oflex\":\n",
        "            du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda_oflex.bwd(\n",
        "                u, delta, A, B, C, D, delta_bias, dout, x, ctx.delta_softplus, 1\n",
        "            )\n",
        "        elif backend == \"core\":\n",
        "            du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda_core.bwd(\n",
        "                u, delta, A, B, C, D, delta_bias, dout, x, ctx.delta_softplus, 1\n",
        "            )\n",
        "        elif backend == \"mamba\":\n",
        "            du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda.bwd(\n",
        "                u, delta, A, B, C, D, None, delta_bias, dout, x, None, None, ctx.delta_softplus,\n",
        "                False\n",
        "            )\n",
        "        return du, ddelta, dA, dB, dC, dD, ddelta_bias, None, None, None\n",
        "\n",
        "\n",
        "def selective_scan_fn(\n",
        "    u: torch.Tensor, # (B, K * C, L)\n",
        "    delta: torch.Tensor, # (B, K * C, L)\n",
        "    A: torch.Tensor, # (K * C, N)\n",
        "    B: torch.Tensor, # (B, K, N, L)\n",
        "    C: torch.Tensor, # (B, K, N, L)\n",
        "    D: torch.Tensor = None, # (K * C)\n",
        "    delta_bias: torch.Tensor = None, # (K * C)\n",
        "    delta_softplus=True,\n",
        "    oflex=True,\n",
        "    backend=None,\n",
        "):\n",
        "    WITH_CUDA = (WITH_SELECTIVESCAN_OFLEX or WITH_SELECTIVESCAN_CORE or WITH_SELECTIVESCAN_MAMBA)\n",
        "    fn = selective_scan_torch if backend == \"torch\" or (not WITH_CUDA) else SelectiveScanCuda.apply\n",
        "    return fn(u, delta, A, B, C, D, delta_bias, delta_softplus, oflex, backend)\n",
        "\n",
        "\n",
        "# fvcore flops =======================================\n",
        "def print_jit_input_names(inputs):\n",
        "    print(\"input params: \", end=\" \", flush=True)\n",
        "    try:\n",
        "        for i in range(10):\n",
        "            print(inputs[i].debugName(), end=\" \", flush=True)\n",
        "    except Exception as e:\n",
        "        pass\n",
        "    print(\"\", flush=True)\n",
        "\n",
        "def flops_selective_scan_fn(B=1, L=256, D=768, N=16, with_D=True, with_Z=False, with_complex=False):\n",
        "    \"\"\"\n",
        "    u: r(B D L)\n",
        "    delta: r(B D L)\n",
        "    A: r(D N)\n",
        "    B: r(B N L)\n",
        "    C: r(B N L)\n",
        "    D: r(D)\n",
        "    z: r(B D L)\n",
        "    delta_bias: r(D), fp32\n",
        "\n",
        "    ignores:\n",
        "        [.float(), +, .softplus, .shape, new_zeros, repeat, stack, to(dtype), silu]\n",
        "    \"\"\"\n",
        "    assert not with_complex\n",
        "    # https://github.com/state-spaces/mamba/issues/110\n",
        "    flops = 9 * B * L * D * N\n",
        "    if with_D:\n",
        "        flops += B * D * L\n",
        "    if with_Z:\n",
        "        flops += B * D * L\n",
        "    return flops\n",
        "\n",
        "# this is only for selective_scan_ref...\n",
        "def flops_selective_scan_ref(B=1, L=256, D=768, N=16, with_D=True, with_Z=False, with_Group=True, with_complex=False):\n",
        "    \"\"\"\n",
        "    u: r(B D L)\n",
        "    delta: r(B D L)\n",
        "    A: r(D N)\n",
        "    B: r(B N L)\n",
        "    C: r(B N L)\n",
        "    D: r(D)\n",
        "    z: r(B D L)\n",
        "    delta_bias: r(D), fp32\n",
        "\n",
        "    ignores:\n",
        "        [.float(), +, .softplus, .shape, new_zeros, repeat, stack, to(dtype), silu]\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    # fvcore.nn.jit_handles\n",
        "    def get_flops_einsum(input_shapes, equation):\n",
        "        np_arrs = [np.zeros(s) for s in input_shapes]\n",
        "        optim = np.einsum_path(equation, *np_arrs, optimize=\"optimal\")[1]\n",
        "        for line in optim.split(\"\\n\"):\n",
        "            if \"optimized flop\" in line.lower():\n",
        "                # divided by 2 because we count MAC (multiply-add counted as one flop)\n",
        "                flop = float(np.floor(float(line.split(\":\")[-1]) / 2))\n",
        "                return flop\n",
        "\n",
        "\n",
        "    assert not with_complex\n",
        "\n",
        "    flops = 0 # below code flops = 0\n",
        "\n",
        "    flops += get_flops_einsum([[B, D, L], [D, N]], \"bdl,dn->bdln\")\n",
        "    if with_Group:\n",
        "        flops += get_flops_einsum([[B, D, L], [B, N, L], [B, D, L]], \"bdl,bnl,bdl->bdln\")\n",
        "    else:\n",
        "        flops += get_flops_einsum([[B, D, L], [B, D, N, L], [B, D, L]], \"bdl,bdnl,bdl->bdln\")\n",
        "\n",
        "    in_for_flops = B * D * N\n",
        "    if with_Group:\n",
        "        in_for_flops += get_flops_einsum([[B, D, N], [B, D, N]], \"bdn,bdn->bd\")\n",
        "    else:\n",
        "        in_for_flops += get_flops_einsum([[B, D, N], [B, N]], \"bdn,bn->bd\")\n",
        "    flops += L * in_for_flops\n",
        "    if with_D:\n",
        "        flops += B * D * L\n",
        "    if with_Z:\n",
        "        flops += B * D * L\n",
        "    return flops\n",
        "\n",
        "def selective_scan_flop_jit(inputs, outputs, backend=\"prefixsum\", verbose=True):\n",
        "    if verbose:\n",
        "        print_jit_input_names(inputs)\n",
        "    flops_fn = flops_selective_scan_ref if backend == \"naive\" else flops_selective_scan_fn\n",
        "    B, D, L = inputs[0].type().sizes()\n",
        "    N = inputs[2].type().sizes()[1]\n",
        "    flops = flops_fn(B=B, L=L, D=D, N=N, with_D=True, with_Z=False)\n",
        "    return flops\n",
        "\n",
        "class Linear2d(nn.Linear):\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # B, C, H, W = x.shape\n",
        "        return F.conv2d(x, self.weight[:, :, None, None], self.bias)\n",
        "\n",
        "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n",
        "        state_dict[prefix + \"weight\"] = state_dict[prefix + \"weight\"].view(self.weight.shape)\n",
        "        return super()._load_from_state_dict(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\n",
        "\n",
        "\n",
        "class LayerNorm2d(nn.LayerNorm):\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchMerging2D(nn.Module):\n",
        "    def __init__(self, dim, out_dim=-1, norm_layer=nn.LayerNorm, channel_first=False):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        Linear = Linear2d if channel_first else nn.Linear\n",
        "        self._patch_merging_pad = self._patch_merging_pad_channel_first if channel_first else self._patch_merging_pad_channel_last\n",
        "        self.reduction = Linear(4 * dim, (2 * dim) if out_dim < 0 else out_dim, bias=False)\n",
        "        self.norm = norm_layer(4 * dim)\n",
        "\n",
        "    @staticmethod\n",
        "    def _patch_merging_pad_channel_last(x: torch.Tensor):\n",
        "        H, W, _ = x.shape[-3:]\n",
        "        if (W % 2 != 0) or (H % 2 != 0):\n",
        "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
        "        x0 = x[..., 0::2, 0::2, :]  # ... H/2 W/2 C\n",
        "        x1 = x[..., 1::2, 0::2, :]  # ... H/2 W/2 C\n",
        "        x2 = x[..., 0::2, 1::2, :]  # ... H/2 W/2 C\n",
        "        x3 = x[..., 1::2, 1::2, :]  # ... H/2 W/2 C\n",
        "        x = torch.cat([x0, x1, x2, x3], -1)  # ... H/2 W/2 4*C\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _patch_merging_pad_channel_first(x: torch.Tensor):\n",
        "        H, W = x.shape[-2:]\n",
        "        if (W % 2 != 0) or (H % 2 != 0):\n",
        "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
        "        x0 = x[..., 0::2, 0::2]  # ... H/2 W/2\n",
        "        x1 = x[..., 1::2, 0::2]  # ... H/2 W/2\n",
        "        x2 = x[..., 0::2, 1::2]  # ... H/2 W/2\n",
        "        x3 = x[..., 1::2, 1::2]  # ... H/2 W/2\n",
        "        x = torch.cat([x0, x1, x2, x3], 1)  # ... H/2 W/2 4*C\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._patch_merging_pad(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Permute(nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return x.permute(*self.args)\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.,channels_first=False):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "\n",
        "        Linear = Linear2d if channels_first else nn.Linear\n",
        "        self.fc1 = Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class gMlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.,channels_first=False):\n",
        "        super().__init__()\n",
        "        self.channel_first = channels_first\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "\n",
        "        Linear = Linear2d if channels_first else nn.Linear\n",
        "        self.fc1 = Linear(in_features, 2 * hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.fc1(x)\n",
        "        x, z = x.chunk(2, dim=(1 if self.channel_first else -1))\n",
        "        x = self.fc2(x * self.act(z))\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SoftmaxSpatial(nn.Softmax):\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        if self.dim == -1:\n",
        "            B, C, H, W = x.shape\n",
        "            return super().forward(x.view(B, C, -1)).view(B, C, H, W)\n",
        "        elif self.dim == 1:\n",
        "            B, H, W, C = x.shape\n",
        "            return super().forward(x.view(B, -1, C)).view(B, H, W, C)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "class mamba_init:\n",
        "    @staticmethod\n",
        "    def dt_init(dt_rank, d_inner, dt_scale=1.0, dt_init=\"random\", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4):\n",
        "        dt_proj = nn.Linear(dt_rank, d_inner, bias=True)\n",
        "\n",
        "        # Initialize special dt projection to preserve variance at initialization\n",
        "        dt_init_std = dt_rank**-0.5 * dt_scale\n",
        "        if dt_init == \"constant\":\n",
        "            nn.init.constant_(dt_proj.weight, dt_init_std)\n",
        "        elif dt_init == \"random\":\n",
        "            nn.init.uniform_(dt_proj.weight, -dt_init_std, dt_init_std)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n",
        "        dt = torch.exp(\n",
        "            torch.rand(d_inner) * (math.log(dt_max) - math.log(dt_min))\n",
        "            + math.log(dt_min)\n",
        "        ).clamp(min=dt_init_floor)\n",
        "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
        "        with torch.no_grad():\n",
        "            dt_proj.bias.copy_(inv_dt)\n",
        "        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
        "        # dt_proj.bias._no_reinit = True\n",
        "\n",
        "        return dt_proj\n",
        "\n",
        "    @staticmethod\n",
        "    def A_log_init(d_state, d_inner, copies=-1, device=None, merge=True):\n",
        "        # S4D real initialization\n",
        "        A = torch.arange(1, d_state + 1, dtype=torch.float32, device=device).view(1, -1).repeat(d_inner, 1).contiguous()\n",
        "        A_log = torch.log(A)  # Keep A_log in fp32\n",
        "        if copies > 0:\n",
        "            A_log = A_log[None].repeat(copies, 1, 1).contiguous()\n",
        "            if merge:\n",
        "                A_log = A_log.flatten(0, 1)\n",
        "        A_log = nn.Parameter(A_log)\n",
        "        A_log._no_weight_decay = True\n",
        "        return A_log\n",
        "\n",
        "    @staticmethod\n",
        "    def D_init(d_inner, copies=-1, device=None, merge=True):\n",
        "        # D \"skip\" parameter\n",
        "        D = torch.ones(d_inner, device=device)\n",
        "        if copies > 0:\n",
        "            D = D[None].repeat(copies, 1).contiguous()\n",
        "            if merge:\n",
        "                D = D.flatten(0, 1)\n",
        "        D = nn.Parameter(D)  # Keep in fp32\n",
        "        D._no_weight_decay = True\n",
        "        return D\n",
        "\n",
        "    @classmethod\n",
        "    def init_dt_A_D(cls, d_state, dt_rank, d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, k_group=4):\n",
        "        # dt proj ============================\n",
        "        dt_projs = [\n",
        "            cls.dt_init(dt_rank, d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor)\n",
        "            for _ in range(k_group)\n",
        "        ]\n",
        "        dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in dt_projs], dim=0)) # (K, inner, rank)\n",
        "        dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in dt_projs], dim=0)) # (K, inner)\n",
        "        del dt_projs\n",
        "\n",
        "        # A, D =======================================\n",
        "        A_logs = cls.A_log_init(d_state, d_inner, copies=k_group, merge=True) # (K * D, N)\n",
        "        Ds = cls.D_init(d_inner, copies=k_group, merge=True) # (K * D)\n",
        "        return A_logs, Ds, dt_projs_weight, dt_projs_bias\n",
        "\n",
        "\n",
        "# support: v0, v0seq\n",
        "class SS2Dv0:\n",
        "    def __initv0__(\n",
        "        self,\n",
        "        # basic dims ===========\n",
        "        d_model=96,\n",
        "        d_state=16,\n",
        "        ssm_ratio=2.0,\n",
        "        dt_rank=\"auto\",\n",
        "        # ======================\n",
        "        dropout=0.0,\n",
        "        # ======================\n",
        "        seq=False,\n",
        "        force_fp32=True,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        if \"channel_first\" in kwargs:\n",
        "            assert not kwargs[\"channel_first\"]\n",
        "        act_layer = nn.SiLU\n",
        "        dt_min = 0.001\n",
        "        dt_max = 0.1\n",
        "        dt_init = \"random\"\n",
        "        dt_scale = 1.0\n",
        "        dt_init_floor = 1e-4\n",
        "        bias = False\n",
        "        conv_bias = True\n",
        "        d_conv = 3\n",
        "        k_group = 4\n",
        "        factory_kwargs = {\"device\": None, \"dtype\": None}\n",
        "        super().__init__()\n",
        "        d_inner = int(ssm_ratio * d_model)\n",
        "        dt_rank = math.ceil(d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
        "\n",
        "        self.forward = self.forwardv0\n",
        "        if seq:\n",
        "            self.forward = partial(self.forwardv0, seq=True)\n",
        "        if not force_fp32:\n",
        "            self.forward = partial(self.forwardv0, force_fp32=False)\n",
        "\n",
        "        # in proj ============================\n",
        "        self.in_proj = nn.Linear(d_model, d_inner * 2, bias=bias)\n",
        "        self.act: nn.Module = act_layer()\n",
        "        self.conv2d = nn.Conv2d(\n",
        "            in_channels=d_inner,\n",
        "            out_channels=d_inner,\n",
        "            groups=d_inner,\n",
        "            bias=conv_bias,\n",
        "            kernel_size=d_conv,\n",
        "            padding=(d_conv - 1) // 2,\n",
        "            **factory_kwargs,\n",
        "        )\n",
        "\n",
        "        # x proj ============================\n",
        "        self.x_proj = [\n",
        "            nn.Linear(d_inner, (dt_rank + d_state * 2), bias=False)\n",
        "            for _ in range(k_group)\n",
        "        ]\n",
        "        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0)) # (K, N, inner)\n",
        "        del self.x_proj\n",
        "\n",
        "        # dt proj, A, D ============================\n",
        "        self.A_logs, self.Ds, self.dt_projs_weight, self.dt_projs_bias = mamba_init.init_dt_A_D(\n",
        "            d_state, dt_rank, d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, k_group=4,\n",
        "        )\n",
        "\n",
        "        # out proj =======================================\n",
        "        self.out_norm = nn.LayerNorm(d_inner)\n",
        "        self.out_proj = nn.Linear(d_inner, d_model, bias=bias)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n",
        "\n",
        "    def forwardv0(self, x: torch.Tensor, seq=False, force_fp32=True, **kwargs):\n",
        "        x = self.in_proj(x)\n",
        "        x, z = x.chunk(2, dim=-1) # (b, h, w, d)\n",
        "        z = self.act(z)\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        x = self.conv2d(x) # (b, d, h, w)\n",
        "        x = self.act(x)\n",
        "        selective_scan = partial(selective_scan_fn)\n",
        "\n",
        "        B, D, H, W = x.shape\n",
        "        D, N = self.A_logs.shape\n",
        "        K, D, R = self.dt_projs_weight.shape\n",
        "        L = H * W\n",
        "\n",
        "        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)\n",
        "        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)\n",
        "\n",
        "        x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs, self.x_proj_weight)\n",
        "        if hasattr(self, \"x_proj_bias\"):\n",
        "            x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n",
        "        dts, Bs, Cs = torch.split(x_dbl, [R, N, N], dim=2)\n",
        "        dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts, self.dt_projs_weight)\n",
        "\n",
        "        xs = xs.view(B, -1, L) # (b, k * d, l)\n",
        "        dts = dts.contiguous().view(B, -1, L) # (b, k * d, l)\n",
        "        Bs = Bs.contiguous() # (b, k, d_state, l)\n",
        "        Cs = Cs.contiguous() # (b, k, d_state, l)\n",
        "\n",
        "        As = -self.A_logs.float().exp() # (k * d, d_state)\n",
        "        Ds = self.Ds.float() # (k * d)\n",
        "        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)\n",
        "\n",
        "        # assert len(xs.shape) == 3 and len(dts.shape) == 3 and len(Bs.shape) == 4 and len(Cs.shape) == 4\n",
        "        # assert len(As.shape) == 2 and len(Ds.shape) == 1 and len(dt_projs_bias.shape) == 1\n",
        "        to_fp32 = lambda *args: (_a.to(torch.float32) for _a in args)\n",
        "\n",
        "        if force_fp32:\n",
        "            xs, dts, Bs, Cs = to_fp32(xs, dts, Bs, Cs)\n",
        "\n",
        "        if seq:\n",
        "            out_y = []\n",
        "            for i in range(4):\n",
        "                yi = selective_scan(\n",
        "                    xs.view(B, K, -1, L)[:, i], dts.view(B, K, -1, L)[:, i],\n",
        "                    As.view(K, -1, N)[i], Bs[:, i].unsqueeze(1), Cs[:, i].unsqueeze(1), Ds.view(K, -1)[i],\n",
        "                    delta_bias=dt_projs_bias.view(K, -1)[i],\n",
        "                    delta_softplus=True,\n",
        "                ).view(B, -1, L)\n",
        "                out_y.append(yi)\n",
        "            out_y = torch.stack(out_y, dim=1)\n",
        "        else:\n",
        "            out_y = selective_scan(\n",
        "                xs, dts,\n",
        "                As, Bs, Cs, Ds,\n",
        "                delta_bias=dt_projs_bias,\n",
        "                delta_softplus=True,\n",
        "            ).view(B, K, -1, L)\n",
        "        assert out_y.dtype == torch.float\n",
        "\n",
        "        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n",
        "        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
        "        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
        "        y = out_y[:, 0] + inv_y[:, 0] + wh_y + invwh_y\n",
        "\n",
        "        y = y.transpose(dim0=1, dim1=2).contiguous() # (B, L, C)\n",
        "        y = self.out_norm(y).view(B, H, W, -1)\n",
        "\n",
        "        y = y * z\n",
        "        out = self.dropout(self.out_proj(y))\n",
        "        return out\n",
        "\n",
        "\n",
        "# support: v01-v05; v051d,v052d,v052dc;\n",
        "# postfix: _onsigmoid,_onsoftmax,_ondwconv3,_onnone;_nozact,_noz;_oact;_no32;\n",
        "# history support: v2,v3;v31d,v32d,v32dc;\n",
        "class SS2Dv2:\n",
        "    def __initv2__(\n",
        "        self,\n",
        "        # basic dims ===========\n",
        "        d_model=96,\n",
        "        d_state=16,\n",
        "        ssm_ratio=2.0,\n",
        "        dt_rank=\"auto\",\n",
        "        act_layer=nn.SiLU,\n",
        "        # dwconv ===============\n",
        "        d_conv=3, # < 2 means no conv\n",
        "        conv_bias=True,\n",
        "        # ======================\n",
        "        dropout=0.0,\n",
        "        bias=False,\n",
        "        # dt init ==============\n",
        "        dt_min=0.001,\n",
        "        dt_max=0.1,\n",
        "        dt_init=\"random\",\n",
        "        dt_scale=1.0,\n",
        "        dt_init_floor=1e-4,\n",
        "        initialize=\"v0\",\n",
        "        # ======================\n",
        "        forward_type=\"v2\",\n",
        "        channel_first=False,\n",
        "        # ======================\n",
        "        **kwargs,\n",
        "    ):\n",
        "        factory_kwargs = {\"device\": None, \"dtype\": None}\n",
        "        super().__init__()\n",
        "        self.k_group = 4\n",
        "        self.d_model = int(d_model)\n",
        "        self.d_state = int(d_state)\n",
        "        self.d_inner = int(ssm_ratio * d_model)\n",
        "        self.dt_rank = int(math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank)\n",
        "        self.channel_first = channel_first\n",
        "        self.with_dconv = d_conv > 1\n",
        "        Linear = Linear2d if channel_first else nn.Linear\n",
        "        self.forward = self.forwardv2\n",
        "\n",
        "        # tags for forward_type ==============================\n",
        "        checkpostfix = self.checkpostfix\n",
        "        self.disable_force32, forward_type = checkpostfix(\"_no32\", forward_type)\n",
        "        self.oact, forward_type = checkpostfix(\"_oact\", forward_type)\n",
        "        self.disable_z, forward_type = checkpostfix(\"_noz\", forward_type)\n",
        "        self.disable_z_act, forward_type = checkpostfix(\"_nozact\", forward_type)\n",
        "        self.out_norm, forward_type = self.get_outnorm(forward_type, self.d_inner, channel_first)\n",
        "\n",
        "        # forward_type debug =======================================\n",
        "        FORWARD_TYPES = dict(\n",
        "            v01=partial(self.forward_corev2, force_fp32=(not self.disable_force32), selective_scan_backend=\"mamba\", scan_force_torch=True),\n",
        "            v02=partial(self.forward_corev2, force_fp32=(not self.disable_force32), selective_scan_backend=\"mamba\"),\n",
        "            v03=partial(self.forward_corev2, force_fp32=(not self.disable_force32), selective_scan_backend=\"oflex\"),\n",
        "            v04=partial(self.forward_corev2, force_fp32=False), # selective_scan_backend=\"oflex\", scan_mode=\"cross2d\"\n",
        "            v05=partial(self.forward_corev2, force_fp32=False, no_einsum=True),  # selective_scan_backend=\"oflex\", scan_mode=\"cross2d\"\n",
        "            # ===============================\n",
        "            v051d=partial(self.forward_corev2, force_fp32=False, no_einsum=True, scan_mode=\"unidi\"),\n",
        "            v052d=partial(self.forward_corev2, force_fp32=False, no_einsum=True, scan_mode=\"bidi\"),\n",
        "            v052dc=partial(self.forward_corev2, force_fp32=False, no_einsum=True, scan_mode=\"cascade2d\"),\n",
        "            v052d3=partial(self.forward_corev2, force_fp32=False, no_einsum=True, scan_mode=3), # debug\n",
        "            # ===============================\n",
        "            v2=partial(self.forward_corev2, force_fp32=(not self.disable_force32), selective_scan_backend=\"core\"),\n",
        "            v3=partial(self.forward_corev2, force_fp32=False, selective_scan_backend=\"oflex\"),\n",
        "        )\n",
        "        self.forward_core = FORWARD_TYPES.get(forward_type, None)\n",
        "\n",
        "        # in proj =======================================\n",
        "        d_proj = self.d_inner if self.disable_z else (self.d_inner * 2)\n",
        "        self.in_proj = Linear(self.d_model, d_proj, bias=bias)\n",
        "        self.act: nn.Module = act_layer()\n",
        "\n",
        "        # conv =======================================\n",
        "        if self.with_dconv:\n",
        "            self.conv2d = nn.Conv2d(\n",
        "                in_channels=self.d_inner,\n",
        "                out_channels=self.d_inner,\n",
        "                groups=self.d_inner,\n",
        "                bias=conv_bias,\n",
        "                kernel_size=d_conv,\n",
        "                padding=(d_conv - 1) // 2,\n",
        "                **factory_kwargs,\n",
        "            )\n",
        "\n",
        "        # x proj ============================\n",
        "        self.x_proj = [\n",
        "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False)\n",
        "            for _ in range(self.k_group)\n",
        "        ]\n",
        "        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0)) # (K, N, inner)\n",
        "        del self.x_proj\n",
        "\n",
        "        # out proj =======================================\n",
        "        self.out_act = nn.GELU() if self.oact else nn.Identity()\n",
        "        self.out_proj = Linear(self.d_inner, self.d_model, bias=bias)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n",
        "\n",
        "        if initialize in [\"v0\"]:\n",
        "            self.A_logs, self.Ds, self.dt_projs_weight, self.dt_projs_bias = mamba_init.init_dt_A_D(\n",
        "                self.d_state, self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, k_group=self.k_group,\n",
        "            )\n",
        "        elif initialize in [\"v1\"]:\n",
        "            # simple init dt_projs, A_logs, Ds\n",
        "            self.Ds = nn.Parameter(torch.ones((self.k_group * self.d_inner)))\n",
        "            self.A_logs = nn.Parameter(torch.randn((self.k_group * self.d_inner, self.d_state))) # A == -A_logs.exp() < 0; # 0 < exp(A * dt) < 1\n",
        "            self.dt_projs_weight = nn.Parameter(0.1 * torch.randn((self.k_group, self.d_inner, self.dt_rank))) # 0.1 is added in 0430\n",
        "            self.dt_projs_bias = nn.Parameter(0.1 * torch.randn((self.k_group, self.d_inner))) # 0.1 is added in 0430\n",
        "        elif initialize in [\"v2\"]:\n",
        "            # simple init dt_projs, A_logs, Ds\n",
        "            self.Ds = nn.Parameter(torch.ones((self.k_group * self.d_inner)))\n",
        "            self.A_logs = nn.Parameter(torch.zeros((self.k_group * self.d_inner, self.d_state))) # A == -A_logs.exp() < 0; # 0 < exp(A * dt) < 1\n",
        "            self.dt_projs_weight = nn.Parameter(0.1 * torch.rand((self.k_group, self.d_inner, self.dt_rank)))\n",
        "            self.dt_projs_bias = nn.Parameter(0.1 * torch.rand((self.k_group, self.d_inner)))\n",
        "\n",
        "    def forward_corev2(\n",
        "        self,\n",
        "        x: torch.Tensor=None,\n",
        "        # ==============================\n",
        "        force_fp32=False, # True: input fp32\n",
        "        # ==============================\n",
        "        ssoflex=True, # True: input 16 or 32 output 32 False: output dtype as input\n",
        "        no_einsum=False, # replace einsum with linear or conv1d to raise throughput\n",
        "        # ==============================\n",
        "        selective_scan_backend = None,\n",
        "        # ==============================\n",
        "        scan_mode = \"cross2d\",\n",
        "        scan_force_torch = False,\n",
        "        # ==============================\n",
        "        **kwargs,\n",
        "    ):\n",
        "        assert selective_scan_backend in [None, \"oflex\", \"mamba\", \"torch\"]\n",
        "        _scan_mode = dict(cross2d=0, unidi=1, bidi=2, cascade2d=-1).get(scan_mode, None) if isinstance(scan_mode, str) else scan_mode # for debug\n",
        "        assert isinstance(_scan_mode, int)\n",
        "        delta_softplus = True\n",
        "        out_norm = self.out_norm\n",
        "        channel_first = self.channel_first\n",
        "        to_fp32 = lambda *args: (_a.to(torch.float32) for _a in args)\n",
        "\n",
        "        B, D, H, W = x.shape\n",
        "        N = self.d_state\n",
        "        K, D, R = self.k_group, self.d_inner, self.dt_rank\n",
        "        L = H * W\n",
        "\n",
        "        def selective_scan(u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=True):\n",
        "            return selective_scan_fn(u, delta, A, B, C, D, delta_bias, delta_softplus, ssoflex, backend=selective_scan_backend)\n",
        "\n",
        "        if _scan_mode == -1:\n",
        "            x_proj_bias = getattr(self, \"x_proj_bias\", None)\n",
        "            def scan_rowcol(\n",
        "                x: torch.Tensor,\n",
        "                proj_weight: torch.Tensor,\n",
        "                proj_bias: torch.Tensor,\n",
        "                dt_weight: torch.Tensor,\n",
        "                dt_bias: torch.Tensor, # (2*c)\n",
        "                _As: torch.Tensor, # As = -torch.exp(A_logs.to(torch.float))[:2,] # (2*c, d_state)\n",
        "                _Ds: torch.Tensor,\n",
        "                width = True,\n",
        "            ):\n",
        "                # x: (B, D, H, W)\n",
        "                # proj_weight: (2 * D, (R+N+N))\n",
        "                XB, XD, XH, XW = x.shape\n",
        "                if width:\n",
        "                    _B, _D, _L = XB * XH, XD, XW\n",
        "                    xs = x.permute(0, 2, 1, 3).contiguous()\n",
        "                else:\n",
        "                    _B, _D, _L = XB * XW, XD, XH\n",
        "                    xs = x.permute(0, 3, 1, 2).contiguous()\n",
        "                xs = torch.stack([xs, xs.flip(dims=[-1])], dim=2) # (B, H, 2, D, W)\n",
        "                if no_einsum:\n",
        "                    x_dbl = F.conv1d(xs.view(_B, -1, _L), proj_weight.view(-1, _D, 1), bias=(proj_bias.view(-1) if proj_bias is not None else None), groups=2)\n",
        "                    dts, Bs, Cs = torch.split(x_dbl.view(_B, 2, -1, _L), [R, N, N], dim=2)\n",
        "                    dts = F.conv1d(dts.contiguous().view(_B, -1, _L), dt_weight.view(2 * _D, -1, 1), groups=2)\n",
        "                else:\n",
        "                    x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs, proj_weight)\n",
        "                    if x_proj_bias is not None:\n",
        "                        x_dbl = x_dbl + x_proj_bias.view(1, 2, -1, 1)\n",
        "                    dts, Bs, Cs = torch.split(x_dbl, [R, N, N], dim=2)\n",
        "                    dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts, dt_weight)\n",
        "\n",
        "                xs = xs.view(_B, -1, _L)\n",
        "                dts = dts.contiguous().view(_B, -1, _L)\n",
        "                As = _As.view(-1, N).to(torch.float)\n",
        "                Bs = Bs.contiguous().view(_B, 2, N, _L)\n",
        "                Cs = Cs.contiguous().view(_B, 2, N, _L)\n",
        "                Ds = _Ds.view(-1)\n",
        "                delta_bias = dt_bias.view(-1).to(torch.float)\n",
        "\n",
        "                if force_fp32:\n",
        "                    xs = xs.to(torch.float)\n",
        "                dts = dts.to(xs.dtype)\n",
        "                Bs = Bs.to(xs.dtype)\n",
        "                Cs = Cs.to(xs.dtype)\n",
        "\n",
        "                ys: torch.Tensor = selective_scan(\n",
        "                    xs, dts, As, Bs, Cs, Ds, delta_bias, delta_softplus\n",
        "                ).view(_B, 2, -1, _L)\n",
        "                return ys\n",
        "\n",
        "            As = -self.A_logs.to(torch.float).exp().view(4, -1, N)\n",
        "            x = F.layer_norm(x.permute(0, 2, 3, 1), normalized_shape=(int(x.shape[1]),)).permute(0, 3, 1, 2).contiguous() # added0510 to avoid nan\n",
        "            y_row = scan_rowcol(\n",
        "                x,\n",
        "                proj_weight = self.x_proj_weight.view(4, -1, D)[:2].contiguous(),\n",
        "                proj_bias = (x_proj_bias.view(4, -1)[:2].contiguous() if x_proj_bias is not None else None),\n",
        "                dt_weight = self.dt_projs_weight.view(4, D, -1)[:2].contiguous(),\n",
        "                dt_bias = (self.dt_projs_bias.view(4, -1)[:2].contiguous() if self.dt_projs_bias is not None else None),\n",
        "                _As = As[:2].contiguous().view(-1, N),\n",
        "                _Ds = self.Ds.view(4, -1)[:2].contiguous().view(-1),\n",
        "                width=True,\n",
        "            ).view(B, H, 2, -1, W).sum(dim=2).permute(0, 2, 1, 3) # (B,C,H,W)\n",
        "            y_row = F.layer_norm(y_row.permute(0, 2, 3, 1), normalized_shape=(int(y_row.shape[1]),)).permute(0, 3, 1, 2).contiguous() # added0510 to avoid nan\n",
        "            y_col = scan_rowcol(\n",
        "                y_row,\n",
        "                proj_weight = self.x_proj_weight.view(4, -1, D)[2:].contiguous().to(y_row.dtype),\n",
        "                proj_bias = (x_proj_bias.view(4, -1)[2:].contiguous().to(y_row.dtype) if x_proj_bias is not None else None),\n",
        "                dt_weight = self.dt_projs_weight.view(4, D, -1)[2:].contiguous().to(y_row.dtype),\n",
        "                dt_bias = (self.dt_projs_bias.view(4, -1)[2:].contiguous().to(y_row.dtype) if self.dt_projs_bias is not None else None),\n",
        "                _As = As[2:].contiguous().view(-1, N),\n",
        "                _Ds = self.Ds.view(4, -1)[2:].contiguous().view(-1),\n",
        "                width=False,\n",
        "            ).view(B, W, 2, -1, H).sum(dim=2).permute(0, 2, 3, 1)\n",
        "            y = y_col\n",
        "        else:\n",
        "            x_proj_bias = getattr(self, \"x_proj_bias\", None)\n",
        "            xs = cross_scan_fn(x, in_channel_first=True, out_channel_first=True, scans=_scan_mode, force_torch=scan_force_torch)\n",
        "            if no_einsum:\n",
        "                x_dbl = F.conv1d(xs.view(B, -1, L), self.x_proj_weight.view(-1, D, 1), bias=(x_proj_bias.view(-1) if x_proj_bias is not None else None), groups=K)\n",
        "                dts, Bs, Cs = torch.split(x_dbl.view(B, K, -1, L), [R, N, N], dim=2)\n",
        "                if hasattr(self, \"dt_projs_weight\"):\n",
        "                    dts = F.conv1d(dts.contiguous().view(B, -1, L), self.dt_projs_weight.view(K * D, -1, 1), groups=K)\n",
        "            else:\n",
        "                x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs, self.x_proj_weight)\n",
        "                if x_proj_bias is not None:\n",
        "                    x_dbl = x_dbl + x_proj_bias.view(1, K, -1, 1)\n",
        "                dts, Bs, Cs = torch.split(x_dbl, [R, N, N], dim=2)\n",
        "                if hasattr(self, \"dt_projs_weight\"):\n",
        "                    dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts, self.dt_projs_weight)\n",
        "\n",
        "            xs = xs.view(B, -1, L)\n",
        "            dts = dts.contiguous().view(B, -1, L)\n",
        "            As = -self.A_logs.to(torch.float).exp() # (k * c, d_state)\n",
        "            Ds = self.Ds.to(torch.float) # (K * c)\n",
        "            Bs = Bs.contiguous().view(B, K, N, L)\n",
        "            Cs = Cs.contiguous().view(B, K, N, L)\n",
        "            delta_bias = self.dt_projs_bias.view(-1).to(torch.float)\n",
        "\n",
        "            if force_fp32:\n",
        "                xs, dts, Bs, Cs = to_fp32(xs, dts, Bs, Cs)\n",
        "\n",
        "            ys: torch.Tensor = selective_scan(\n",
        "                xs, dts, As, Bs, Cs, Ds, delta_bias, delta_softplus\n",
        "            ).view(B, K, -1, H, W)\n",
        "\n",
        "            y: torch.Tensor = cross_merge_fn(ys, in_channel_first=True, out_channel_first=True, scans=_scan_mode, force_torch=scan_force_torch)\n",
        "\n",
        "            if getattr(self, \"__DEBUG__\", False):\n",
        "                setattr(self, \"__data__\", dict(\n",
        "                    A_logs=self.A_logs, Bs=Bs, Cs=Cs, Ds=Ds,\n",
        "                    us=xs, dts=dts, delta_bias=delta_bias,\n",
        "                    ys=ys, y=y, H=H, W=W,\n",
        "                ))\n",
        "\n",
        "        y = y.view(B, -1, H, W)\n",
        "        if not channel_first:\n",
        "            y = y.view(B, -1, H * W).transpose(dim0=1, dim1=2).contiguous().view(B, H, W, -1) # (B, L, C)\n",
        "        y = out_norm(y)\n",
        "\n",
        "        return y.to(x.dtype)\n",
        "\n",
        "    def forwardv2(self, x: torch.Tensor, **kwargs):\n",
        "        x = self.in_proj(x)\n",
        "        if not self.disable_z:\n",
        "            x, z = x.chunk(2, dim=(1 if self.channel_first else -1)) # (b, h, w, d)\n",
        "            if not self.disable_z_act:\n",
        "                z = self.act(z)\n",
        "        if not self.channel_first:\n",
        "            x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        if self.with_dconv:\n",
        "            x = self.conv2d(x) # (b, d, h, w)\n",
        "        x = self.act(x)\n",
        "        y = self.forward_core(x)\n",
        "        y = self.out_act(y)\n",
        "        if not self.disable_z:\n",
        "            y = y * z\n",
        "        out = self.dropout(self.out_proj(y))\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def get_outnorm(forward_type=\"\", d_inner=192, channel_first=True):\n",
        "        def checkpostfix(tag, value):\n",
        "            ret = value[-len(tag):] == tag\n",
        "            if ret:\n",
        "                value = value[:-len(tag)]\n",
        "            return ret, value\n",
        "\n",
        "        LayerNorm = LayerNorm2d if channel_first else nn.LayerNorm\n",
        "\n",
        "        out_norm_none, forward_type = checkpostfix(\"_onnone\", forward_type)\n",
        "        out_norm_dwconv3, forward_type = checkpostfix(\"_ondwconv3\", forward_type)\n",
        "        out_norm_cnorm, forward_type = checkpostfix(\"_oncnorm\", forward_type)\n",
        "        out_norm_softmax, forward_type = checkpostfix(\"_onsoftmax\", forward_type)\n",
        "        out_norm_sigmoid, forward_type = checkpostfix(\"_onsigmoid\", forward_type)\n",
        "\n",
        "        out_norm = nn.Identity()\n",
        "        if out_norm_none:\n",
        "            out_norm = nn.Identity()\n",
        "        elif out_norm_cnorm:\n",
        "            out_norm = nn.Sequential(\n",
        "                LayerNorm(d_inner),\n",
        "                (nn.Identity() if channel_first else Permute(0, 3, 1, 2)),\n",
        "                nn.Conv2d(d_inner, d_inner, kernel_size=3, padding=1, groups=d_inner, bias=False),\n",
        "                (nn.Identity() if channel_first else Permute(0, 2, 3, 1)),\n",
        "            )\n",
        "        elif out_norm_dwconv3:\n",
        "            out_norm = nn.Sequential(\n",
        "                (nn.Identity() if channel_first else Permute(0, 3, 1, 2)),\n",
        "                nn.Conv2d(d_inner, d_inner, kernel_size=3, padding=1, groups=d_inner, bias=False),\n",
        "                (nn.Identity() if channel_first else Permute(0, 2, 3, 1)),\n",
        "            )\n",
        "        elif out_norm_softmax:\n",
        "            out_norm = SoftmaxSpatial(dim=(-1 if channel_first else 1))\n",
        "        elif out_norm_sigmoid:\n",
        "            out_norm = nn.Sigmoid()\n",
        "        else:\n",
        "            out_norm = LayerNorm(d_inner)\n",
        "\n",
        "        return out_norm, forward_type\n",
        "\n",
        "    @staticmethod\n",
        "    def checkpostfix(tag, value):\n",
        "        ret = value[-len(tag):] == tag\n",
        "        if ret:\n",
        "            value = value[:-len(tag)]\n",
        "        return ret, value\n",
        "\n",
        "\n",
        "# support: xv1a,xv2a,xv3a;\n",
        "# postfix: _cpos;_ocov;_ocov2;_ca,_ca1;_act;_mul;_onsigmoid,_onsoftmax,_ondwconv3,_onnone;\n",
        "class SS2Dv3:\n",
        "    def __initxv__(\n",
        "        self,\n",
        "        # basic dims ===========\n",
        "        d_model=96,\n",
        "        d_state=16,\n",
        "        ssm_ratio=2.0,\n",
        "        dt_rank=\"auto\",\n",
        "        # dwconv ===============\n",
        "        d_conv=3, # < 2 means no conv\n",
        "        conv_bias=True,\n",
        "        # ======================\n",
        "        dropout=0.0,\n",
        "        bias=False,\n",
        "        # dt init ==============\n",
        "        dt_min=0.001,\n",
        "        dt_max=0.1,\n",
        "        dt_init=\"random\",\n",
        "        dt_scale=1.0,\n",
        "        dt_init_floor=1e-4,\n",
        "        initialize=\"v0\",\n",
        "        # ======================\n",
        "        forward_type=\"v2\",\n",
        "        channel_first=False,\n",
        "        # ======================\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        d_inner = int(ssm_ratio * d_model)\n",
        "        dt_rank = math.ceil(d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
        "        self.channel_first = channel_first\n",
        "        self.d_state = d_state\n",
        "        self.dt_rank = dt_rank\n",
        "        self.d_inner = d_inner\n",
        "        k_group = 4\n",
        "        self.with_dconv = d_conv > 1\n",
        "        Linear = Linear2d if channel_first else nn.Linear\n",
        "        self.forward = self.forwardxv\n",
        "\n",
        "        # tags for forward_type ==============================\n",
        "        checkpostfix = SS2Dv2.checkpostfix\n",
        "        self.out_norm, forward_type = SS2Dv2.get_outnorm(forward_type, d_inner, channel_first)\n",
        "        self.omul, forward_type = checkpostfix(\"_mul\", forward_type)\n",
        "        self.oact, forward_type = checkpostfix(\"_act\", forward_type)\n",
        "        self.f_omul = nn.Identity() if self.omul else None\n",
        "        self.out_act = nn.GELU() if self.oact else nn.Identity()\n",
        "\n",
        "        mode = forward_type[:4]\n",
        "        assert mode in [\"xv1a\", \"xv2a\", \"xv3a\"]\n",
        "\n",
        "        self.forward = partial(self.forwardxv, mode=mode)\n",
        "        self.dts_dim = dict(xv1a=self.dt_rank, xv2a=self.d_inner, xv3a=4 * self.dt_rank)[mode]\n",
        "        d_inner_all = d_inner + self.dts_dim + 8 * d_state\n",
        "        self.in_proj = Linear(d_model, d_inner_all, bias=bias)\n",
        "\n",
        "        # conv =======================================\n",
        "        self.cpos = False\n",
        "        self.iconv = False\n",
        "        self.oconv = False\n",
        "        self.oconv2 = False\n",
        "        if self.with_dconv:\n",
        "            cact, forward_type = checkpostfix(\"_ca\", forward_type)\n",
        "            cact1, forward_type = checkpostfix(\"_ca1\", forward_type)\n",
        "            self.cact = nn.SiLU() if cact else nn.Identity()\n",
        "            self.cact = nn.GELU() if cact1 else self.cact\n",
        "\n",
        "            self.oconv2, forward_type = checkpostfix(\"_ocov2\", forward_type)\n",
        "            self.oconv, forward_type = checkpostfix(\"_ocov\", forward_type)\n",
        "            self.cpos, forward_type = checkpostfix(\"_cpos\", forward_type)\n",
        "            self.iconv = (not self.oconv) and (not self.oconv2)\n",
        "\n",
        "            if self.iconv:\n",
        "                self.conv2d = nn.Conv2d(\n",
        "                    in_channels=d_model,\n",
        "                    out_channels=d_model,\n",
        "                    groups=d_model,\n",
        "                    bias=conv_bias,\n",
        "                    kernel_size=d_conv,\n",
        "                    padding=(d_conv - 1) // 2,\n",
        "                )\n",
        "            if self.oconv:\n",
        "                self.oconv2d = nn.Conv2d(\n",
        "                    in_channels=d_inner,\n",
        "                    out_channels=d_inner,\n",
        "                    groups=d_inner,\n",
        "                    bias=conv_bias,\n",
        "                    kernel_size=d_conv,\n",
        "                    padding=(d_conv - 1) // 2,\n",
        "                )\n",
        "            if self.oconv2:\n",
        "                self.conv2d = nn.Conv2d(\n",
        "                    in_channels=d_inner_all,\n",
        "                    out_channels=d_inner_all,\n",
        "                    groups=d_inner_all,\n",
        "                    bias=conv_bias,\n",
        "                    kernel_size=d_conv,\n",
        "                    padding=(d_conv - 1) // 2,\n",
        "                )\n",
        "\n",
        "        # out proj =======================================\n",
        "        self.out_proj = Linear(d_inner, d_model, bias=bias)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
        "\n",
        "        if initialize in [\"v0\"]:\n",
        "            self.A_logs, self.Ds, self.dt_projs_weight, self.dt_projs_bias = mamba_init.init_dt_A_D(\n",
        "                d_state, dt_rank, d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, k_group=4,\n",
        "            )\n",
        "        elif initialize in [\"v1\"]:\n",
        "            # simple init dt_projs, A_logs, Ds\n",
        "            self.Ds = nn.Parameter(torch.ones((k_group * d_inner)))\n",
        "            self.A_logs = nn.Parameter(torch.randn((k_group * d_inner, d_state))) # A == -A_logs.exp() < 0; # 0 < exp(A * dt) < 1\n",
        "            self.dt_projs_weight = nn.Parameter(torch.randn((k_group, d_inner, dt_rank)))\n",
        "            self.dt_projs_bias = nn.Parameter(torch.randn((k_group, d_inner)))\n",
        "        elif initialize in [\"v2\"]:\n",
        "            # simple init dt_projs, A_logs, Ds\n",
        "            self.Ds = nn.Parameter(torch.ones((k_group * d_inner)))\n",
        "            self.A_logs = nn.Parameter(torch.zeros((k_group * d_inner, d_state))) # A == -A_logs.exp() < 0; # 0 < exp(A * dt) < 1\n",
        "            self.dt_projs_weight = nn.Parameter(0.1 * torch.rand((k_group, d_inner, dt_rank)))\n",
        "            self.dt_projs_bias = nn.Parameter(0.1 * torch.rand((k_group, d_inner)))\n",
        "\n",
        "\n",
        "        if forward_type.startswith(\"xv2\"):\n",
        "            del self.dt_projs_weight\n",
        "            self.dt_projs_weight = None\n",
        "\n",
        "    def forwardxv(self, x: torch.Tensor, **kwargs):\n",
        "        B, (H, W) = x.shape[0], (x.shape[2:4] if self.channel_first else x.shape[1:3])\n",
        "        L = H * W\n",
        "        force_fp32 = False\n",
        "        delta_softplus = True\n",
        "        out_norm = self.out_norm\n",
        "        to_dtype = True\n",
        "\n",
        "        to_fp32 = lambda *args: (_a.to(torch.float32) for _a in args)\n",
        "\n",
        "        def selective_scan(u, delta, A, B, C, D, delta_bias, delta_softplus):\n",
        "            return selective_scan_fn(u, delta, A, B, C, D, delta_bias, delta_softplus, oflex=True, backend=None)\n",
        "\n",
        "        if self.iconv:\n",
        "            x = self.cact(self.conv2d(x)) # (b, d, h, w)\n",
        "        elif self.cpos:\n",
        "            x = x + self.conv2d(x) # (b, d, h, w)\n",
        "\n",
        "        x = self.in_proj(x)\n",
        "\n",
        "        if self.oconv2:\n",
        "            x = self.conv2d(x) # (b, d, h, w)\n",
        "\n",
        "        us, dts, Bs, Cs = x.split([self.d_inner, self.dts_dim, 4 * self.d_state, 4 * self.d_state], dim=(1 if self.channel_first else -1))\n",
        "\n",
        "        _us = us\n",
        "        # Bs, Cs = Bs.view(B, H, W, 4, -1), Cs.view(B, H, W, 4, -1)\n",
        "        # Bs, Cs = Bs.view(B, 4, -1, H, W), Cs.view(B, 4, -1, H, W)\n",
        "        us = cross_scan_fn(us.contiguous(), in_channel_first=self.channel_first, out_channel_first=True).view(B, -1, L)\n",
        "        Bs = cross_scan_fn(Bs.contiguous(), in_channel_first=self.channel_first, out_channel_first=True, one_by_one=True).view(B, 4, -1, L)\n",
        "        Cs = cross_scan_fn(Cs.contiguous(), in_channel_first=self.channel_first, out_channel_first=True, one_by_one=True).view(B, 4, -1, L)\n",
        "        dts = cross_scan_fn(dts.contiguous(), in_channel_first=self.channel_first, out_channel_first=True, one_by_one=(self.dts_dim == 4 * self.dt_rank)).view(B, L, -1)\n",
        "        if self.dts_dim == self.dt_rank:\n",
        "            dts = F.conv1d(dts, self.dt_projs_weight.view(4 * self.d_inner, self.dt_rank, 1), None, groups=4)\n",
        "        elif self.dts_dim == 4 * self.dt_rank:\n",
        "            dts = F.conv1d(dts, self.dt_projs_weight.view(4 * self.d_inner, self.dt_rank, 1), None, groups=4)\n",
        "\n",
        "        As = -self.A_logs.to(torch.float).exp() # (k * c, d_state)\n",
        "        Ds = self.Ds.to(torch.float) # (K * c)\n",
        "        delta_bias = self.dt_projs_bias.view(-1).to(torch.float) # (K * c)\n",
        "\n",
        "        if force_fp32:\n",
        "            us, dts, Bs, Cs = to_fp32(us, dts, Bs, Cs)\n",
        "\n",
        "        ys: torch.Tensor = selective_scan(\n",
        "            us, dts, As, Bs, Cs, Ds, delta_bias, delta_softplus\n",
        "        ).view(B, 4, -1, H, W)\n",
        "        y: torch.Tensor = cross_merge_fn(ys.contiguous(), in_channel_first=self.channel_first, out_channel_first=True)\n",
        "        y = y.view(B, -1, H, W) if self.channel_first else y.view(B, H, W, -1)\n",
        "        y = out_norm(y)\n",
        "\n",
        "        if getattr(self, \"__DEBUG__\", False):\n",
        "            setattr(self, \"__data__\", dict(\n",
        "                A_logs=self.A_logs, Bs=Bs, Cs=Cs, Ds=Ds,\n",
        "                us=us, dts=dts, delta_bias=delta_bias,\n",
        "                ys=ys, y=y,\n",
        "            ))\n",
        "\n",
        "        y = (y.to(x.dtype) if to_dtype else y)\n",
        "\n",
        "        y = self.out_act(y)\n",
        "\n",
        "        if self.omul:\n",
        "            y = y * _us\n",
        "\n",
        "        if self.oconv:\n",
        "            y = y + self.cact(self.oconv2d(_us))\n",
        "\n",
        "        out = self.dropout(self.out_proj(y))\n",
        "        return out\n",
        "\n",
        "\n",
        "# mamba2 support ================================\n",
        "class SS2Dm0:\n",
        "    def __initm0__(\n",
        "        self,\n",
        "        # basic dims ===========\n",
        "        d_model=96,\n",
        "        d_state=16, # now with mamba2, dstate should be bigger...\n",
        "        ssm_ratio=2.0,\n",
        "        dt_rank=\"auto\",\n",
        "        act_layer=nn.GELU,\n",
        "        # dwconv ===============\n",
        "        d_conv=3, # < 2 means no conv\n",
        "        conv_bias=True,\n",
        "        # ======================\n",
        "        dropout=0.0,\n",
        "        bias=False,\n",
        "        # dt init ==============\n",
        "        dt_min=0.001,\n",
        "        dt_max=0.1,\n",
        "        dt_init=\"random\",\n",
        "        dt_scale=1.0,\n",
        "        dt_init_floor=1e-4,\n",
        "        initialize=\"v2\",\n",
        "        # ======================\n",
        "        forward_type=\"m0\",\n",
        "        # ======================\n",
        "        with_initial_state=False,\n",
        "        # ======================\n",
        "        **kwargs,\n",
        "    ):\n",
        "        factory_kwargs = {\"device\": None, \"dtype\": None}\n",
        "        super().__init__()\n",
        "        d_inner = int(ssm_ratio * d_model)\n",
        "        dt_rank = math.ceil(d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
        "        assert d_inner % dt_rank == 0\n",
        "        self.with_dconv = d_conv > 1\n",
        "        Linear = nn.Linear\n",
        "        self.forward = self.forwardm0\n",
        "\n",
        "        # tags for forward_type ==============================\n",
        "        checkpostfix = SS2Dv2.checkpostfix\n",
        "        self.disable_force32, forward_type = checkpostfix(\"_no32\", forward_type)\n",
        "        self.oact, forward_type = checkpostfix(\"_oact\", forward_type)\n",
        "        self.disable_z, forward_type = checkpostfix(\"_noz\", forward_type)\n",
        "        self.disable_z_act, forward_type = checkpostfix(\"_nozact\", forward_type)\n",
        "        self.out_norm, forward_type = SS2Dv2.get_outnorm(forward_type, d_inner, False)\n",
        "\n",
        "        # forward_type debug =======================================\n",
        "        FORWARD_TYPES = dict(\n",
        "            m0=partial(self.forward_corem0, force_fp32=False, dstate=d_state),\n",
        "        )\n",
        "        self.forward_core = FORWARD_TYPES.get(forward_type, None)\n",
        "        k_group = 4\n",
        "\n",
        "        # in proj =======================================\n",
        "        d_proj = d_inner if self.disable_z else (d_inner * 2)\n",
        "        self.in_proj = Linear(d_model, d_proj, bias=bias)\n",
        "        self.act: nn.Module = act_layer()\n",
        "\n",
        "        # conv =======================================\n",
        "        if self.with_dconv:\n",
        "            self.conv2d = nn.Sequential(\n",
        "                Permute(0, 3, 1, 2),\n",
        "                nn.Conv2d(\n",
        "                    in_channels=d_inner,\n",
        "                    out_channels=d_inner,\n",
        "                    groups=d_inner,\n",
        "                    bias=conv_bias,\n",
        "                    kernel_size=d_conv,\n",
        "                    padding=(d_conv - 1) // 2,\n",
        "                    **factory_kwargs,\n",
        "                ),\n",
        "                Permute(0, 2, 3, 1),\n",
        "            )\n",
        "\n",
        "        # x proj ============================\n",
        "        self.x_proj = [\n",
        "            nn.Linear(d_inner, (dt_rank + d_state * 2), bias=False)\n",
        "            for _ in range(k_group)\n",
        "        ]\n",
        "        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0)) # (K, N, inner)\n",
        "        del self.x_proj\n",
        "\n",
        "        # out proj =======================================\n",
        "        self.out_act = nn.GELU() if self.oact else nn.Identity()\n",
        "        self.out_proj = Linear(d_inner, d_model, bias=bias)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n",
        "\n",
        "        if initialize in [\"v1\"]:\n",
        "            # simple init dt_projs, A_logs, Ds\n",
        "            self.Ds = nn.Parameter(torch.ones((k_group, dt_rank, int(d_inner // dt_rank))))\n",
        "            self.A_logs = nn.Parameter(torch.randn((k_group, dt_rank))) # A == -A_logs.exp() < 0; # 0 < exp(A * dt) < 1\n",
        "            self.dt_projs_bias = nn.Parameter(0.1 * torch.randn((k_group, dt_rank))) # 0.1 is added in 0430\n",
        "        elif initialize in [\"v2\"]:\n",
        "            # simple init dt_projs, A_logs, Ds\n",
        "            self.Ds = nn.Parameter(torch.ones((k_group, dt_rank, int(d_inner // dt_rank))))\n",
        "            self.A_logs = nn.Parameter(torch.zeros((k_group, dt_rank))) # A == -A_logs.exp() < 0; # 0 < exp(A * dt) < 1\n",
        "            self.dt_projs_bias = nn.Parameter(0.1 * torch.rand((k_group, dt_rank)))\n",
        "\n",
        "        # init state ============================\n",
        "        self.initial_state = None\n",
        "        if with_initial_state:\n",
        "            self.initial_state = nn.Parameter(torch.zeros((1, k_group * dt_rank, int(d_inner // dt_rank), d_state)), requires_grad=False)\n",
        "\n",
        "    def forward_corem0(\n",
        "        self,\n",
        "        x: torch.Tensor=None,\n",
        "        # ==============================\n",
        "        force_fp32=False, # True: input fp32\n",
        "        chunk_size = 64,\n",
        "        dstate = 64,\n",
        "        # ==============================\n",
        "        selective_scan_backend = None,\n",
        "        scan_mode = \"cross2d\",\n",
        "        scan_force_torch = False,\n",
        "        # ==============================\n",
        "        **kwargs,\n",
        "    ):\n",
        "        assert scan_mode in [\"unidi\", \"bidi\", \"cross2d\"]\n",
        "        assert selective_scan_backend in [None, \"triton\", \"torch\"]\n",
        "        x_proj_bias = getattr(self, \"x_proj_bias\", None)\n",
        "        to_fp32 = lambda *args: (_a.to(torch.float32) for _a in args)\n",
        "\n",
        "        N = dstate\n",
        "        B, H, W, RD = x.shape\n",
        "        K, R = self.A_logs.shape\n",
        "        K, R, D = self.Ds.shape\n",
        "        assert RD == R * D\n",
        "        L = H * W\n",
        "        KR = K * R\n",
        "        _scan_mode = dict(cross2d=0, unidi=1, bidi=2, cascade2d=3)[scan_mode]\n",
        "\n",
        "        initial_state = None\n",
        "        if self.initial_state is not None:\n",
        "            assert self.initial_state.shape[-1] == dstate\n",
        "            initial_state = self.initial_state.detach().repeat(B, 1, 1, 1)\n",
        "        xs = cross_scan_fn(x.view(B, H, W, RD), in_channel_first=False, out_channel_first=False, scans=_scan_mode, force_torch=scan_force_torch) # (B, H, W, 4, D)\n",
        "        x_dbl = torch.einsum(\"b l k d, k c d -> b l k c\", xs, self.x_proj_weight)\n",
        "        if x_proj_bias is not None:\n",
        "            x_dbl = x_dbl + x_proj_bias.view(1, -1, K, 1)\n",
        "        dts, Bs, Cs = torch.split(x_dbl, [R, N, N], dim=3)\n",
        "        xs = xs.contiguous().view(B, L, KR, D)\n",
        "        dts = dts.contiguous().view(B, L, KR)\n",
        "        Bs = Bs.contiguous().view(B, L, K, N)\n",
        "        Cs = Cs.contiguous().view(B, L, K, N)\n",
        "        if force_fp32:\n",
        "            xs, dts, Bs, Cs = to_fp32(xs, dts, Bs, Cs)\n",
        "\n",
        "        As = -self.A_logs.to(torch.float).exp().view(KR)\n",
        "        Ds = self.Ds.to(torch.float).view(KR, D)\n",
        "        dt_bias = self.dt_projs_bias.view(KR)\n",
        "\n",
        "        if force_fp32:\n",
        "            xs, dts, Bs, Cs = to_fp32(xs, dts, Bs, Cs)\n",
        "\n",
        "        ys, final_state = selective_scan_chunk_fn(\n",
        "            xs, dts, As, Bs, Cs, chunk_size=chunk_size, D=Ds, dt_bias=dt_bias,\n",
        "            initial_states=initial_state, dt_softplus=True, return_final_states=True,\n",
        "            backend=selective_scan_backend,\n",
        "        )\n",
        "        y: torch.Tensor = cross_merge_fn(ys.view(B, H, W, K, RD), in_channel_first=False, out_channel_first=False, scans=_scan_mode, force_torch=scan_force_torch)\n",
        "\n",
        "        if getattr(self, \"__DEBUG__\", False):\n",
        "            setattr(self, \"__data__\", dict(\n",
        "                A_logs=self.A_logs, Bs=Bs, Cs=Cs, Ds=self.Ds,\n",
        "                us=xs, dts=dts, delta_bias=self.dt_projs_bias,\n",
        "                initial_state=self.initial_state, final_satte=final_state,\n",
        "                ys=ys, y=y, H=H, W=W,\n",
        "            ))\n",
        "        if self.initial_state is not None:\n",
        "            self.initial_state = nn.Parameter(final_state.detach().sum(0, keepdim=True), requires_grad=False)\n",
        "\n",
        "        y = self.out_norm(y.view(B, H, W, -1))\n",
        "\n",
        "        return y.to(x.dtype)\n",
        "\n",
        "    def forwardm0(self, x: torch.Tensor, **kwargs):\n",
        "        x = self.in_proj(x)\n",
        "        if not self.disable_z:\n",
        "            x, z = x.chunk(2, dim=(1 if self.channel_first else -1)) # (b, h, w, d)\n",
        "            if not self.disable_z_act:\n",
        "                z = self.act(z)\n",
        "        if self.with_dconv:\n",
        "            x = self.conv2d(x) # (b, d, h, w)\n",
        "        x = self.act(x)\n",
        "        y = self.forward_core(x)\n",
        "        y = self.out_act(y)\n",
        "        if not self.disable_z:\n",
        "            y = y * z\n",
        "        out = self.dropout(self.out_proj(y))\n",
        "        return out\n",
        "\n",
        "\n",
        "class SS2D(nn.Module, SS2Dv0, SS2Dv2, SS2Dv3, SS2Dm0):\n",
        "    def __init__(\n",
        "        self,\n",
        "        # basic dims ===========\n",
        "        d_model=96,\n",
        "        d_state=16,\n",
        "        ssm_ratio=2.0,\n",
        "        dt_rank=\"auto\",\n",
        "        act_layer=nn.SiLU,\n",
        "        # dwconv ===============\n",
        "        d_conv=3, # < 2 means no conv\n",
        "        conv_bias=True,\n",
        "        # ======================\n",
        "        dropout=0.0,\n",
        "        bias=False,\n",
        "        # dt init ==============\n",
        "        dt_min=0.001,\n",
        "        dt_max=0.1,\n",
        "        dt_init=\"random\",\n",
        "        dt_scale=1.0,\n",
        "        dt_init_floor=1e-4,\n",
        "        initialize=\"v0\",\n",
        "        # ======================\n",
        "        forward_type=\"v2\",\n",
        "        channel_first=False,\n",
        "        # ======================\n",
        "        **kwargs,\n",
        "    ):\n",
        "        nn.Module.__init__(self)\n",
        "        kwargs.update(\n",
        "            d_model=d_model, d_state=d_state, ssm_ratio=ssm_ratio, dt_rank=dt_rank,\n",
        "            act_layer=act_layer, d_conv=d_conv, conv_bias=conv_bias, dropout=dropout, bias=bias,\n",
        "            dt_min=dt_min, dt_max=dt_max, dt_init=dt_init, dt_scale=dt_scale, dt_init_floor=dt_init_floor,\n",
        "            initialize=initialize, forward_type=forward_type, channel_first=channel_first,\n",
        "        )\n",
        "        if forward_type in [\"v0\", \"v0seq\"]:\n",
        "            self.__initv0__(seq=(\"seq\" in forward_type), **kwargs)\n",
        "        elif forward_type.startswith(\"xv\"):\n",
        "            self.__initxv__(**kwargs)\n",
        "        elif forward_type.startswith(\"m\"):\n",
        "            self.__initm0__(**kwargs)\n",
        "        else:\n",
        "            self.__initv2__(**kwargs)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "class VSSBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim: int = 0,\n",
        "        drop_path: float = 0,\n",
        "        norm_layer: nn.Module = nn.LayerNorm,\n",
        "        channel_first=False,\n",
        "        # =============================\n",
        "        ssm_d_state: int = 16,\n",
        "        ssm_ratio=2.0,\n",
        "        ssm_dt_rank: Any = \"auto\",\n",
        "        ssm_act_layer=nn.SiLU,\n",
        "        ssm_conv: int = 3,\n",
        "        ssm_conv_bias=True,\n",
        "        ssm_drop_rate: float = 0,\n",
        "        ssm_init=\"v0\",\n",
        "        forward_type=\"v2\",\n",
        "        # =============================\n",
        "        mlp_ratio=4.0,\n",
        "        mlp_act_layer=nn.GELU,\n",
        "        mlp_drop_rate: float = 0.0,\n",
        "        gmlp=False,\n",
        "        # =============================\n",
        "        use_checkpoint: bool = False,\n",
        "        post_norm: bool = False,\n",
        "        # =============================\n",
        "        _SS2D: type = SS2D,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ssm_branch = ssm_ratio > 0\n",
        "        self.mlp_branch = mlp_ratio > 0\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.post_norm = post_norm\n",
        "\n",
        "        if self.ssm_branch:\n",
        "            self.norm = norm_layer(hidden_dim)\n",
        "            self.op = _SS2D(\n",
        "                d_model=hidden_dim,\n",
        "                d_state=ssm_d_state,\n",
        "                ssm_ratio=ssm_ratio,\n",
        "                dt_rank=ssm_dt_rank,\n",
        "                act_layer=ssm_act_layer,\n",
        "                # ==========================\n",
        "                d_conv=ssm_conv,\n",
        "                conv_bias=ssm_conv_bias,\n",
        "                # ==========================\n",
        "                dropout=ssm_drop_rate,\n",
        "                # bias=False,\n",
        "                # ==========================\n",
        "                # dt_min=0.001,\n",
        "                # dt_max=0.1,\n",
        "                # dt_init=\"random\",\n",
        "                # dt_scale=\"random\",\n",
        "                # dt_init_floor=1e-4,\n",
        "                initialize=ssm_init,\n",
        "                # ==========================\n",
        "                forward_type=forward_type,\n",
        "                channel_first=channel_first,\n",
        "            )\n",
        "\n",
        "        self.drop_path = DropPath(drop_path)\n",
        "\n",
        "        if self.mlp_branch:\n",
        "            _MLP = Mlp if not gmlp else gMlp\n",
        "            self.norm2 = norm_layer(hidden_dim)\n",
        "            mlp_hidden_dim = int(hidden_dim * mlp_ratio)\n",
        "            self.mlp = _MLP(in_features=hidden_dim, hidden_features=mlp_hidden_dim, act_layer=mlp_act_layer, drop=mlp_drop_rate, channels_first=channel_first)\n",
        "\n",
        "    def _forward(self, input: torch.Tensor):\n",
        "        x = input\n",
        "        if self.ssm_branch:\n",
        "            if self.post_norm:\n",
        "                x = x + self.drop_path(self.norm(self.op(x)))\n",
        "            else:\n",
        "                x = x + self.drop_path(self.op(self.norm(x)))\n",
        "        if self.mlp_branch:\n",
        "            if self.post_norm:\n",
        "                x = x + self.drop_path(self.norm2(self.mlp(x))) # FFN\n",
        "            else:\n",
        "                x = x + self.drop_path(self.mlp(self.norm2(x))) # FFN\n",
        "        return x\n",
        "\n",
        "    def forward(self, input: torch.Tensor):\n",
        "        if self.use_checkpoint:\n",
        "            return checkpoint.checkpoint(self._forward, input)\n",
        "        else:\n",
        "            return self._forward(input)\n",
        "\n",
        "\n",
        "class VSSM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        patch_size=4,\n",
        "        in_chans=3,\n",
        "        num_classes=1000,\n",
        "        depths=[2, 2, 9, 2],\n",
        "        dims=[96, 192, 384, 768],\n",
        "        # =========================\n",
        "        ssm_d_state=16,\n",
        "        ssm_ratio=2.0,\n",
        "        ssm_dt_rank=\"auto\",\n",
        "        ssm_act_layer=\"silu\",\n",
        "        ssm_conv=3,\n",
        "        ssm_conv_bias=True,\n",
        "        ssm_drop_rate=0.0,\n",
        "        ssm_init=\"v0\",\n",
        "        forward_type=\"v2\",\n",
        "        # =========================\n",
        "        mlp_ratio=4.0,\n",
        "        mlp_act_layer=\"gelu\",\n",
        "        mlp_drop_rate=0.0,\n",
        "        gmlp=False,\n",
        "        # =========================\n",
        "        drop_path_rate=0.1,\n",
        "        patch_norm=True,\n",
        "        norm_layer=\"LN\", # \"BN\", \"LN2D\"\n",
        "        downsample_version: str = \"v2\", # \"v1\", \"v2\", \"v3\"\n",
        "        patchembed_version: str = \"v1\", # \"v1\", \"v2\"\n",
        "        use_checkpoint=False,\n",
        "        # =========================\n",
        "        posembed=False,\n",
        "        imgsize=224,\n",
        "        _SS2D=SS2D,\n",
        "        # =========================\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channel_first = (norm_layer.lower() in [\"bn\", \"ln2d\"])\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = len(depths)\n",
        "        if isinstance(dims, int):\n",
        "            dims = [int(dims * 2 ** i_layer) for i_layer in range(self.num_layers)]\n",
        "        self.num_features = dims[-1]\n",
        "        self.dims = dims\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
        "\n",
        "        _NORMLAYERS = dict(\n",
        "            ln=nn.LayerNorm,\n",
        "            ln2d=LayerNorm2d,\n",
        "            bn=nn.BatchNorm2d,\n",
        "        )\n",
        "\n",
        "        _ACTLAYERS = dict(\n",
        "            silu=nn.SiLU,\n",
        "            gelu=nn.GELU,\n",
        "            relu=nn.ReLU,\n",
        "            sigmoid=nn.Sigmoid,\n",
        "        )\n",
        "\n",
        "        norm_layer: nn.Module = _NORMLAYERS.get(norm_layer.lower(), None)\n",
        "        ssm_act_layer: nn.Module = _ACTLAYERS.get(ssm_act_layer.lower(), None)\n",
        "        mlp_act_layer: nn.Module = _ACTLAYERS.get(mlp_act_layer.lower(), None)\n",
        "\n",
        "        self.pos_embed = self._pos_embed(dims[0], patch_size, imgsize) if posembed else None\n",
        "\n",
        "        _make_patch_embed = dict(\n",
        "            v1=self._make_patch_embed,\n",
        "            v2=self._make_patch_embed_v2,\n",
        "        ).get(patchembed_version, None)\n",
        "        self.patch_embed = _make_patch_embed(in_chans, dims[0], patch_size, patch_norm, norm_layer, channel_first=self.channel_first)\n",
        "\n",
        "        _make_downsample = dict(\n",
        "            v1=PatchMerging2D,\n",
        "            v2=self._make_downsample,\n",
        "            v3=self._make_downsample_v3,\n",
        "            none=(lambda *_, **_k: None),\n",
        "        ).get(downsample_version, None)\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            downsample = _make_downsample(\n",
        "                self.dims[i_layer],\n",
        "                self.dims[i_layer + 1],\n",
        "                norm_layer=norm_layer,\n",
        "                channel_first=self.channel_first,\n",
        "            ) if (i_layer < self.num_layers - 1) else nn.Identity()\n",
        "\n",
        "            self.layers.append(self._make_layer(\n",
        "                dim = self.dims[i_layer],\n",
        "                drop_path = dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                norm_layer=norm_layer,\n",
        "                downsample=downsample,\n",
        "                channel_first=self.channel_first,\n",
        "                # =================\n",
        "                ssm_d_state=ssm_d_state,\n",
        "                ssm_ratio=ssm_ratio,\n",
        "                ssm_dt_rank=ssm_dt_rank,\n",
        "                ssm_act_layer=ssm_act_layer,\n",
        "                ssm_conv=ssm_conv,\n",
        "                ssm_conv_bias=ssm_conv_bias,\n",
        "                ssm_drop_rate=ssm_drop_rate,\n",
        "                ssm_init=ssm_init,\n",
        "                forward_type=forward_type,\n",
        "                # =================\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                mlp_act_layer=mlp_act_layer,\n",
        "                mlp_drop_rate=mlp_drop_rate,\n",
        "                gmlp=gmlp,\n",
        "                # =================\n",
        "                _SS2D=_SS2D,\n",
        "            ))\n",
        "\n",
        "        self.classifier = nn.Sequential(OrderedDict(\n",
        "            norm=norm_layer(self.num_features), # B,H,W,C\n",
        "            permute=(Permute(0, 3, 1, 2) if not self.channel_first else nn.Identity()),\n",
        "            avgpool=nn.AdaptiveAvgPool2d(1),\n",
        "            flatten=nn.Flatten(1),\n",
        "            head=nn.Linear(self.num_features, num_classes),\n",
        "        ))\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    @staticmethod\n",
        "    def _pos_embed(embed_dims, patch_size, img_size):\n",
        "        patch_height, patch_width = (img_size // patch_size, img_size // patch_size)\n",
        "        pos_embed = nn.Parameter(torch.zeros(1, embed_dims, patch_height, patch_width))\n",
        "        trunc_normal_(pos_embed, std=0.02)\n",
        "        return pos_embed\n",
        "\n",
        "    def _init_weights(self, m: nn.Module):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    # used in building optimizer\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {\"pos_embed\"}\n",
        "\n",
        "    # used in building optimizer\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        return {}\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_patch_embed(in_chans=3, embed_dim=96, patch_size=4, patch_norm=True, norm_layer=nn.LayerNorm, channel_first=False):\n",
        "        # if channel first, then Norm and Output are both channel_first\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=True),\n",
        "            (nn.Identity() if channel_first else Permute(0, 2, 3, 1)),\n",
        "            (norm_layer(embed_dim) if patch_norm else nn.Identity()),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_patch_embed_v2(in_chans=3, embed_dim=96, patch_size=4, patch_norm=True, norm_layer=nn.LayerNorm, channel_first=False):\n",
        "        # if channel first, then Norm and Output are both channel_first\n",
        "        stride = patch_size // 2\n",
        "        kernel_size = stride + 1\n",
        "        padding = 1\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_chans, embed_dim // 2, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            (nn.Identity() if (channel_first or (not patch_norm)) else Permute(0, 2, 3, 1)),\n",
        "            (norm_layer(embed_dim // 2) if patch_norm else nn.Identity()),\n",
        "            (nn.Identity() if (channel_first or (not patch_norm)) else Permute(0, 3, 1, 2)),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            (nn.Identity() if channel_first else Permute(0, 2, 3, 1)),\n",
        "            (norm_layer(embed_dim) if patch_norm else nn.Identity()),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_downsample(dim=96, out_dim=192, norm_layer=nn.LayerNorm, channel_first=False):\n",
        "        # if channel first, then Norm and Output are both channel_first\n",
        "        return nn.Sequential(\n",
        "            (nn.Identity() if channel_first else Permute(0, 3, 1, 2)),\n",
        "            nn.Conv2d(dim, out_dim, kernel_size=2, stride=2),\n",
        "            (nn.Identity() if channel_first else Permute(0, 2, 3, 1)),\n",
        "            norm_layer(out_dim),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_downsample_v3(dim=96, out_dim=192, norm_layer=nn.LayerNorm, channel_first=False):\n",
        "        # if channel first, then Norm and Output are both channel_first\n",
        "        return nn.Sequential(\n",
        "            (nn.Identity() if channel_first else Permute(0, 3, 1, 2)),\n",
        "            nn.Conv2d(dim, out_dim, kernel_size=3, stride=2, padding=1),\n",
        "            (nn.Identity() if channel_first else Permute(0, 2, 3, 1)),\n",
        "            norm_layer(out_dim),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_layer(\n",
        "        dim=96,\n",
        "        drop_path=[0.1, 0.1],\n",
        "        use_checkpoint=False,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        downsample=nn.Identity(),\n",
        "        channel_first=False,\n",
        "        # ===========================\n",
        "        ssm_d_state=16,\n",
        "        ssm_ratio=2.0,\n",
        "        ssm_dt_rank=\"auto\",\n",
        "        ssm_act_layer=nn.SiLU,\n",
        "        ssm_conv=3,\n",
        "        ssm_conv_bias=True,\n",
        "        ssm_drop_rate=0.0,\n",
        "        ssm_init=\"v0\",\n",
        "        forward_type=\"v2\",\n",
        "        # ===========================\n",
        "        mlp_ratio=4.0,\n",
        "        mlp_act_layer=nn.GELU,\n",
        "        mlp_drop_rate=0.0,\n",
        "        gmlp=False,\n",
        "        # ===========================\n",
        "        _SS2D=SS2D,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # if channel first, then Norm and Output are both channel_first\n",
        "        depth = len(drop_path)\n",
        "        blocks = []\n",
        "        for d in range(depth):\n",
        "            blocks.append(VSSBlock(\n",
        "                hidden_dim=dim,\n",
        "                drop_path=drop_path[d],\n",
        "                norm_layer=norm_layer,\n",
        "                channel_first=channel_first,\n",
        "                ssm_d_state=ssm_d_state,\n",
        "                ssm_ratio=ssm_ratio,\n",
        "                ssm_dt_rank=ssm_dt_rank,\n",
        "                ssm_act_layer=ssm_act_layer,\n",
        "                ssm_conv=ssm_conv,\n",
        "                ssm_conv_bias=ssm_conv_bias,\n",
        "                ssm_drop_rate=ssm_drop_rate,\n",
        "                ssm_init=ssm_init,\n",
        "                forward_type=forward_type,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                mlp_act_layer=mlp_act_layer,\n",
        "                mlp_drop_rate=mlp_drop_rate,\n",
        "                gmlp=gmlp,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                _SS2D=_SS2D,\n",
        "            ))\n",
        "\n",
        "        return nn.Sequential(OrderedDict(\n",
        "            blocks=nn.Sequential(*blocks,),\n",
        "            downsample=downsample,\n",
        "        ))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.patch_embed(x)\n",
        "        if self.pos_embed is not None:\n",
        "            pos_embed = self.pos_embed.permute(0, 2, 3, 1) if not self.channel_first else self.pos_embed\n",
        "            x = x + pos_embed\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def flops(self, shape=(3, 224, 224), verbose=True):\n",
        "        # shape = self.__input_shape__[1:]\n",
        "        supported_ops={\n",
        "            \"aten::silu\": None, # as relu is in _IGNORED_OPS\n",
        "            \"aten::neg\": None, # as relu is in _IGNORED_OPS\n",
        "            \"aten::exp\": None, # as relu is in _IGNORED_OPS\n",
        "            \"aten::flip\": None, # as permute is in _IGNORED_OPS\n",
        "            # \"prim::PythonOp.CrossScan\": None,\n",
        "            # \"prim::PythonOp.CrossMerge\": None,\n",
        "            \"prim::PythonOp.SelectiveScanCuda\": partial(selective_scan_flop_jit, backend=\"prefixsum\", verbose=verbose),\n",
        "        }\n",
        "\n",
        "        model = copy.deepcopy(self)\n",
        "        model.cuda().eval()\n",
        "\n",
        "        input = torch.randn((1, *shape), device=next(model.parameters()).device)\n",
        "        params = parameter_count(model)[\"\"]\n",
        "        Gflops, unsupported = flop_count(model=model, inputs=(input,), supported_ops=supported_ops)\n",
        "\n",
        "        del model, input\n",
        "        return sum(Gflops.values()) * 1e9\n",
        "        return f\"params {params} GFLOPs {sum(Gflops.values())}\"\n",
        "\n",
        "    # used to load ckpt from previous training code\n",
        "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n",
        "\n",
        "        def check_name(src, state_dict: dict = state_dict, strict=False):\n",
        "            if strict:\n",
        "                if prefix + src in list(state_dict.keys()):\n",
        "                    return True\n",
        "            else:\n",
        "                key = prefix + src\n",
        "                for k in list(state_dict.keys()):\n",
        "                    if k.startswith(key):\n",
        "                        return True\n",
        "            return False\n",
        "\n",
        "        def change_name(src, dst, state_dict: dict = state_dict, strict=False):\n",
        "            if strict:\n",
        "                if prefix + src in list(state_dict.keys()):\n",
        "                    state_dict[prefix + dst] = state_dict[prefix + src]\n",
        "                    state_dict.pop(prefix + src)\n",
        "            else:\n",
        "                key = prefix + src\n",
        "                for k in list(state_dict.keys()):\n",
        "                    if k.startswith(key):\n",
        "                        new_k = prefix + dst + k[len(key):]\n",
        "                        state_dict[new_k] = state_dict[k]\n",
        "                        state_dict.pop(k)\n",
        "\n",
        "        if check_name(\"pos_embed\", strict=True):\n",
        "            srcEmb: torch.Tensor = state_dict[prefix + \"pos_embed\"]\n",
        "            state_dict[prefix + \"pos_embed\"] = F.interpolate(srcEmb.float(), size=self.pos_embed.shape[2:4], align_corners=False, mode=\"bicubic\").to(srcEmb.device)\n",
        "\n",
        "        change_name(\"patch_embed.proj\", \"patch_embed.0\")\n",
        "        change_name(\"patch_embed.norm\", \"patch_embed.2\")\n",
        "        for i in range(100):\n",
        "            for j in range(100):\n",
        "                change_name(f\"layers.{i}.blocks.{j}.ln_1\", f\"layers.{i}.blocks.{j}.norm\")\n",
        "                change_name(f\"layers.{i}.blocks.{j}.self_attention\", f\"layers.{i}.blocks.{j}.op\")\n",
        "        change_name(\"norm\", \"classifier.norm\")\n",
        "        change_name(\"head\", \"classifier.head\")\n",
        "\n",
        "        return super()._load_from_state_dict(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\n",
        "\n",
        "\n",
        "# compatible with openmmlab\n",
        "class Backbone_VSSM(VSSM):\n",
        "    def __init__(self, out_indices=(0, 1, 2, 3), pretrained=None, norm_layer=\"ln\", **kwargs):\n",
        "        kwargs.update(norm_layer=norm_layer)\n",
        "        super().__init__(**kwargs)\n",
        "        self.channel_first = (norm_layer.lower() in [\"bn\", \"ln2d\"])\n",
        "        _NORMLAYERS = dict(\n",
        "            ln=nn.LayerNorm,\n",
        "            ln2d=LayerNorm2d,\n",
        "            bn=nn.BatchNorm2d,\n",
        "        )\n",
        "        norm_layer: nn.Module = _NORMLAYERS.get(norm_layer.lower(), None)\n",
        "\n",
        "        self.out_indices = out_indices\n",
        "        for i in out_indices:\n",
        "            layer = norm_layer(self.dims[i])\n",
        "            layer_name = f'outnorm{i}'\n",
        "            self.add_module(layer_name, layer)\n",
        "\n",
        "        del self.classifier\n",
        "        self.load_pretrained(pretrained)\n",
        "\n",
        "    def load_pretrained(self, ckpt=None, key=\"model\"):\n",
        "        if ckpt is None:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            _ckpt = torch.load(open(ckpt, \"rb\"), map_location=torch.device(\"cpu\"))\n",
        "            print(f\"Successfully load ckpt {ckpt}\")\n",
        "            incompatibleKeys = self.load_state_dict(_ckpt[key], strict=False)\n",
        "            print(incompatibleKeys)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed loading checkpoint form {ckpt}: {e}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        def layer_forward(l, x):\n",
        "            x = l.blocks(x)\n",
        "            y = l.downsample(x)\n",
        "            return x, y\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        outs = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            o, x = layer_forward(layer, x) # (B, H, W, C)\n",
        "            if i in self.out_indices:\n",
        "                norm_layer = getattr(self, f'outnorm{i}')\n",
        "                out = norm_layer(o)\n",
        "                if not self.channel_first:\n",
        "                    out = out.permute(0, 3, 1, 2)\n",
        "                outs.append(out.contiguous())\n",
        "\n",
        "        if len(self.out_indices) == 0:\n",
        "            return x\n",
        "\n",
        "        return outs\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "def vanilla_vmamba_tiny():\n",
        "    return VSSM(\n",
        "        depths=[2, 2, 9, 2], dims=96, drop_path_rate=0.2,\n",
        "        patch_size=4, in_chans=3, num_classes=1000,\n",
        "        ssm_d_state=16, ssm_ratio=2.0, ssm_dt_rank=\"auto\", ssm_act_layer=\"silu\",\n",
        "        ssm_conv=3, ssm_conv_bias=True, ssm_drop_rate=0.0,\n",
        "        ssm_init=\"v0\", forward_type=\"v0\",\n",
        "        mlp_ratio=0.0, mlp_act_layer=\"gelu\", mlp_drop_rate=0.0, gmlp=False,\n",
        "        patch_norm=True, norm_layer=\"ln\",\n",
        "        downsample_version=\"v1\", patchembed_version=\"v1\",\n",
        "        use_checkpoint=False, posembed=False, imgsize=224,\n",
        "    )\n",
        "\n",
        "\n",
        "def vanilla_vmamba_small():\n",
        "    return VSSM(\n",
        "        depths=[2, 2, 27, 2], dims=96, drop_path_rate=0.3,\n",
        "        patch_size=4, in_chans=3, num_classes=1000,\n",
        "        ssm_d_state=16, ssm_ratio=2.0, ssm_dt_rank=\"auto\", ssm_act_layer=\"silu\",\n",
        "        ssm_conv=3, ssm_conv_bias=True, ssm_drop_rate=0.0,\n",
        "        ssm_init=\"v0\", forward_type=\"v0\",\n",
        "        mlp_ratio=0.0, mlp_act_layer=\"gelu\", mlp_drop_rate=0.0, gmlp=False,\n",
        "        patch_norm=True, norm_layer=\"ln\",\n",
        "        downsample_version=\"v1\", patchembed_version=\"v1\",\n",
        "        use_checkpoint=False, posembed=False, imgsize=224,\n",
        "    )\n",
        "\n",
        "\n",
        "def vanilla_vmamba_base():\n",
        "    return VSSM(\n",
        "        depths=[2, 2, 27, 2], dims=128, drop_path_rate=0.6,\n",
        "        patch_size=4, in_chans=3, num_classes=1000,\n",
        "        ssm_d_state=16, ssm_ratio=2.0, ssm_dt_rank=\"auto\", ssm_act_layer=\"silu\",\n",
        "        ssm_conv=3, ssm_conv_bias=True, ssm_drop_rate=0.0,\n",
        "        ssm_init=\"v0\", forward_type=\"v0\",\n",
        "        mlp_ratio=0.0, mlp_act_layer=\"gelu\", mlp_drop_rate=0.0, gmlp=False,\n",
        "        patch_norm=True, norm_layer=\"ln\",\n",
        "        downsample_version=\"v1\", patchembed_version=\"v1\",\n",
        "        use_checkpoint=False, posembed=False, imgsize=224,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advzOLVM4JSv"
      },
      "source": [
        "### VMamba Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4oi7TRx4wiw",
        "outputId": "a38f66ca-1fbb-4206-8509-8fbfe2a423d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Low-level features: torch.Size([1, 64, 80, 80])\n",
            "Medium-level features: torch.Size([1, 128, 40, 40])\n",
            "High-level features: torch.Size([1, 256, 20, 20])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VSSMBackbone(nn.Module):\n",
        "    def __init__(self, model_size, base_channels=16,base_depth=1, deep_mul=1, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
        "        if model_size=='tiny':\n",
        "          model = vanilla_vmamba_tiny()\n",
        "          low_in_channels = 192\n",
        "          medium_in_channels = 384\n",
        "          high_in_channels = 768\n",
        "        elif model_size == 'small':\n",
        "          model = vanilla_vmamba_small()\n",
        "          low_in_channels = 192\n",
        "          medium_in_channels = 384\n",
        "          high_in_channels = 768\n",
        "        elif model_size == 'base':\n",
        "          model = vanilla_vmamba_base()\n",
        "          low_in_channels = 256\n",
        "          medium_in_channels = 512\n",
        "          high_in_channels = 1024\n",
        "        else:\n",
        "          raise NotImplementedError\n",
        "        super(VSSMBackbone, self).__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.out_indices = [0, 1, 2]\n",
        "\n",
        "        self.patch_embed = model.patch_embed\n",
        "        self.layers = model.layers\n",
        "\n",
        "        # Define convolution layers to adjust feature map channels\n",
        "        self.conv_low = nn.Conv2d(in_channels=low_in_channels, out_channels=base_channels * 4, kernel_size=1)  # Low-level: [B, base_channels*4, H, W]\n",
        "        self.conv_medium = nn.Conv2d(in_channels=medium_in_channels, out_channels=base_channels * 8, kernel_size=1)  # Medium-level: [B, base_channels*8, H, W]\n",
        "        self.conv_high = nn.Conv2d(in_channels=high_in_channels, out_channels=int(base_channels * 16 * deep_mul), kernel_size=1)  # High-level: [B, base_channels*16, H, W]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Step 1: Patch Embedding\n",
        "        x = self.patch_embed(x.to(device))  # Output: [B, 160, 160, C]\n",
        "        # Step 2: Feature extraction from layers\n",
        "        low_feat, medium_feat, high_feat = None, None, None\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x.to(device))\n",
        "            # Capture the features from the specified layers\n",
        "            if i == 0:\n",
        "                low_feat = x  # Features after Stage 1 (low-level)\n",
        "            elif i == 1:\n",
        "                medium_feat = x  # Features after Stage 2 (medium-level)\n",
        "            elif i == 2:\n",
        "                high_feat = x  # Features after Stage 3 (high-level)\n",
        "\n",
        "        # Reshape and apply convolutions for final feature maps output\n",
        "        low_feat = low_feat.permute(0, 3, 1, 2).contiguous()  # Convert to [B, C, H, W]\n",
        "        low_feat = self.conv_low(low_feat)  # Adjust channels to [B, base_channels*4, H, W]\n",
        "\n",
        "        medium_feat = medium_feat.permute(0, 3, 1, 2).contiguous()  # Convert to [B, C, H, W]\n",
        "        medium_feat = self.conv_medium(medium_feat)  # Adjust channels to [B, base_channels*8, H, W]\n",
        "\n",
        "        high_feat = high_feat.permute(0, 3, 1, 2).contiguous()  # Convert to [B, C, H, W]\n",
        "        high_feat = self.conv_high(high_feat)  # Adjust channels to [B, base_channels*16, H, W]\n",
        "\n",
        "        return low_feat, medium_feat, high_feat\n",
        "\n",
        "# Example usage:\n",
        "base_channels=16\n",
        "base_depth=3\n",
        "deep_mul=1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "backbone = VSSMBackbone(\"base\", base_channels, base_depth, deep_mul, device).to('cuda')\n",
        "test_input = torch.randn(1, 3, 640, 640)  # Input image of size 640x640\n",
        "low_feat, medium_feat, high_feat = backbone(test_input)\n",
        "\n",
        "# Print feature map shapes\n",
        "print(\"Low-level features:\", low_feat.shape)   # Expected: [B, base_channels*4, 160, 160]\n",
        "print(\"Medium-level features:\", medium_feat.shape)  # Expected: [B, base_channels*8, 80, 80]\n",
        "print(\"High-level features:\", high_feat.shape)  # Expected: [B, base_channels*16, 40, 40]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjW_WNGIBnOE"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGwSltT-CAp2"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yaIqGV58CINJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "#---------------------------------------------------------#\n",
        "#   RGB\n",
        "#   RGBRGB\n",
        "#---------------------------------------------------------#\n",
        "def cvtColor(image):\n",
        "    if len(np.shape(image)) == 3 and np.shape(image)[2] == 3:\n",
        "        return image\n",
        "    else:\n",
        "        image = image.convert('RGB')\n",
        "        return image\n",
        "\n",
        "#---------------------------------------------------#\n",
        "#   resize\n",
        "#---------------------------------------------------#\n",
        "def resize_image(image, size, letterbox_image):\n",
        "    iw, ih  = image.size\n",
        "    w, h    = size\n",
        "    if letterbox_image:\n",
        "        scale   = min(w/iw, h/ih)\n",
        "        nw      = int(iw*scale)\n",
        "        nh      = int(ih*scale)\n",
        "\n",
        "        image   = image.resize((nw,nh), Image.BICUBIC)\n",
        "        new_image = Image.new('RGB', size, (128,128,128))\n",
        "        new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
        "    else:\n",
        "        new_image = image.resize((w, h), Image.BICUBIC)\n",
        "    return new_image\n",
        "\n",
        "#---------------------------------------------------#\n",
        "#   \n",
        "#---------------------------------------------------#\n",
        "def get_classes(classes_path):\n",
        "    with open(classes_path, encoding='utf-8') as f:\n",
        "        class_names = f.readlines()\n",
        "    class_names = [c.strip() for c in class_names]\n",
        "    return class_names, len(class_names)\n",
        "\n",
        "#---------------------------------------------------#\n",
        "#   \n",
        "#---------------------------------------------------#\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "#---------------------------------------------------#\n",
        "#   \n",
        "#---------------------------------------------------#\n",
        "def seed_everything(seed=11):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "#---------------------------------------------------#\n",
        "#   Dataloader\n",
        "#---------------------------------------------------#\n",
        "def worker_init_fn(worker_id, rank, seed):\n",
        "    worker_seed = rank + seed\n",
        "    random.seed(worker_seed)\n",
        "    np.random.seed(worker_seed)\n",
        "    torch.manual_seed(worker_seed)\n",
        "\n",
        "def preprocess_input(image):\n",
        "    image /= 255.0\n",
        "    return image\n",
        "\n",
        "def show_config(**kwargs):\n",
        "    print('Configurations:')\n",
        "    print('-' * 70)\n",
        "    print('|%25s | %40s|' % ('keys', 'values'))\n",
        "    print('-' * 70)\n",
        "    for key, value in kwargs.items():\n",
        "        print('|%25s | %40s|' % (str(key), str(value)))\n",
        "    print('-' * 70)\n",
        "\n",
        "def download_weights(phi, model_dir=\"./model_data\"):\n",
        "    import os\n",
        "\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "\n",
        "    download_urls = {\n",
        "        \"n\" : 'https://github.com/bubbliiiing/yolov8-pytorch/releases/download/v1.0/yolov8_n_backbone_weights.pth',\n",
        "        \"s\" : 'https://github.com/bubbliiiing/yolov8-pytorch/releases/download/v1.0/yolov8_s_backbone_weights.pth',\n",
        "        \"m\" : 'https://github.com/bubbliiiing/yolov8-pytorch/releases/download/v1.0/yolov8_m_backbone_weights.pth',\n",
        "        \"l\" : 'https://github.com/bubbliiiing/yolov8-pytorch/releases/download/v1.0/yolov8_l_backbone_weights.pth',\n",
        "        \"x\" : 'https://github.com/bubbliiiing/yolov8-pytorch/releases/download/v1.0/yolov8_x_backbone_weights.pth',\n",
        "    }\n",
        "    url = download_urls[phi]\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    load_state_dict_from_url(url, model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj4D9WJ_B-zg"
      },
      "source": [
        "## Utils Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S1kST_iICMXY"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import json\n",
        "import math\n",
        "import operator\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "try:\n",
        "    from pycocotools.coco import COCO\n",
        "    from pycocotools.cocoeval import COCOeval\n",
        "except:\n",
        "    pass\n",
        "import cv2\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "    0,0 ------> x (width)\n",
        "     |\n",
        "     |  (Left,Top)\n",
        "     |      *_________\n",
        "     |      |         |\n",
        "            |         |\n",
        "     y      |_________|\n",
        "  (height)            *\n",
        "                (Right,Bottom)\n",
        "'''\n",
        "\n",
        "def log_average_miss_rate(precision, fp_cumsum, num_images):\n",
        "    \"\"\"\n",
        "        log-average miss rate:\n",
        "            Calculated by averaging miss rates at 9 evenly spaced FPPI points\n",
        "            between 10e-2 and 10e0, in log-space.\n",
        "\n",
        "        output:\n",
        "                lamr | log-average miss rate\n",
        "                mr | miss rate\n",
        "                fppi | false positives per image\n",
        "\n",
        "        references:\n",
        "            [1] Dollar, Piotr, et al. \"Pedestrian Detection: An Evaluation of the\n",
        "               State of the Art.\" Pattern Analysis and Machine Intelligence, IEEE\n",
        "               Transactions on 34.4 (2012): 743 - 761.\n",
        "    \"\"\"\n",
        "\n",
        "    if precision.size == 0:\n",
        "        lamr = 0\n",
        "        mr = 1\n",
        "        fppi = 0\n",
        "        return lamr, mr, fppi\n",
        "\n",
        "    fppi = fp_cumsum / float(num_images)\n",
        "    mr = (1 - precision)\n",
        "\n",
        "    fppi_tmp = np.insert(fppi, 0, -1.0)\n",
        "    mr_tmp = np.insert(mr, 0, 1.0)\n",
        "\n",
        "    ref = np.logspace(-2.0, 0.0, num = 9)\n",
        "    for i, ref_i in enumerate(ref):\n",
        "        j = np.where(fppi_tmp <= ref_i)[-1][-1]\n",
        "        ref[i] = mr_tmp[j]\n",
        "\n",
        "    lamr = math.exp(np.mean(np.log(np.maximum(1e-10, ref))))\n",
        "\n",
        "    return lamr, mr, fppi\n",
        "\n",
        "\"\"\"\n",
        " throw error and exit\n",
        "\"\"\"\n",
        "def error(msg):\n",
        "    print(msg)\n",
        "    sys.exit(0)\n",
        "\n",
        "\"\"\"\n",
        " check if the number is a float between 0.0 and 1.0\n",
        "\"\"\"\n",
        "def is_float_between_0_and_1(value):\n",
        "    try:\n",
        "        val = float(value)\n",
        "        if val > 0.0 and val < 1.0:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "\"\"\"\n",
        " Calculate the AP given the recall and precision array\n",
        "    1st) We compute a version of the measured precision/recall curve with\n",
        "         precision monotonically decreasing\n",
        "    2nd) We compute the AP as the area under this curve by numerical integration.\n",
        "\"\"\"\n",
        "def voc_ap(rec, prec):\n",
        "    \"\"\"\n",
        "    --- Official matlab code VOC2012---\n",
        "    mrec=[0 ; rec ; 1];\n",
        "    mpre=[0 ; prec ; 0];\n",
        "    for i=numel(mpre)-1:-1:1\n",
        "            mpre(i)=max(mpre(i),mpre(i+1));\n",
        "    end\n",
        "    i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
        "    ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
        "    \"\"\"\n",
        "    rec.insert(0, 0.0) # insert 0.0 at begining of list\n",
        "    rec.append(1.0) # insert 1.0 at end of list\n",
        "    mrec = rec[:]\n",
        "    prec.insert(0, 0.0) # insert 0.0 at begining of list\n",
        "    prec.append(0.0) # insert 0.0 at end of list\n",
        "    mpre = prec[:]\n",
        "    \"\"\"\n",
        "     This part makes the precision monotonically decreasing\n",
        "        (goes from the end to the beginning)\n",
        "        matlab: for i=numel(mpre)-1:-1:1\n",
        "                    mpre(i)=max(mpre(i),mpre(i+1));\n",
        "    \"\"\"\n",
        "    for i in range(len(mpre)-2, -1, -1):\n",
        "        mpre[i] = max(mpre[i], mpre[i+1])\n",
        "    \"\"\"\n",
        "     This part creates a list of indexes where the recall changes\n",
        "        matlab: i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
        "    \"\"\"\n",
        "    i_list = []\n",
        "    for i in range(1, len(mrec)):\n",
        "        if mrec[i] != mrec[i-1]:\n",
        "            i_list.append(i) # if it was matlab would be i + 1\n",
        "    \"\"\"\n",
        "     The Average Precision (AP) is the area under the curve\n",
        "        (numerical integration)\n",
        "        matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
        "    \"\"\"\n",
        "    ap = 0.0\n",
        "    for i in i_list:\n",
        "        ap += ((mrec[i]-mrec[i-1])*mpre[i])\n",
        "    return ap, mrec, mpre\n",
        "\n",
        "\n",
        "\"\"\"\n",
        " Convert the lines of a file to a list\n",
        "\"\"\"\n",
        "def file_lines_to_list(path):\n",
        "    # open txt file lines to a list\n",
        "    with open(path) as f:\n",
        "        content = f.readlines()\n",
        "    # remove whitespace characters like `\\n` at the end of each line\n",
        "    content = [x.strip() for x in content]\n",
        "    return content\n",
        "\n",
        "\"\"\"\n",
        " Draws text in image\n",
        "\"\"\"\n",
        "def draw_text_in_image(img, text, pos, color, line_width):\n",
        "    font = cv2.FONT_HERSHEY_PLAIN\n",
        "    fontScale = 1\n",
        "    lineType = 1\n",
        "    bottomLeftCornerOfText = pos\n",
        "    cv2.putText(img, text,\n",
        "            bottomLeftCornerOfText,\n",
        "            font,\n",
        "            fontScale,\n",
        "            color,\n",
        "            lineType)\n",
        "    text_width, _ = cv2.getTextSize(text, font, fontScale, lineType)[0]\n",
        "    return img, (line_width + text_width)\n",
        "\n",
        "\"\"\"\n",
        " Plot - adjust axes\n",
        "\"\"\"\n",
        "def adjust_axes(r, t, fig, axes):\n",
        "    # get text width for re-scaling\n",
        "    bb = t.get_window_extent(renderer=r)\n",
        "    text_width_inches = bb.width / fig.dpi\n",
        "    # get axis width in inches\n",
        "    current_fig_width = fig.get_figwidth()\n",
        "    new_fig_width = current_fig_width + text_width_inches\n",
        "    propotion = new_fig_width / current_fig_width\n",
        "    # get axis limit\n",
        "    x_lim = axes.get_xlim()\n",
        "    axes.set_xlim([x_lim[0], x_lim[1]*propotion])\n",
        "\n",
        "\"\"\"\n",
        " Draw plot using Matplotlib\n",
        "\"\"\"\n",
        "def draw_plot_func(dictionary, n_classes, window_title, plot_title, x_label, output_path, to_show, plot_color, true_p_bar):\n",
        "    # sort the dictionary by decreasing value, into a list of tuples\n",
        "    sorted_dic_by_value = sorted(dictionary.items(), key=operator.itemgetter(1))\n",
        "    # unpacking the list of tuples into two lists\n",
        "    sorted_keys, sorted_values = zip(*sorted_dic_by_value)\n",
        "    #\n",
        "    if true_p_bar != \"\":\n",
        "        \"\"\"\n",
        "         Special case to draw in:\n",
        "            - green -> TP: True Positives (object detected and matches ground-truth)\n",
        "            - red -> FP: False Positives (object detected but does not match ground-truth)\n",
        "            - orange -> FN: False Negatives (object not detected but present in the ground-truth)\n",
        "        \"\"\"\n",
        "        fp_sorted = []\n",
        "        tp_sorted = []\n",
        "        for key in sorted_keys:\n",
        "            fp_sorted.append(dictionary[key] - true_p_bar[key])\n",
        "            tp_sorted.append(true_p_bar[key])\n",
        "        plt.barh(range(n_classes), fp_sorted, align='center', color='crimson', label='False Positive')\n",
        "        plt.barh(range(n_classes), tp_sorted, align='center', color='forestgreen', label='True Positive', left=fp_sorted)\n",
        "        # add legend\n",
        "        plt.legend(loc='lower right')\n",
        "        \"\"\"\n",
        "         Write number on side of bar\n",
        "        \"\"\"\n",
        "        fig = plt.gcf() # gcf - get current figure\n",
        "        axes = plt.gca()\n",
        "        r = fig.canvas.get_renderer()\n",
        "        for i, val in enumerate(sorted_values):\n",
        "            fp_val = fp_sorted[i]\n",
        "            tp_val = tp_sorted[i]\n",
        "            fp_str_val = \" \" + str(fp_val)\n",
        "            tp_str_val = fp_str_val + \" \" + str(tp_val)\n",
        "            # trick to paint multicolor with offset:\n",
        "            # first paint everything and then repaint the first number\n",
        "            t = plt.text(val, i, tp_str_val, color='forestgreen', va='center', fontweight='bold')\n",
        "            plt.text(val, i, fp_str_val, color='crimson', va='center', fontweight='bold')\n",
        "            if i == (len(sorted_values)-1): # largest bar\n",
        "                adjust_axes(r, t, fig, axes)\n",
        "    else:\n",
        "        plt.barh(range(n_classes), sorted_values, color=plot_color)\n",
        "        \"\"\"\n",
        "         Write number on side of bar\n",
        "        \"\"\"\n",
        "        fig = plt.gcf() # gcf - get current figure\n",
        "        axes = plt.gca()\n",
        "        r = fig.canvas.get_renderer()\n",
        "        for i, val in enumerate(sorted_values):\n",
        "            str_val = \" \" + str(val) # add a space before\n",
        "            if val < 1.0:\n",
        "                str_val = \" {0:.2f}\".format(val)\n",
        "            t = plt.text(val, i, str_val, color=plot_color, va='center', fontweight='bold')\n",
        "            # re-set axes to show number inside the figure\n",
        "            if i == (len(sorted_values)-1): # largest bar\n",
        "                adjust_axes(r, t, fig, axes)\n",
        "    # set window title\n",
        "    fig.canvas.set_window_title(window_title)\n",
        "    # write classes in y axis\n",
        "    tick_font_size = 12\n",
        "    plt.yticks(range(n_classes), sorted_keys, fontsize=tick_font_size)\n",
        "    \"\"\"\n",
        "     Re-scale height accordingly\n",
        "    \"\"\"\n",
        "    init_height = fig.get_figheight()\n",
        "    # comput the matrix height in points and inches\n",
        "    dpi = fig.dpi\n",
        "    height_pt = n_classes * (tick_font_size * 1.4) # 1.4 (some spacing)\n",
        "    height_in = height_pt / dpi\n",
        "    # compute the required figure height\n",
        "    top_margin = 0.15 # in percentage of the figure height\n",
        "    bottom_margin = 0.05 # in percentage of the figure height\n",
        "    figure_height = height_in / (1 - top_margin - bottom_margin)\n",
        "    # set new height\n",
        "    if figure_height > init_height:\n",
        "        fig.set_figheight(figure_height)\n",
        "\n",
        "    # set plot title\n",
        "    plt.title(plot_title, fontsize=14)\n",
        "    # set axis titles\n",
        "    # plt.xlabel('classes')\n",
        "    plt.xlabel(x_label, fontsize='large')\n",
        "    # adjust size of window\n",
        "    fig.tight_layout()\n",
        "    # save the plot\n",
        "    fig.savefig(output_path)\n",
        "    # show image\n",
        "    if to_show:\n",
        "        plt.show()\n",
        "    # close the plot\n",
        "    plt.close()\n",
        "\n",
        "def get_map(MINOVERLAP, draw_plot, score_threhold=0.5, path = './map_out'):\n",
        "    GT_PATH             = os.path.join(path, 'ground-truth')\n",
        "    DR_PATH             = os.path.join(path, 'detection-results')\n",
        "    IMG_PATH            = os.path.join(path, 'images-optional')\n",
        "    TEMP_FILES_PATH     = os.path.join(path, '.temp_files')\n",
        "    RESULTS_FILES_PATH  = os.path.join(path, 'results')\n",
        "\n",
        "    show_animation = True\n",
        "    if os.path.exists(IMG_PATH):\n",
        "        for dirpath, dirnames, files in os.walk(IMG_PATH):\n",
        "            if not files:\n",
        "                show_animation = False\n",
        "    else:\n",
        "        show_animation = False\n",
        "\n",
        "    if not os.path.exists(TEMP_FILES_PATH):\n",
        "        os.makedirs(TEMP_FILES_PATH)\n",
        "\n",
        "    if os.path.exists(RESULTS_FILES_PATH):\n",
        "        shutil.rmtree(RESULTS_FILES_PATH)\n",
        "    else:\n",
        "        os.makedirs(RESULTS_FILES_PATH)\n",
        "    if draw_plot:\n",
        "        try:\n",
        "            matplotlib.use('TkAgg')\n",
        "        except:\n",
        "            pass\n",
        "        os.makedirs(os.path.join(RESULTS_FILES_PATH, \"AP\"))\n",
        "        os.makedirs(os.path.join(RESULTS_FILES_PATH, \"F1\"))\n",
        "        os.makedirs(os.path.join(RESULTS_FILES_PATH, \"Recall\"))\n",
        "        os.makedirs(os.path.join(RESULTS_FILES_PATH, \"Precision\"))\n",
        "    if show_animation:\n",
        "        os.makedirs(os.path.join(RESULTS_FILES_PATH, \"images\", \"detections_one_by_one\"))\n",
        "\n",
        "    ground_truth_files_list = glob.glob(GT_PATH + '/*.txt')\n",
        "    if len(ground_truth_files_list) == 0:\n",
        "        error(\"Error: No ground-truth files found!\")\n",
        "    ground_truth_files_list.sort()\n",
        "    gt_counter_per_class     = {}\n",
        "    counter_images_per_class = {}\n",
        "\n",
        "    for txt_file in ground_truth_files_list:\n",
        "        file_id     = txt_file.split(\".txt\", 1)[0]\n",
        "        file_id     = os.path.basename(os.path.normpath(file_id))\n",
        "        temp_path   = os.path.join(DR_PATH, (file_id + \".txt\"))\n",
        "        if not os.path.exists(temp_path):\n",
        "            error_msg = \"Error. File not found: {}\\n\".format(temp_path)\n",
        "            error(error_msg)\n",
        "        lines_list      = file_lines_to_list(txt_file)\n",
        "        bounding_boxes  = []\n",
        "        is_difficult    = False\n",
        "        already_seen_classes = []\n",
        "        for line in lines_list:\n",
        "            try:\n",
        "                if \"difficult\" in line:\n",
        "                    class_name, left, top, right, bottom, _difficult = line.split()\n",
        "                    is_difficult = True\n",
        "                else:\n",
        "                    class_name, left, top, right, bottom = line.split()\n",
        "            except:\n",
        "                if \"difficult\" in line:\n",
        "                    line_split  = line.split()\n",
        "                    _difficult  = line_split[-1]\n",
        "                    bottom      = line_split[-2]\n",
        "                    right       = line_split[-3]\n",
        "                    top         = line_split[-4]\n",
        "                    left        = line_split[-5]\n",
        "                    class_name  = \"\"\n",
        "                    for name in line_split[:-5]:\n",
        "                        class_name += name + \" \"\n",
        "                    class_name  = class_name[:-1]\n",
        "                    is_difficult = True\n",
        "                else:\n",
        "                    line_split  = line.split()\n",
        "                    bottom      = line_split[-1]\n",
        "                    right       = line_split[-2]\n",
        "                    top         = line_split[-3]\n",
        "                    left        = line_split[-4]\n",
        "                    class_name  = \"\"\n",
        "                    for name in line_split[:-4]:\n",
        "                        class_name += name + \" \"\n",
        "                    class_name = class_name[:-1]\n",
        "\n",
        "            bbox = left + \" \" + top + \" \" + right + \" \" + bottom\n",
        "            if is_difficult:\n",
        "                bounding_boxes.append({\"class_name\":class_name, \"bbox\":bbox, \"used\":False, \"difficult\":True})\n",
        "                is_difficult = False\n",
        "            else:\n",
        "                bounding_boxes.append({\"class_name\":class_name, \"bbox\":bbox, \"used\":False})\n",
        "                if class_name in gt_counter_per_class:\n",
        "                    gt_counter_per_class[class_name] += 1\n",
        "                else:\n",
        "                    gt_counter_per_class[class_name] = 1\n",
        "\n",
        "                if class_name not in already_seen_classes:\n",
        "                    if class_name in counter_images_per_class:\n",
        "                        counter_images_per_class[class_name] += 1\n",
        "                    else:\n",
        "                        counter_images_per_class[class_name] = 1\n",
        "                    already_seen_classes.append(class_name)\n",
        "\n",
        "        with open(TEMP_FILES_PATH + \"/\" + file_id + \"_ground_truth.json\", 'w') as outfile:\n",
        "            json.dump(bounding_boxes, outfile)\n",
        "\n",
        "    gt_classes  = list(gt_counter_per_class.keys())\n",
        "    gt_classes  = sorted(gt_classes)\n",
        "    n_classes   = len(gt_classes)\n",
        "\n",
        "    dr_files_list = glob.glob(DR_PATH + '/*.txt')\n",
        "    dr_files_list.sort()\n",
        "    for class_index, class_name in enumerate(gt_classes):\n",
        "        bounding_boxes = []\n",
        "        for txt_file in dr_files_list:\n",
        "            file_id = txt_file.split(\".txt\",1)[0]\n",
        "            file_id = os.path.basename(os.path.normpath(file_id))\n",
        "            temp_path = os.path.join(GT_PATH, (file_id + \".txt\"))\n",
        "            if class_index == 0:\n",
        "                if not os.path.exists(temp_path):\n",
        "                    error_msg = \"Error. File not found: {}\\n\".format(temp_path)\n",
        "                    error(error_msg)\n",
        "            lines = file_lines_to_list(txt_file)\n",
        "            for line in lines:\n",
        "                try:\n",
        "                    tmp_class_name, confidence, left, top, right, bottom = line.split()\n",
        "                except:\n",
        "                    line_split      = line.split()\n",
        "                    bottom          = line_split[-1]\n",
        "                    right           = line_split[-2]\n",
        "                    top             = line_split[-3]\n",
        "                    left            = line_split[-4]\n",
        "                    confidence      = line_split[-5]\n",
        "                    tmp_class_name  = \"\"\n",
        "                    for name in line_split[:-5]:\n",
        "                        tmp_class_name += name + \" \"\n",
        "                    tmp_class_name  = tmp_class_name[:-1]\n",
        "\n",
        "                if tmp_class_name == class_name:\n",
        "                    bbox = left + \" \" + top + \" \" + right + \" \" +bottom\n",
        "                    bounding_boxes.append({\"confidence\":confidence, \"file_id\":file_id, \"bbox\":bbox})\n",
        "\n",
        "        bounding_boxes.sort(key=lambda x:float(x['confidence']), reverse=True)\n",
        "        with open(TEMP_FILES_PATH + \"/\" + class_name + \"_dr.json\", 'w') as outfile:\n",
        "            json.dump(bounding_boxes, outfile)\n",
        "\n",
        "    sum_AP = 0.0\n",
        "    ap_dictionary = {}\n",
        "    lamr_dictionary = {}\n",
        "    with open(RESULTS_FILES_PATH + \"/results.txt\", 'w') as results_file:\n",
        "        results_file.write(\"# AP and precision/recall per class\\n\")\n",
        "        count_true_positives = {}\n",
        "\n",
        "        for class_index, class_name in enumerate(gt_classes):\n",
        "            count_true_positives[class_name] = 0\n",
        "            dr_file = TEMP_FILES_PATH + \"/\" + class_name + \"_dr.json\"\n",
        "            dr_data = json.load(open(dr_file))\n",
        "\n",
        "            nd          = len(dr_data)\n",
        "            tp          = [0] * nd\n",
        "            fp          = [0] * nd\n",
        "            score       = [0] * nd\n",
        "            score_threhold_idx = 0\n",
        "            for idx, detection in enumerate(dr_data):\n",
        "                file_id     = detection[\"file_id\"]\n",
        "                score[idx]  = float(detection[\"confidence\"])\n",
        "                if score[idx] >= score_threhold:\n",
        "                    score_threhold_idx = idx\n",
        "\n",
        "                if show_animation:\n",
        "                    ground_truth_img = glob.glob1(IMG_PATH, file_id + \".*\")\n",
        "                    if len(ground_truth_img) == 0:\n",
        "                        error(\"Error. Image not found with id: \" + file_id)\n",
        "                    elif len(ground_truth_img) > 1:\n",
        "                        error(\"Error. Multiple image with id: \" + file_id)\n",
        "                    else:\n",
        "                        img = cv2.imread(IMG_PATH + \"/\" + ground_truth_img[0])\n",
        "                        img_cumulative_path = RESULTS_FILES_PATH + \"/images/\" + ground_truth_img[0]\n",
        "                        if os.path.isfile(img_cumulative_path):\n",
        "                            img_cumulative = cv2.imread(img_cumulative_path)\n",
        "                        else:\n",
        "                            img_cumulative = img.copy()\n",
        "                        bottom_border = 60\n",
        "                        BLACK = [0, 0, 0]\n",
        "                        img = cv2.copyMakeBorder(img, 0, bottom_border, 0, 0, cv2.BORDER_CONSTANT, value=BLACK)\n",
        "\n",
        "                gt_file             = TEMP_FILES_PATH + \"/\" + file_id + \"_ground_truth.json\"\n",
        "                ground_truth_data   = json.load(open(gt_file))\n",
        "                ovmax       = -1\n",
        "                gt_match    = -1\n",
        "                bb          = [float(x) for x in detection[\"bbox\"].split()]\n",
        "                for obj in ground_truth_data:\n",
        "                    if obj[\"class_name\"] == class_name:\n",
        "                        bbgt    = [ float(x) for x in obj[\"bbox\"].split() ]\n",
        "                        bi      = [max(bb[0],bbgt[0]), max(bb[1],bbgt[1]), min(bb[2],bbgt[2]), min(bb[3],bbgt[3])]\n",
        "                        iw      = bi[2] - bi[0] + 1\n",
        "                        ih      = bi[3] - bi[1] + 1\n",
        "                        if iw > 0 and ih > 0:\n",
        "                            ua = (bb[2] - bb[0] + 1) * (bb[3] - bb[1] + 1) + (bbgt[2] - bbgt[0]\n",
        "                                            + 1) * (bbgt[3] - bbgt[1] + 1) - iw * ih\n",
        "                            ov = iw * ih / ua\n",
        "                            if ov > ovmax:\n",
        "                                ovmax = ov\n",
        "                                gt_match = obj\n",
        "\n",
        "                if show_animation:\n",
        "                    status = \"NO MATCH FOUND!\"\n",
        "\n",
        "                min_overlap = MINOVERLAP\n",
        "                if ovmax >= min_overlap:\n",
        "                    if \"difficult\" not in gt_match:\n",
        "                        if not bool(gt_match[\"used\"]):\n",
        "                            tp[idx] = 1\n",
        "                            gt_match[\"used\"] = True\n",
        "                            count_true_positives[class_name] += 1\n",
        "                            with open(gt_file, 'w') as f:\n",
        "                                    f.write(json.dumps(ground_truth_data))\n",
        "                            if show_animation:\n",
        "                                status = \"MATCH!\"\n",
        "                        else:\n",
        "                            fp[idx] = 1\n",
        "                            if show_animation:\n",
        "                                status = \"REPEATED MATCH!\"\n",
        "                else:\n",
        "                    fp[idx] = 1\n",
        "                    if ovmax > 0:\n",
        "                        status = \"INSUFFICIENT OVERLAP\"\n",
        "\n",
        "                \"\"\"\n",
        "                Draw image to show animation\n",
        "                \"\"\"\n",
        "                if show_animation:\n",
        "                    height, widht = img.shape[:2]\n",
        "                    white           = (255,255,255)\n",
        "                    light_blue      = (255,200,100)\n",
        "                    green           = (0,255,0)\n",
        "                    light_red       = (30,30,255)\n",
        "                    margin          = 10\n",
        "                    # 1nd line\n",
        "                    v_pos           = int(height - margin - (bottom_border / 2.0))\n",
        "                    text            = \"Image: \" + ground_truth_img[0] + \" \"\n",
        "                    img, line_width = draw_text_in_image(img, text, (margin, v_pos), white, 0)\n",
        "                    text            = \"Class [\" + str(class_index) + \"/\" + str(n_classes) + \"]: \" + class_name + \" \"\n",
        "                    img, line_width = draw_text_in_image(img, text, (margin + line_width, v_pos), light_blue, line_width)\n",
        "                    if ovmax != -1:\n",
        "                        color       = light_red\n",
        "                        if status   == \"INSUFFICIENT OVERLAP\":\n",
        "                            text    = \"IoU: {0:.2f}% \".format(ovmax*100) + \"< {0:.2f}% \".format(min_overlap*100)\n",
        "                        else:\n",
        "                            text    = \"IoU: {0:.2f}% \".format(ovmax*100) + \">= {0:.2f}% \".format(min_overlap*100)\n",
        "                            color   = green\n",
        "                        img, _ = draw_text_in_image(img, text, (margin + line_width, v_pos), color, line_width)\n",
        "                    # 2nd line\n",
        "                    v_pos           += int(bottom_border / 2.0)\n",
        "                    rank_pos        = str(idx+1)\n",
        "                    text            = \"Detection #rank: \" + rank_pos + \" confidence: {0:.2f}% \".format(float(detection[\"confidence\"])*100)\n",
        "                    img, line_width = draw_text_in_image(img, text, (margin, v_pos), white, 0)\n",
        "                    color           = light_red\n",
        "                    if status == \"MATCH!\":\n",
        "                        color = green\n",
        "                    text            = \"Result: \" + status + \" \"\n",
        "                    img, line_width = draw_text_in_image(img, text, (margin + line_width, v_pos), color, line_width)\n",
        "\n",
        "                    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                    if ovmax > 0:\n",
        "                        bbgt = [ int(round(float(x))) for x in gt_match[\"bbox\"].split() ]\n",
        "                        cv2.rectangle(img,(bbgt[0],bbgt[1]),(bbgt[2],bbgt[3]),light_blue,2)\n",
        "                        cv2.rectangle(img_cumulative,(bbgt[0],bbgt[1]),(bbgt[2],bbgt[3]),light_blue,2)\n",
        "                        cv2.putText(img_cumulative, class_name, (bbgt[0],bbgt[1] - 5), font, 0.6, light_blue, 1, cv2.LINE_AA)\n",
        "                    bb = [int(i) for i in bb]\n",
        "                    cv2.rectangle(img,(bb[0],bb[1]),(bb[2],bb[3]),color,2)\n",
        "                    cv2.rectangle(img_cumulative,(bb[0],bb[1]),(bb[2],bb[3]),color,2)\n",
        "                    cv2.putText(img_cumulative, class_name, (bb[0],bb[1] - 5), font, 0.6, color, 1, cv2.LINE_AA)\n",
        "\n",
        "                    cv2.imshow(\"Animation\", img)\n",
        "                    cv2.waitKey(20)\n",
        "                    output_img_path = RESULTS_FILES_PATH + \"/images/detections_one_by_one/\" + class_name + \"_detection\" + str(idx) + \".jpg\"\n",
        "                    cv2.imwrite(output_img_path, img)\n",
        "                    cv2.imwrite(img_cumulative_path, img_cumulative)\n",
        "\n",
        "            cumsum = 0\n",
        "            for idx, val in enumerate(fp):\n",
        "                fp[idx] += cumsum\n",
        "                cumsum += val\n",
        "\n",
        "            cumsum = 0\n",
        "            for idx, val in enumerate(tp):\n",
        "                tp[idx] += cumsum\n",
        "                cumsum += val\n",
        "\n",
        "            rec = tp[:]\n",
        "            for idx, val in enumerate(tp):\n",
        "                rec[idx] = float(tp[idx]) / np.maximum(gt_counter_per_class[class_name], 1)\n",
        "\n",
        "            prec = tp[:]\n",
        "            for idx, val in enumerate(tp):\n",
        "                prec[idx] = float(tp[idx]) / np.maximum((fp[idx] + tp[idx]), 1)\n",
        "\n",
        "            ap, mrec, mprec = voc_ap(rec[:], prec[:])\n",
        "            F1  = np.array(rec)*np.array(prec)*2 / np.where((np.array(prec)+np.array(rec))==0, 1, (np.array(prec)+np.array(rec)))\n",
        "\n",
        "            sum_AP  += ap\n",
        "            text    = \"{0:.2f}%\".format(ap*100) + \" = \" + class_name + \" AP \" #class_name + \" AP = {0:.2f}%\".format(ap*100)\n",
        "\n",
        "            if len(prec)>0:\n",
        "                F1_text         = \"{0:.2f}\".format(F1[score_threhold_idx]) + \" = \" + class_name + \" F1 \"\n",
        "                Recall_text     = \"{0:.2f}%\".format(rec[score_threhold_idx]*100) + \" = \" + class_name + \" Recall \"\n",
        "                Precision_text  = \"{0:.2f}%\".format(prec[score_threhold_idx]*100) + \" = \" + class_name + \" Precision \"\n",
        "            else:\n",
        "                F1_text         = \"0.00\" + \" = \" + class_name + \" F1 \"\n",
        "                Recall_text     = \"0.00%\" + \" = \" + class_name + \" Recall \"\n",
        "                Precision_text  = \"0.00%\" + \" = \" + class_name + \" Precision \"\n",
        "\n",
        "            rounded_prec    = [ '%.2f' % elem for elem in prec ]\n",
        "            rounded_rec     = [ '%.2f' % elem for elem in rec ]\n",
        "            results_file.write(text + \"\\n Precision: \" + str(rounded_prec) + \"\\n Recall :\" + str(rounded_rec) + \"\\n\\n\")\n",
        "\n",
        "            if len(prec)>0:\n",
        "                print(text + \"\\t||\\tscore_threhold=\" + str(score_threhold) + \" : \" + \"F1=\" + \"{0:.2f}\".format(F1[score_threhold_idx])\\\n",
        "                    + \" ; Recall=\" + \"{0:.2f}%\".format(rec[score_threhold_idx]*100) + \" ; Precision=\" + \"{0:.2f}%\".format(prec[score_threhold_idx]*100))\n",
        "            else:\n",
        "                print(text + \"\\t||\\tscore_threhold=\" + str(score_threhold) + \" : \" + \"F1=0.00% ; Recall=0.00% ; Precision=0.00%\")\n",
        "            ap_dictionary[class_name] = ap\n",
        "\n",
        "            n_images = counter_images_per_class[class_name]\n",
        "            lamr, mr, fppi = log_average_miss_rate(np.array(rec), np.array(fp), n_images)\n",
        "            lamr_dictionary[class_name] = lamr\n",
        "\n",
        "            if draw_plot:\n",
        "                plt.plot(rec, prec, '-o')\n",
        "                area_under_curve_x = mrec[:-1] + [mrec[-2]] + [mrec[-1]]\n",
        "                area_under_curve_y = mprec[:-1] + [0.0] + [mprec[-1]]\n",
        "                plt.fill_between(area_under_curve_x, 0, area_under_curve_y, alpha=0.2, edgecolor='r')\n",
        "\n",
        "                fig = plt.gcf()\n",
        "                fig.canvas.set_window_title('AP ' + class_name)\n",
        "\n",
        "                plt.title('class: ' + text)\n",
        "                plt.xlabel('Recall')\n",
        "                plt.ylabel('Precision')\n",
        "                axes = plt.gca()\n",
        "                axes.set_xlim([0.0,1.0])\n",
        "                axes.set_ylim([0.0,1.05])\n",
        "                fig.savefig(RESULTS_FILES_PATH + \"/AP/\" + class_name + \".png\")\n",
        "                plt.cla()\n",
        "\n",
        "                plt.plot(score, F1, \"-\", color='orangered')\n",
        "                plt.title('class: ' + F1_text + \"\\nscore_threhold=\" + str(score_threhold))\n",
        "                plt.xlabel('Score_Threhold')\n",
        "                plt.ylabel('F1')\n",
        "                axes = plt.gca()\n",
        "                axes.set_xlim([0.0,1.0])\n",
        "                axes.set_ylim([0.0,1.05])\n",
        "                fig.savefig(RESULTS_FILES_PATH + \"/F1/\" + class_name + \".png\")\n",
        "                plt.cla()\n",
        "\n",
        "                plt.plot(score, rec, \"-H\", color='gold')\n",
        "                plt.title('class: ' + Recall_text + \"\\nscore_threhold=\" + str(score_threhold))\n",
        "                plt.xlabel('Score_Threhold')\n",
        "                plt.ylabel('Recall')\n",
        "                axes = plt.gca()\n",
        "                axes.set_xlim([0.0,1.0])\n",
        "                axes.set_ylim([0.0,1.05])\n",
        "                fig.savefig(RESULTS_FILES_PATH + \"/Recall/\" + class_name + \".png\")\n",
        "                plt.cla()\n",
        "\n",
        "                plt.plot(score, prec, \"-s\", color='palevioletred')\n",
        "                plt.title('class: ' + Precision_text + \"\\nscore_threhold=\" + str(score_threhold))\n",
        "                plt.xlabel('Score_Threhold')\n",
        "                plt.ylabel('Precision')\n",
        "                axes = plt.gca()\n",
        "                axes.set_xlim([0.0,1.0])\n",
        "                axes.set_ylim([0.0,1.05])\n",
        "                fig.savefig(RESULTS_FILES_PATH + \"/Precision/\" + class_name + \".png\")\n",
        "                plt.cla()\n",
        "\n",
        "        if show_animation:\n",
        "            cv2.destroyAllWindows()\n",
        "        if n_classes == 0:\n",
        "            print(\"get_map.pyclasses_path\")\n",
        "            return 0\n",
        "        results_file.write(\"\\n# mAP of all classes\\n\")\n",
        "        mAP     = sum_AP / n_classes\n",
        "        text    = \"mAP = {0:.2f}%\".format(mAP*100)\n",
        "        results_file.write(text + \"\\n\")\n",
        "        print(text)\n",
        "\n",
        "    shutil.rmtree(TEMP_FILES_PATH)\n",
        "\n",
        "    \"\"\"\n",
        "    Count total of detection-results\n",
        "    \"\"\"\n",
        "    det_counter_per_class = {}\n",
        "    for txt_file in dr_files_list:\n",
        "        lines_list = file_lines_to_list(txt_file)\n",
        "        for line in lines_list:\n",
        "            class_name = line.split()[0]\n",
        "            if class_name in det_counter_per_class:\n",
        "                det_counter_per_class[class_name] += 1\n",
        "            else:\n",
        "                det_counter_per_class[class_name] = 1\n",
        "    dr_classes = list(det_counter_per_class.keys())\n",
        "\n",
        "    \"\"\"\n",
        "    Write number of ground-truth objects per class to results.txt\n",
        "    \"\"\"\n",
        "    with open(RESULTS_FILES_PATH + \"/results.txt\", 'a') as results_file:\n",
        "        results_file.write(\"\\n# Number of ground-truth objects per class\\n\")\n",
        "        for class_name in sorted(gt_counter_per_class):\n",
        "            results_file.write(class_name + \": \" + str(gt_counter_per_class[class_name]) + \"\\n\")\n",
        "\n",
        "    \"\"\"\n",
        "    Finish counting true positives\n",
        "    \"\"\"\n",
        "    for class_name in dr_classes:\n",
        "        if class_name not in gt_classes:\n",
        "            count_true_positives[class_name] = 0\n",
        "\n",
        "    \"\"\"\n",
        "    Write number of detected objects per class to results.txt\n",
        "    \"\"\"\n",
        "    with open(RESULTS_FILES_PATH + \"/results.txt\", 'a') as results_file:\n",
        "        results_file.write(\"\\n# Number of detected objects per class\\n\")\n",
        "        for class_name in sorted(dr_classes):\n",
        "            n_det = det_counter_per_class[class_name]\n",
        "            text = class_name + \": \" + str(n_det)\n",
        "            text += \" (tp:\" + str(count_true_positives[class_name]) + \"\"\n",
        "            text += \", fp:\" + str(n_det - count_true_positives[class_name]) + \")\\n\"\n",
        "            results_file.write(text)\n",
        "\n",
        "    \"\"\"\n",
        "    Plot the total number of occurences of each class in the ground-truth\n",
        "    \"\"\"\n",
        "    if draw_plot:\n",
        "        window_title = \"ground-truth-info\"\n",
        "        plot_title = \"ground-truth\\n\"\n",
        "        plot_title += \"(\" + str(len(ground_truth_files_list)) + \" files and \" + str(n_classes) + \" classes)\"\n",
        "        x_label = \"Number of objects per class\"\n",
        "        output_path = RESULTS_FILES_PATH + \"/ground-truth-info.png\"\n",
        "        to_show = False\n",
        "        plot_color = 'forestgreen'\n",
        "        draw_plot_func(\n",
        "            gt_counter_per_class,\n",
        "            n_classes,\n",
        "            window_title,\n",
        "            plot_title,\n",
        "            x_label,\n",
        "            output_path,\n",
        "            to_show,\n",
        "            plot_color,\n",
        "            '',\n",
        "            )\n",
        "\n",
        "    # \"\"\"\n",
        "    # Plot the total number of occurences of each class in the \"detection-results\" folder\n",
        "    # \"\"\"\n",
        "    # if draw_plot:\n",
        "    #     window_title = \"detection-results-info\"\n",
        "    #     # Plot title\n",
        "    #     plot_title = \"detection-results\\n\"\n",
        "    #     plot_title += \"(\" + str(len(dr_files_list)) + \" files and \"\n",
        "    #     count_non_zero_values_in_dictionary = sum(int(x) > 0 for x in list(det_counter_per_class.values()))\n",
        "    #     plot_title += str(count_non_zero_values_in_dictionary) + \" detected classes)\"\n",
        "    #     # end Plot title\n",
        "    #     x_label = \"Number of objects per class\"\n",
        "    #     output_path = RESULTS_FILES_PATH + \"/detection-results-info.png\"\n",
        "    #     to_show = False\n",
        "    #     plot_color = 'forestgreen'\n",
        "    #     true_p_bar = count_true_positives\n",
        "    #     draw_plot_func(\n",
        "    #         det_counter_per_class,\n",
        "    #         len(det_counter_per_class),\n",
        "    #         window_title,\n",
        "    #         plot_title,\n",
        "    #         x_label,\n",
        "    #         output_path,\n",
        "    #         to_show,\n",
        "    #         plot_color,\n",
        "    #         true_p_bar\n",
        "    #         )\n",
        "\n",
        "    \"\"\"\n",
        "    Draw log-average miss rate plot (Show lamr of all classes in decreasing order)\n",
        "    \"\"\"\n",
        "    if draw_plot:\n",
        "        window_title = \"lamr\"\n",
        "        plot_title = \"log-average miss rate\"\n",
        "        x_label = \"log-average miss rate\"\n",
        "        output_path = RESULTS_FILES_PATH + \"/lamr.png\"\n",
        "        to_show = False\n",
        "        plot_color = 'royalblue'\n",
        "        draw_plot_func(\n",
        "            lamr_dictionary,\n",
        "            n_classes,\n",
        "            window_title,\n",
        "            plot_title,\n",
        "            x_label,\n",
        "            output_path,\n",
        "            to_show,\n",
        "            plot_color,\n",
        "            \"\"\n",
        "            )\n",
        "\n",
        "    \"\"\"\n",
        "    Draw mAP plot (Show AP's of all classes in decreasing order)\n",
        "    \"\"\"\n",
        "    if draw_plot:\n",
        "        window_title = \"mAP\"\n",
        "        plot_title = \"mAP = {0:.2f}%\".format(mAP*100)\n",
        "        x_label = \"Average Precision\"\n",
        "        output_path = RESULTS_FILES_PATH + \"/mAP.png\"\n",
        "        to_show = True\n",
        "        plot_color = 'royalblue'\n",
        "        draw_plot_func(\n",
        "            ap_dictionary,\n",
        "            n_classes,\n",
        "            window_title,\n",
        "            plot_title,\n",
        "            x_label,\n",
        "            output_path,\n",
        "            to_show,\n",
        "            plot_color,\n",
        "            \"\"\n",
        "            )\n",
        "    return mAP\n",
        "\n",
        "def preprocess_gt(gt_path, class_names):\n",
        "    image_ids   = os.listdir(gt_path)\n",
        "    results = {}\n",
        "\n",
        "    images = []\n",
        "    bboxes = []\n",
        "    for i, image_id in enumerate(image_ids):\n",
        "        lines_list      = file_lines_to_list(os.path.join(gt_path, image_id))\n",
        "        boxes_per_image = []\n",
        "        image           = {}\n",
        "        image_id        = os.path.splitext(image_id)[0]\n",
        "        image['file_name'] = image_id + '.jpg'\n",
        "        image['width']     = 1\n",
        "        image['height']    = 1\n",
        "        #-----------------------------------------------------------------#\n",
        "        #     \n",
        "        #   'Results do not correspond to current coco set'\n",
        "        #-----------------------------------------------------------------#\n",
        "        image['id']        = str(image_id)\n",
        "\n",
        "        for line in lines_list:\n",
        "            difficult = 0\n",
        "            if \"difficult\" in line:\n",
        "                line_split  = line.split()\n",
        "                left, top, right, bottom, _difficult = line_split[-5:]\n",
        "                class_name  = \"\"\n",
        "                for name in line_split[:-5]:\n",
        "                    class_name += name + \" \"\n",
        "                class_name  = class_name[:-1]\n",
        "                difficult = 1\n",
        "            else:\n",
        "                line_split  = line.split()\n",
        "                left, top, right, bottom = line_split[-4:]\n",
        "                class_name  = \"\"\n",
        "                for name in line_split[:-4]:\n",
        "                    class_name += name + \" \"\n",
        "                class_name = class_name[:-1]\n",
        "\n",
        "            left, top, right, bottom = float(left), float(top), float(right), float(bottom)\n",
        "            if class_name not in class_names:\n",
        "                continue\n",
        "            cls_id  = class_names.index(class_name) + 1\n",
        "            bbox    = [left, top, right - left, bottom - top, difficult, str(image_id), cls_id, (right - left) * (bottom - top) - 10.0]\n",
        "            boxes_per_image.append(bbox)\n",
        "        images.append(image)\n",
        "        bboxes.extend(boxes_per_image)\n",
        "    results['images']        = images\n",
        "\n",
        "    categories = []\n",
        "    for i, cls in enumerate(class_names):\n",
        "        category = {}\n",
        "        category['supercategory']   = cls\n",
        "        category['name']            = cls\n",
        "        category['id']              = i + 1\n",
        "        categories.append(category)\n",
        "    results['categories']   = categories\n",
        "\n",
        "    annotations = []\n",
        "    for i, box in enumerate(bboxes):\n",
        "        annotation = {}\n",
        "        annotation['area']        = box[-1]\n",
        "        annotation['category_id'] = box[-2]\n",
        "        annotation['image_id']    = box[-3]\n",
        "        annotation['iscrowd']     = box[-4]\n",
        "        annotation['bbox']        = box[:4]\n",
        "        annotation['id']          = i\n",
        "        annotations.append(annotation)\n",
        "    results['annotations'] = annotations\n",
        "    return results\n",
        "\n",
        "def preprocess_dr(dr_path, class_names):\n",
        "    image_ids = os.listdir(dr_path)\n",
        "    results = []\n",
        "    for image_id in image_ids:\n",
        "        lines_list      = file_lines_to_list(os.path.join(dr_path, image_id))\n",
        "        image_id        = os.path.splitext(image_id)[0]\n",
        "        for line in lines_list:\n",
        "            line_split  = line.split()\n",
        "            confidence, left, top, right, bottom = line_split[-5:]\n",
        "            class_name  = \"\"\n",
        "            for name in line_split[:-5]:\n",
        "                class_name += name + \" \"\n",
        "            class_name  = class_name[:-1]\n",
        "            left, top, right, bottom = float(left), float(top), float(right), float(bottom)\n",
        "            result                  = {}\n",
        "            result[\"image_id\"]      = str(image_id)\n",
        "            if class_name not in class_names:\n",
        "                continue\n",
        "            result[\"category_id\"]   = class_names.index(class_name) + 1\n",
        "            result[\"bbox\"]          = [left, top, right - left, bottom - top]\n",
        "            result[\"score\"]         = float(confidence)\n",
        "            results.append(result)\n",
        "    return results\n",
        "\n",
        "def get_coco_map(class_names, path):\n",
        "    GT_PATH     = os.path.join(path, 'ground-truth')\n",
        "    DR_PATH     = os.path.join(path, 'detection-results')\n",
        "    COCO_PATH   = os.path.join(path, 'coco_eval')\n",
        "\n",
        "    if not os.path.exists(COCO_PATH):\n",
        "        os.makedirs(COCO_PATH)\n",
        "\n",
        "    GT_JSON_PATH = os.path.join(COCO_PATH, 'instances_gt.json')\n",
        "    DR_JSON_PATH = os.path.join(COCO_PATH, 'instances_dr.json')\n",
        "\n",
        "    with open(GT_JSON_PATH, \"w\") as f:\n",
        "        results_gt  = preprocess_gt(GT_PATH, class_names)\n",
        "        json.dump(results_gt, f, indent=4)\n",
        "\n",
        "    with open(DR_JSON_PATH, \"w\") as f:\n",
        "        results_dr  = preprocess_dr(DR_PATH, class_names)\n",
        "        json.dump(results_dr, f, indent=4)\n",
        "        if len(results_dr) == 0:\n",
        "            print(\"\")\n",
        "            return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "    cocoGt      = COCO(GT_JSON_PATH)\n",
        "    cocoDt      = cocoGt.loadRes(DR_JSON_PATH)\n",
        "    cocoEval    = COCOeval(cocoGt, cocoDt, 'bbox')\n",
        "    cocoEval.evaluate()\n",
        "    cocoEval.accumulate()\n",
        "    cocoEval.summarize()\n",
        "\n",
        "    return cocoEval.stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNqtoX1dB7cT"
      },
      "source": [
        "## Utils Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v3sa-c1kCP4l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def fit_one_epoch(model_train, model, ema, yolo_loss, loss_history, eval_callback, optimizer, epoch, epoch_step, epoch_step_val, gen, gen_val, Epoch, cuda, fp16, scaler, save_period, save_dir, local_rank=0):\n",
        "    loss        = 0\n",
        "    val_loss    = 0\n",
        "\n",
        "    if local_rank == 0:\n",
        "        print('Start Train')\n",
        "        pbar = tqdm(total=epoch_step,desc=f'Epoch {epoch + 1}/{Epoch}',postfix=dict,mininterval=0.3)\n",
        "    model_train.train()\n",
        "    for iteration, batch in enumerate(gen):\n",
        "        if iteration >= epoch_step:\n",
        "            break\n",
        "\n",
        "        images, bboxes = batch\n",
        "        with torch.no_grad():\n",
        "            if cuda:\n",
        "                images = images.cuda(local_rank)\n",
        "                bboxes = bboxes.cuda(local_rank)\n",
        "        #----------------------#\n",
        "        #   \n",
        "        #----------------------#\n",
        "        optimizer.zero_grad()\n",
        "        if not fp16:\n",
        "            #----------------------#\n",
        "            #   \n",
        "            #----------------------#\n",
        "            # dbox, cls, origin_cls, anchors, strides\n",
        "            outputs = model_train(images)\n",
        "            loss_value = yolo_loss(outputs, bboxes)\n",
        "            #----------------------#\n",
        "            #   \n",
        "            #----------------------#\n",
        "            loss_value.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model_train.parameters(), max_norm=10.0)  # clip gradients\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            from torch.cuda.amp import autocast\n",
        "            with autocast():\n",
        "                #----------------------#\n",
        "                #   \n",
        "                #----------------------#\n",
        "                outputs         = model_train(images)\n",
        "                loss_value = yolo_loss(outputs, bboxes)\n",
        "\n",
        "            #----------------------#\n",
        "            #   \n",
        "            #----------------------#\n",
        "            scaler.scale(loss_value).backward()\n",
        "            scaler.unscale_(optimizer)  # unscale gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model_train.parameters(), max_norm=10.0)  # clip gradients\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        if ema:\n",
        "            ema.update(model_train)\n",
        "\n",
        "        loss += loss_value.item()\n",
        "\n",
        "        if local_rank == 0:\n",
        "            pbar.set_postfix(**{'loss'  : loss / (iteration + 1),\n",
        "                                'lr'    : get_lr(optimizer)})\n",
        "            pbar.update(1)\n",
        "\n",
        "    if local_rank == 0:\n",
        "        pbar.close()\n",
        "        print('Finish Train')\n",
        "        print('Start Validation')\n",
        "        pbar = tqdm(total=epoch_step_val, desc=f'Epoch {epoch + 1}/{Epoch}',postfix=dict,mininterval=0.3)\n",
        "\n",
        "    if ema:\n",
        "        model_train_eval = ema.ema\n",
        "    else:\n",
        "        model_train_eval = model_train.eval()\n",
        "\n",
        "    for iteration, batch in enumerate(gen_val):\n",
        "        if iteration >= epoch_step_val:\n",
        "            break\n",
        "        images, bboxes = batch[0], batch[1]\n",
        "        with torch.no_grad():\n",
        "            if cuda:\n",
        "                images = images.cuda(local_rank)\n",
        "                bboxes = bboxes.cuda(local_rank)\n",
        "            #----------------------#\n",
        "            #   \n",
        "            #----------------------#\n",
        "            optimizer.zero_grad()\n",
        "            #----------------------#\n",
        "            #   \n",
        "            #----------------------#\n",
        "            outputs     = model_train_eval(images)\n",
        "            loss_value  = yolo_loss(outputs, bboxes)\n",
        "\n",
        "        val_loss += loss_value.item()\n",
        "        if local_rank == 0:\n",
        "            pbar.set_postfix(**{'val_loss': val_loss / (iteration + 1)})\n",
        "            pbar.update(1)\n",
        "\n",
        "    if local_rank == 0:\n",
        "        pbar.close()\n",
        "        print('Finish Validation')\n",
        "        loss_history.append_loss(epoch + 1, loss / epoch_step, val_loss / epoch_step_val)\n",
        "        eval_callback.on_epoch_end(epoch + 1, model_train_eval)\n",
        "        print('Epoch:'+ str(epoch + 1) + '/' + str(Epoch))\n",
        "        print('Total Loss: %.3f || Val Loss: %.3f ' % (loss / epoch_step, val_loss / epoch_step_val))\n",
        "\n",
        "        #-----------------------------------------------#\n",
        "        #   \n",
        "        #-----------------------------------------------#\n",
        "        if ema:\n",
        "            save_state_dict = ema.ema.state_dict()\n",
        "        else:\n",
        "            save_state_dict = model.state_dict()\n",
        "\n",
        "        if (epoch + 1) % save_period == 0 or epoch + 1 == Epoch:\n",
        "            torch.save(save_state_dict, os.path.join(save_dir, \"ep%03d-loss%.3f-val_loss%.3f.pth\" % (epoch + 1, loss / epoch_step, val_loss / epoch_step_val)))\n",
        "\n",
        "        if len(loss_history.val_loss) <= 1 or (val_loss / epoch_step_val) <= min(loss_history.val_loss):\n",
        "            print('Save best model to best_epoch_weights.pth')\n",
        "            torch.save(save_state_dict, os.path.join(save_dir, \"best_epoch_weights.pth\"))\n",
        "\n",
        "        torch.save(save_state_dict, os.path.join(save_dir, \"last_epoch_weights.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwEIM_6sB4Ud"
      },
      "source": [
        "## Utils BBox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "p4iL3sNPCYzp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.ops import nms\n",
        "import pkg_resources as pkg\n",
        "\n",
        "def check_version(current: str = \"0.0.0\",\n",
        "                  minimum: str = \"0.0.0\",\n",
        "                  name: str = \"version \",\n",
        "                  pinned: bool = False) -> bool:\n",
        "    current, minimum = (pkg.parse_version(x) for x in (current, minimum))\n",
        "    result = (current == minimum) if pinned else (current >= minimum)  # bool\n",
        "    return result\n",
        "\n",
        "TORCH_1_10 = check_version(torch.__version__, '1.10.0')\n",
        "\n",
        "def make_anchors(feats, strides, grid_cell_offset=0.5):\n",
        "    \"\"\"Generate anchors from features.\"\"\"\n",
        "    anchor_points, stride_tensor = [], []\n",
        "    assert feats is not None\n",
        "    dtype, device = feats[0].dtype, feats[0].device\n",
        "    for i, stride in enumerate(strides):\n",
        "        _, _, h, w  = feats[i].shape\n",
        "        sx          = torch.arange(end=w, device=device, dtype=dtype) + grid_cell_offset  # shift x\n",
        "        sy          = torch.arange(end=h, device=device, dtype=dtype) + grid_cell_offset  # shift y\n",
        "        sy, sx      = torch.meshgrid(sy, sx, indexing='ij') if TORCH_1_10 else torch.meshgrid(sy, sx)\n",
        "        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
        "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n",
        "    return torch.cat(anchor_points), torch.cat(stride_tensor)\n",
        "\n",
        "def dist2bbox(distance, anchor_points, xywh=True, dim=-1):\n",
        "    \"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\n",
        "    # \n",
        "    lt, rb  = torch.split(distance, 2, dim)\n",
        "    x1y1    = anchor_points - lt\n",
        "    x2y2    = anchor_points + rb\n",
        "    if xywh:\n",
        "        c_xy    = (x1y1 + x2y2) / 2\n",
        "        wh      = x2y2 - x1y1\n",
        "        return torch.cat((c_xy, wh), dim)  # xywh bbox\n",
        "    return torch.cat((x1y1, x2y2), dim)  # xyxy bbox\n",
        "\n",
        "class DecodeBox():\n",
        "    def __init__(self, num_classes, input_shape):\n",
        "        super(DecodeBox, self).__init__()\n",
        "        self.num_classes    = num_classes\n",
        "        self.bbox_attrs     = 4 + num_classes\n",
        "        self.input_shape    = input_shape\n",
        "\n",
        "    def decode_box(self, inputs):\n",
        "        # dbox  batch_size, 4, 8400\n",
        "        # cls   batch_size, 20, 8400\n",
        "        dbox, cls, origin_cls, anchors, strides = inputs\n",
        "        # \n",
        "        dbox    = dist2bbox(dbox, anchors.unsqueeze(0), xywh=True, dim=1) * strides\n",
        "        y       = torch.cat((dbox, cls.sigmoid()), 1).permute(0, 2, 1)\n",
        "        # 0~1\n",
        "        y[:, :, :4] = y[:, :, :4] / torch.Tensor([self.input_shape[1], self.input_shape[0], self.input_shape[1], self.input_shape[0]]).to(y.device)\n",
        "        return y\n",
        "\n",
        "    def yolo_correct_boxes(self, box_xy, box_wh, input_shape, image_shape, letterbox_image):\n",
        "        #-----------------------------------------------------------------#\n",
        "        #   y\n",
        "        #-----------------------------------------------------------------#\n",
        "        box_yx = box_xy[..., ::-1]\n",
        "        box_hw = box_wh[..., ::-1]\n",
        "        input_shape = np.array(input_shape)\n",
        "        image_shape = np.array(image_shape)\n",
        "\n",
        "        if letterbox_image:\n",
        "            #-----------------------------------------------------------------#\n",
        "            #   offset\n",
        "            #   new_shape\n",
        "            #-----------------------------------------------------------------#\n",
        "            new_shape = np.round(image_shape * np.min(input_shape/image_shape))\n",
        "            offset  = (input_shape - new_shape)/2./input_shape\n",
        "            scale   = input_shape/new_shape\n",
        "\n",
        "            box_yx  = (box_yx - offset) * scale\n",
        "            box_hw *= scale\n",
        "\n",
        "        box_mins    = box_yx - (box_hw / 2.)\n",
        "        box_maxes   = box_yx + (box_hw / 2.)\n",
        "        boxes  = np.concatenate([box_mins[..., 0:1], box_mins[..., 1:2], box_maxes[..., 0:1], box_maxes[..., 1:2]], axis=-1)\n",
        "        boxes *= np.concatenate([image_shape, image_shape], axis=-1)\n",
        "        return boxes\n",
        "\n",
        "    def non_max_suppression(self, prediction, num_classes, input_shape, image_shape, letterbox_image, conf_thres=0.5, nms_thres=0.4):\n",
        "        #----------------------------------------------------------#\n",
        "        #   \n",
        "        #   prediction  [batch_size, num_anchors, 85]\n",
        "        #----------------------------------------------------------#\n",
        "        box_corner          = prediction.new(prediction.shape)\n",
        "        box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
        "        box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
        "        box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
        "        box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
        "        prediction[:, :, :4] = box_corner[:, :, :4]\n",
        "\n",
        "        output = [None for _ in range(len(prediction))]\n",
        "        for i, image_pred in enumerate(prediction):\n",
        "            #----------------------------------------------------------#\n",
        "            #   max\n",
        "            #   class_conf  [num_anchors, 1]    \n",
        "            #   class_pred  [num_anchors, 1]    \n",
        "            #----------------------------------------------------------#\n",
        "            class_conf, class_pred = torch.max(image_pred[:, 4:4 + num_classes], 1, keepdim=True)\n",
        "\n",
        "            #----------------------------------------------------------#\n",
        "            #   \n",
        "            #----------------------------------------------------------#\n",
        "            conf_mask = (class_conf[:, 0] >= conf_thres).squeeze()\n",
        "\n",
        "            #----------------------------------------------------------#\n",
        "            #   \n",
        "            #----------------------------------------------------------#\n",
        "            image_pred = image_pred[conf_mask]\n",
        "            class_conf = class_conf[conf_mask]\n",
        "            class_pred = class_pred[conf_mask]\n",
        "            if not image_pred.size(0):\n",
        "                continue\n",
        "            #-------------------------------------------------------------------------#\n",
        "            #   detections  [num_anchors, 6]\n",
        "            #   6x1, y1, x2, y2, class_conf, class_pred\n",
        "            #-------------------------------------------------------------------------#\n",
        "            detections = torch.cat((image_pred[:, :4], class_conf.float(), class_pred.float()), 1)\n",
        "\n",
        "            #------------------------------------------#\n",
        "            #   \n",
        "            #------------------------------------------#\n",
        "            unique_labels = detections[:, -1].cpu().unique()\n",
        "\n",
        "            if prediction.is_cuda:\n",
        "                unique_labels = unique_labels.cuda()\n",
        "                detections = detections.cuda()\n",
        "\n",
        "            for c in unique_labels:\n",
        "                #------------------------------------------#\n",
        "                #   \n",
        "                #------------------------------------------#\n",
        "                detections_class = detections[detections[:, -1] == c]\n",
        "                #------------------------------------------#\n",
        "                #   \n",
        "                #   \n",
        "                #------------------------------------------#\n",
        "                keep = nms(\n",
        "                    detections_class[:, :4],\n",
        "                    detections_class[:, 4],\n",
        "                    nms_thres\n",
        "                )\n",
        "                max_detections = detections_class[keep]\n",
        "\n",
        "                # # \n",
        "                # _, conf_sort_index = torch.sort(detections_class[:, 4]*detections_class[:, 5], descending=True)\n",
        "                # detections_class = detections_class[conf_sort_index]\n",
        "                # # \n",
        "                # max_detections = []\n",
        "                # while detections_class.size(0):\n",
        "                #     # nms_thres\n",
        "                #     max_detections.append(detections_class[0].unsqueeze(0))\n",
        "                #     if len(detections_class) == 1:\n",
        "                #         break\n",
        "                #     ious = bbox_iou(max_detections[-1], detections_class[1:])\n",
        "                #     detections_class = detections_class[1:][ious < nms_thres]\n",
        "                # # \n",
        "                # max_detections = torch.cat(max_detections).data\n",
        "\n",
        "                # Add max detections to outputs\n",
        "                output[i] = max_detections if output[i] is None else torch.cat((output[i], max_detections))\n",
        "\n",
        "            if output[i] is not None:\n",
        "                output[i]           = output[i].cpu().numpy()\n",
        "                box_xy, box_wh      = (output[i][:, 0:2] + output[i][:, 2:4])/2, output[i][:, 2:4] - output[i][:, 0:2]\n",
        "                output[i][:, :4]    = self.yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image)\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-twtVbwB1y0"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pbC3wnROpQvF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data.dataset import Dataset\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, annotation_lines, input_shape, num_classes, epoch_length, \\\n",
        "                        mosaic, mixup, mosaic_prob, mixup_prob, train, special_aug_ratio = 0.7):\n",
        "        super(YoloDataset, self).__init__()\n",
        "        self.annotation_lines   = annotation_lines\n",
        "        self.input_shape        = input_shape\n",
        "        self.num_classes        = num_classes\n",
        "        self.epoch_length       = epoch_length\n",
        "        self.mosaic             = mosaic\n",
        "        self.mosaic_prob        = mosaic_prob\n",
        "        self.mixup              = mixup\n",
        "        self.mixup_prob         = mixup_prob\n",
        "        self.train              = train\n",
        "        self.special_aug_ratio  = special_aug_ratio\n",
        "\n",
        "        self.epoch_now          = -1\n",
        "        self.length             = len(self.annotation_lines)\n",
        "\n",
        "        self.bbox_attrs         = 5 + num_classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index       = index % self.length\n",
        "\n",
        "        #---------------------------------------------------#\n",
        "        #   \n",
        "        #   \n",
        "        #---------------------------------------------------#\n",
        "        if self.mosaic and self.rand() < self.mosaic_prob and self.epoch_now < self.epoch_length * self.special_aug_ratio:\n",
        "            lines = random.sample(self.annotation_lines, 3)\n",
        "            lines.append(self.annotation_lines[index])\n",
        "            random.shuffle(lines)\n",
        "            image, box  = self.get_random_data_with_Mosaic(lines, self.input_shape)\n",
        "\n",
        "            if self.mixup and self.rand() < self.mixup_prob:\n",
        "                lines           = random.sample(self.annotation_lines, 1)\n",
        "                image_2, box_2  = self.get_random_data(lines[0], self.input_shape, random = self.train)\n",
        "                image, box      = self.get_random_data_with_MixUp(image, box, image_2, box_2)\n",
        "        else:\n",
        "            image, box      = self.get_random_data(self.annotation_lines[index], self.input_shape, random = self.train)\n",
        "\n",
        "        image       = np.transpose(preprocess_input(np.array(image, dtype=np.float32)), (2, 0, 1))\n",
        "        box         = np.array(box, dtype=np.float32)\n",
        "\n",
        "        #---------------------------------------------------#\n",
        "        #   \n",
        "        #---------------------------------------------------#\n",
        "        nL          = len(box)\n",
        "        labels_out  = np.zeros((nL, 6))\n",
        "        if nL:\n",
        "            #---------------------------------------------------#\n",
        "            #   0-1\n",
        "            #---------------------------------------------------#\n",
        "            box[:, [0, 2]] = box[:, [0, 2]] / self.input_shape[1]\n",
        "            box[:, [1, 3]] = box[:, [1, 3]] / self.input_shape[0]\n",
        "            #---------------------------------------------------#\n",
        "            #   01\n",
        "            #   23\n",
        "            #   4\n",
        "            #---------------------------------------------------#\n",
        "            box[:, 2:4] = box[:, 2:4] - box[:, 0:2]\n",
        "            box[:, 0:2] = box[:, 0:2] + box[:, 2:4] / 2\n",
        "\n",
        "            #---------------------------------------------------#\n",
        "            #   \n",
        "            #   labels_out0collect\n",
        "            #---------------------------------------------------#\n",
        "            labels_out[:, 1] = box[:, -1]\n",
        "            labels_out[:, 2:] = box[:, :4]\n",
        "\n",
        "        return image, labels_out\n",
        "\n",
        "    def rand(self, a=0, b=1):\n",
        "        return np.random.rand()*(b-a) + a\n",
        "\n",
        "    def get_random_data(self, annotation_line, input_shape, jitter=.3, hue=.1, sat=0.7, val=0.4, random=True):\n",
        "        line    = annotation_line.split()\n",
        "        #------------------------------#\n",
        "        #   RGB\n",
        "        #------------------------------#\n",
        "        image   = Image.open(line[0])\n",
        "        image   = cvtColor(image)\n",
        "        #------------------------------#\n",
        "        #   \n",
        "        #------------------------------#\n",
        "        iw, ih  = image.size\n",
        "        h, w    = input_shape\n",
        "        #------------------------------#\n",
        "        #   \n",
        "        #------------------------------#\n",
        "        # box     = np.array([np.array(list(map(int,bbox.split(',')))) for bbox in line[1:]])\n",
        "        boxes = []\n",
        "        for box_str in line[1:]:\n",
        "            box_values = re.findall(r'(\\d+)', box_str)  # Finds all numeric values in the string\n",
        "            if len(box_values) == 5:  # Ensure it has 5 elements (x_min, y_min, x_max, y_max, class_id)\n",
        "                x_min, y_min, x_max, y_max, class_id = map(int, box_values)\n",
        "                boxes.append([x_min, y_min, x_max, y_max, class_id])\n",
        "\n",
        "        box = np.array(boxes)\n",
        "        # print(\"Parsed bounding boxes:\", box)\n",
        "        if not random:\n",
        "            scale = min(w/iw, h/ih)\n",
        "            nw = int(iw*scale)\n",
        "            nh = int(ih*scale)\n",
        "            dx = (w-nw)//2\n",
        "            dy = (h-nh)//2\n",
        "\n",
        "            #---------------------------------#\n",
        "            #   \n",
        "            #---------------------------------#\n",
        "            image       = image.resize((nw,nh), Image.BICUBIC)\n",
        "            new_image   = Image.new('RGB', (w,h), (128,128,128))\n",
        "            new_image.paste(image, (dx, dy))\n",
        "            image_data  = np.array(new_image, np.float32)\n",
        "\n",
        "            #---------------------------------#\n",
        "            #   \n",
        "            #---------------------------------#\n",
        "            if len(box)>0:\n",
        "                np.random.shuffle(box)\n",
        "                box[:, [0,2]] = box[:, [0,2]]*nw/iw + dx\n",
        "                box[:, [1,3]] = box[:, [1,3]]*nh/ih + dy\n",
        "                box[:, 0:2][box[:, 0:2]<0] = 0\n",
        "                box[:, 2][box[:, 2]>w] = w\n",
        "                box[:, 3][box[:, 3]>h] = h\n",
        "                box_w = box[:, 2] - box[:, 0]\n",
        "                box_h = box[:, 3] - box[:, 1]\n",
        "                box = box[np.logical_and(box_w>1, box_h>1)] # discard invalid box\n",
        "\n",
        "            return image_data, box\n",
        "\n",
        "        #------------------------------------------#\n",
        "        #   \n",
        "        #------------------------------------------#\n",
        "        new_ar = iw/ih * self.rand(1-jitter,1+jitter) / self.rand(1-jitter,1+jitter)\n",
        "        scale = self.rand(.25, 2)\n",
        "        if new_ar < 1:\n",
        "            nh = int(scale*h)\n",
        "            nw = int(nh*new_ar)\n",
        "        else:\n",
        "            nw = int(scale*w)\n",
        "            nh = int(nw/new_ar)\n",
        "        image = image.resize((nw,nh), Image.BICUBIC)\n",
        "\n",
        "        #------------------------------------------#\n",
        "        #   \n",
        "        #------------------------------------------#\n",
        "        dx = int(self.rand(0, w-nw))\n",
        "        dy = int(self.rand(0, h-nh))\n",
        "        new_image = Image.new('RGB', (w,h), (128,128,128))\n",
        "        new_image.paste(image, (dx, dy))\n",
        "        image = new_image\n",
        "\n",
        "        #------------------------------------------#\n",
        "        #   \n",
        "        #------------------------------------------#\n",
        "        flip = self.rand()<.5\n",
        "        if flip: image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "        image_data      = np.array(image, np.uint8)\n",
        "        #---------------------------------#\n",
        "        #   \n",
        "        #   \n",
        "        #---------------------------------#\n",
        "        r               = np.random.uniform(-1, 1, 3) * [hue, sat, val] + 1\n",
        "        #---------------------------------#\n",
        "        #   HSV\n",
        "        #---------------------------------#\n",
        "        hue, sat, val   = cv2.split(cv2.cvtColor(image_data, cv2.COLOR_RGB2HSV))\n",
        "        dtype           = image_data.dtype\n",
        "        #---------------------------------#\n",
        "        #   \n",
        "        #---------------------------------#\n",
        "        x       = np.arange(0, 256, dtype=r.dtype)\n",
        "        lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
        "        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
        "        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
        "\n",
        "        image_data = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\n",
        "        image_data = cv2.cvtColor(image_data, cv2.COLOR_HSV2RGB)\n",
        "\n",
        "        #---------------------------------#\n",
        "        #   \n",
        "        #---------------------------------#\n",
        "        if len(box)>0:\n",
        "            np.random.shuffle(box)\n",
        "            box[:, [0,2]] = box[:, [0,2]]*nw/iw + dx\n",
        "            box[:, [1,3]] = box[:, [1,3]]*nh/ih + dy\n",
        "            if flip: box[:, [0,2]] = w - box[:, [2,0]]\n",
        "            box[:, 0:2][box[:, 0:2]<0] = 0\n",
        "            box[:, 2][box[:, 2]>w] = w\n",
        "            box[:, 3][box[:, 3]>h] = h\n",
        "            box_w = box[:, 2] - box[:, 0]\n",
        "            box_h = box[:, 3] - box[:, 1]\n",
        "            box = box[np.logical_and(box_w>1, box_h>1)]\n",
        "\n",
        "        return image_data, box\n",
        "\n",
        "    def merge_bboxes(self, bboxes, cutx, cuty):\n",
        "        merge_bbox = []\n",
        "        for i in range(len(bboxes)):\n",
        "            for box in bboxes[i]:\n",
        "                tmp_box = []\n",
        "                x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n",
        "\n",
        "                if i == 0:\n",
        "                    if y1 > cuty or x1 > cutx:\n",
        "                        continue\n",
        "                    if y2 >= cuty and y1 <= cuty:\n",
        "                        y2 = cuty\n",
        "                    if x2 >= cutx and x1 <= cutx:\n",
        "                        x2 = cutx\n",
        "\n",
        "                if i == 1:\n",
        "                    if y2 < cuty or x1 > cutx:\n",
        "                        continue\n",
        "                    if y2 >= cuty and y1 <= cuty:\n",
        "                        y1 = cuty\n",
        "                    if x2 >= cutx and x1 <= cutx:\n",
        "                        x2 = cutx\n",
        "\n",
        "                if i == 2:\n",
        "                    if y2 < cuty or x2 < cutx:\n",
        "                        continue\n",
        "                    if y2 >= cuty and y1 <= cuty:\n",
        "                        y1 = cuty\n",
        "                    if x2 >= cutx and x1 <= cutx:\n",
        "                        x1 = cutx\n",
        "\n",
        "                if i == 3:\n",
        "                    if y1 > cuty or x2 < cutx:\n",
        "                        continue\n",
        "                    if y2 >= cuty and y1 <= cuty:\n",
        "                        y2 = cuty\n",
        "                    if x2 >= cutx and x1 <= cutx:\n",
        "                        x1 = cutx\n",
        "                tmp_box.append(x1)\n",
        "                tmp_box.append(y1)\n",
        "                tmp_box.append(x2)\n",
        "                tmp_box.append(y2)\n",
        "                tmp_box.append(box[-1])\n",
        "                merge_bbox.append(tmp_box)\n",
        "        return merge_bbox\n",
        "\n",
        "    def get_random_data_with_Mosaic(self, annotation_line, input_shape, jitter=0.3, hue=.1, sat=0.7, val=0.4):\n",
        "        h, w = input_shape\n",
        "        min_offset_x = self.rand(0.3, 0.7)\n",
        "        min_offset_y = self.rand(0.3, 0.7)\n",
        "\n",
        "        image_datas = []\n",
        "        box_datas   = []\n",
        "        index       = 0\n",
        "        for line in annotation_line:\n",
        "            #---------------------------------#\n",
        "            #   \n",
        "            #---------------------------------#\n",
        "            line_content = line.split()\n",
        "            #---------------------------------#\n",
        "            #   \n",
        "            #---------------------------------#\n",
        "            image = Image.open(line_content[0])\n",
        "            image = cvtColor(image)\n",
        "\n",
        "            #---------------------------------#\n",
        "            #   \n",
        "            #---------------------------------#\n",
        "            iw, ih = image.size\n",
        "            #---------------------------------#\n",
        "            #   \n",
        "            #---------------------------------#\n",
        "            # box = np.array([np.array(list(map(int,bbox.split(',')))) for bbox in line_content[1:]])\n",
        "            boxes = []\n",
        "            for box_str in line[1:]:\n",
        "                box_values = re.findall(r'(\\d+)', box_str)  # Finds all numeric values in the string\n",
        "                if len(box_values) == 5:  # Ensure it has 5 elements (x_min, y_min, x_max, y_max, class_id)\n",
        "                    x_min, y_min, x_max, y_max, class_id = map(int, box_values)\n",
        "                    boxes.append([x_min, y_min, x_max, y_max, class_id])\n",
        "\n",
        "            box = np.array(boxes)\n",
        "            #---------------------------------#\n",
        "            #   \n",
        "            #---------------------------------#\n",
        "            flip = self.rand()<.5\n",
        "            if flip and len(box)>0:\n",
        "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                box[:, [0,2]] = iw - box[:, [2,0]]\n",
        "\n",
        "            #------------------------------------------#\n",
        "            #   \n",
        "            #------------------------------------------#\n",
        "            new_ar = iw/ih * self.rand(1-jitter,1+jitter) / self.rand(1-jitter,1+jitter)\n",
        "            scale = self.rand(.4, 1)\n",
        "            if new_ar < 1:\n",
        "                nh = int(scale*h)\n",
        "                nw = int(nh*new_ar)\n",
        "            else:\n",
        "                nw = int(scale*w)\n",
        "                nh = int(nw/new_ar)\n",
        "            image = image.resize((nw, nh), Image.BICUBIC)\n",
        "\n",
        "            #-----------------------------------------------#\n",
        "            #   \n",
        "            #-----------------------------------------------#\n",
        "            if index == 0:\n",
        "                dx = int(w*min_offset_x) - nw\n",
        "                dy = int(h*min_offset_y) - nh\n",
        "            elif index == 1:\n",
        "                dx = int(w*min_offset_x) - nw\n",
        "                dy = int(h*min_offset_y)\n",
        "            elif index == 2:\n",
        "                dx = int(w*min_offset_x)\n",
        "                dy = int(h*min_offset_y)\n",
        "            elif index == 3:\n",
        "                dx = int(w*min_offset_x)\n",
        "                dy = int(h*min_offset_y) - nh\n",
        "\n",
        "            new_image = Image.new('RGB', (w,h), (128,128,128))\n",
        "            new_image.paste(image, (dx, dy))\n",
        "            image_data = np.array(new_image)\n",
        "\n",
        "            index = index + 1\n",
        "            box_data = []\n",
        "            #---------------------------------#\n",
        "            #   box\n",
        "            #---------------------------------#\n",
        "            if len(box)>0:\n",
        "                np.random.shuffle(box)\n",
        "                box[:, [0,2]] = box[:, [0,2]]*nw/iw + dx\n",
        "                box[:, [1,3]] = box[:, [1,3]]*nh/ih + dy\n",
        "                box[:, 0:2][box[:, 0:2]<0] = 0\n",
        "                box[:, 2][box[:, 2]>w] = w\n",
        "                box[:, 3][box[:, 3]>h] = h\n",
        "                box_w = box[:, 2] - box[:, 0]\n",
        "                box_h = box[:, 3] - box[:, 1]\n",
        "                box = box[np.logical_and(box_w>1, box_h>1)]\n",
        "                box_data = np.zeros((len(box),5))\n",
        "                box_data[:len(box)] = box\n",
        "\n",
        "            image_datas.append(image_data)\n",
        "            box_datas.append(box_data)\n",
        "\n",
        "        #---------------------------------#\n",
        "        #   \n",
        "        #---------------------------------#\n",
        "        cutx = int(w * min_offset_x)\n",
        "        cuty = int(h * min_offset_y)\n",
        "\n",
        "        new_image = np.zeros([h, w, 3])\n",
        "        new_image[:cuty, :cutx, :] = image_datas[0][:cuty, :cutx, :]\n",
        "        new_image[cuty:, :cutx, :] = image_datas[1][cuty:, :cutx, :]\n",
        "        new_image[cuty:, cutx:, :] = image_datas[2][cuty:, cutx:, :]\n",
        "        new_image[:cuty, cutx:, :] = image_datas[3][:cuty, cutx:, :]\n",
        "\n",
        "        new_image       = np.array(new_image, np.uint8)\n",
        "        #---------------------------------#\n",
        "        #   \n",
        "        #   \n",
        "        #---------------------------------#\n",
        "        r               = np.random.uniform(-1, 1, 3) * [hue, sat, val] + 1\n",
        "        #---------------------------------#\n",
        "        #   HSV\n",
        "        #---------------------------------#\n",
        "        hue, sat, val   = cv2.split(cv2.cvtColor(new_image, cv2.COLOR_RGB2HSV))\n",
        "        dtype           = new_image.dtype\n",
        "        #---------------------------------#\n",
        "        #   \n",
        "        #---------------------------------#\n",
        "        x       = np.arange(0, 256, dtype=r.dtype)\n",
        "        lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
        "        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
        "        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
        "\n",
        "        new_image = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\n",
        "        new_image = cv2.cvtColor(new_image, cv2.COLOR_HSV2RGB)\n",
        "\n",
        "        #---------------------------------#\n",
        "        #   \n",
        "        #---------------------------------#\n",
        "        new_boxes = self.merge_bboxes(box_datas, cutx, cuty)\n",
        "\n",
        "        return new_image, new_boxes\n",
        "\n",
        "    def get_random_data_with_MixUp(self, image_1, box_1, image_2, box_2):\n",
        "        new_image = np.array(image_1, np.float32) * 0.5 + np.array(image_2, np.float32) * 0.5\n",
        "        if len(box_1) == 0:\n",
        "            new_boxes = box_2\n",
        "        elif len(box_2) == 0:\n",
        "            new_boxes = box_1\n",
        "        else:\n",
        "            new_boxes = np.concatenate([box_1, box_2], axis=0)\n",
        "        return new_image, new_boxes\n",
        "\n",
        "\n",
        "# DataLoadercollate_fn\n",
        "def yolo_dataset_collate(batch):\n",
        "    images  = []\n",
        "    bboxes  = []\n",
        "    for i, (img, box) in enumerate(batch):\n",
        "        images.append(img)\n",
        "        box[:, 0] = i\n",
        "        bboxes.append(box)\n",
        "\n",
        "    images  = torch.from_numpy(np.array(images)).type(torch.FloatTensor)\n",
        "    bboxes  = torch.from_numpy(np.concatenate(bboxes, 0)).type(torch.FloatTensor)\n",
        "    return images, bboxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUw5Ph9qBzjL"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GrNlC5JMCcvk"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import scipy.signal\n",
        "from matplotlib import pyplot as plt\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class LossHistory():\n",
        "    def __init__(self, log_dir, model, input_shape):\n",
        "        self.log_dir    = log_dir\n",
        "        self.losses     = []\n",
        "        self.val_loss   = []\n",
        "\n",
        "        os.makedirs(self.log_dir)\n",
        "        self.writer     = SummaryWriter(self.log_dir)\n",
        "        # try:\n",
        "        #     dummy_input     = torch.randn(2, 3, input_shape[0], input_shape[1])\n",
        "        #     self.writer.add_graph(model, dummy_input)\n",
        "        # except:\n",
        "        #     pass\n",
        "\n",
        "    def append_loss(self, epoch, loss, val_loss):\n",
        "        if not os.path.exists(self.log_dir):\n",
        "            os.makedirs(self.log_dir)\n",
        "\n",
        "        self.losses.append(loss)\n",
        "        self.val_loss.append(val_loss)\n",
        "\n",
        "        with open(os.path.join(self.log_dir, \"epoch_loss.txt\"), 'a') as f:\n",
        "            f.write(str(loss))\n",
        "            f.write(\"\\n\")\n",
        "        with open(os.path.join(self.log_dir, \"epoch_val_loss.txt\"), 'a') as f:\n",
        "            f.write(str(val_loss))\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "        self.writer.add_scalar('loss', loss, epoch)\n",
        "        self.writer.add_scalar('val_loss', val_loss, epoch)\n",
        "        self.loss_plot()\n",
        "\n",
        "    def loss_plot(self):\n",
        "        iters = range(len(self.losses))\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(iters, self.losses, 'red', linewidth = 2, label='train loss')\n",
        "        plt.plot(iters, self.val_loss, 'coral', linewidth = 2, label='val loss')\n",
        "        # try:\n",
        "        #     if len(self.losses) < 25:\n",
        "        #         num = 5\n",
        "        #     else:\n",
        "        #         num = 15\n",
        "\n",
        "        #     plt.plot(iters, scipy.signal.savgol_filter(self.losses, num, 3), 'green', linestyle = '--', linewidth = 2, label='smooth train loss')\n",
        "        #     plt.plot(iters, scipy.signal.savgol_filter(self.val_loss, num, 3), '#8B4513', linestyle = '--', linewidth = 2, label='smooth val loss')\n",
        "        # except:\n",
        "        #     pass\n",
        "\n",
        "        plt.grid(True)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend(loc=\"upper right\")\n",
        "\n",
        "        plt.savefig(os.path.join(self.log_dir, \"epoch_loss.png\"))\n",
        "\n",
        "        plt.cla()\n",
        "        plt.close(\"all\")\n",
        "\n",
        "class EvalCallback():\n",
        "    def __init__(self, net, input_shape, class_names, num_classes, val_lines, log_dir, cuda, \\\n",
        "            map_out_path=\".temp_map_out\", max_boxes=100, confidence=0.05, nms_iou=0.5, letterbox_image=True, MINOVERLAP=0.5, eval_flag=True, period=1):\n",
        "        super(EvalCallback, self).__init__()\n",
        "\n",
        "        self.net                = net\n",
        "        self.input_shape        = input_shape\n",
        "        self.class_names        = class_names\n",
        "        self.num_classes        = num_classes\n",
        "        self.val_lines          = val_lines\n",
        "        self.log_dir            = log_dir\n",
        "        self.cuda               = cuda\n",
        "        self.map_out_path       = map_out_path\n",
        "        self.max_boxes          = max_boxes\n",
        "        self.confidence         = confidence\n",
        "        self.nms_iou            = nms_iou\n",
        "        self.letterbox_image    = letterbox_image\n",
        "        self.MINOVERLAP         = MINOVERLAP\n",
        "        self.eval_flag          = eval_flag\n",
        "        self.period             = period\n",
        "\n",
        "        self.bbox_util          = DecodeBox(self.num_classes, (self.input_shape[0], self.input_shape[1]))\n",
        "\n",
        "        self.maps       = [0]\n",
        "        self.epoches    = [0]\n",
        "        if self.eval_flag:\n",
        "            with open(os.path.join(self.log_dir, \"epoch_map.txt\"), 'a') as f:\n",
        "                f.write(str(0))\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "    def get_map_txt(self, image_id, image, class_names, map_out_path):\n",
        "        f = open(os.path.join(map_out_path, \"detection-results/\"+image_id+\".txt\"), \"w\", encoding='utf-8')\n",
        "        image_shape = np.array(np.shape(image)[0:2])\n",
        "        image       = cvtColor(image)\n",
        "        image_data  = resize_image(image, (self.input_shape[1], self.input_shape[0]), self.letterbox_image)\n",
        "\n",
        "        image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, dtype='float32')), (2, 0, 1)), 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            images = torch.from_numpy(image_data)\n",
        "            if self.cuda:\n",
        "                images = images.cuda()\n",
        "\n",
        "            outputs = self.net(images)\n",
        "            outputs = self.bbox_util.decode_box(outputs)\n",
        "\n",
        "            results = self.bbox_util.non_max_suppression(outputs, self.num_classes, self.input_shape,\n",
        "                        image_shape, self.letterbox_image, conf_thres = self.confidence, nms_thres = self.nms_iou)\n",
        "\n",
        "            if results[0] is None:\n",
        "                return\n",
        "\n",
        "            top_label   = np.array(results[0][:, 5], dtype = 'int32')\n",
        "            top_conf    = results[0][:, 4]\n",
        "            top_boxes   = results[0][:, :4]\n",
        "\n",
        "        top_100     = np.argsort(top_conf)[::-1][:self.max_boxes]\n",
        "        top_boxes   = top_boxes[top_100]\n",
        "        top_conf    = top_conf[top_100]\n",
        "        top_label   = top_label[top_100]\n",
        "\n",
        "        for i, c in list(enumerate(top_label)):\n",
        "            predicted_class = self.class_names[int(c)]\n",
        "            box             = top_boxes[i]\n",
        "            score           = str(top_conf[i])\n",
        "\n",
        "            top, left, bottom, right = box\n",
        "            if predicted_class not in class_names:\n",
        "                continue\n",
        "\n",
        "            f.write(\"%s %s %s %s %s %s\\n\" % (predicted_class, score[:6], str(int(left)), str(int(top)), str(int(right)),str(int(bottom))))\n",
        "\n",
        "        f.close()\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, model_eval):\n",
        "        if epoch % self.period == 0 and self.eval_flag:\n",
        "            self.net = model_eval\n",
        "            if not os.path.exists(self.map_out_path):\n",
        "                os.makedirs(self.map_out_path)\n",
        "            if not os.path.exists(os.path.join(self.map_out_path, \"ground-truth\")):\n",
        "                os.makedirs(os.path.join(self.map_out_path, \"ground-truth\"))\n",
        "            if not os.path.exists(os.path.join(self.map_out_path, \"detection-results\")):\n",
        "                os.makedirs(os.path.join(self.map_out_path, \"detection-results\"))\n",
        "            print(\"Get map.\")\n",
        "            for annotation_line in tqdm(self.val_lines):\n",
        "                line        = annotation_line.split()\n",
        "                image_id    = os.path.basename(line[0]).split('.')[0]\n",
        "\n",
        "                image       = Image.open(line[0])\n",
        "\n",
        "                gt_boxes    = np.array([np.array(list(map(int,box.split(',')))) for box in line[1:]])\n",
        "\n",
        "                self.get_map_txt(image_id, image, self.class_names, self.map_out_path)\n",
        "\n",
        "                with open(os.path.join(self.map_out_path, \"ground-truth/\"+image_id+\".txt\"), \"w\") as new_f:\n",
        "                    for box in gt_boxes:\n",
        "                        left, top, right, bottom, obj = box\n",
        "                        obj_name = self.class_names[obj]\n",
        "                        new_f.write(\"%s %s %s %s %s\\n\" % (obj_name, left, top, right, bottom))\n",
        "\n",
        "            print(\"Calculate Map.\")\n",
        "            try:\n",
        "                temp_map = get_coco_map(class_names = self.class_names, path = self.map_out_path)[1]\n",
        "            except:\n",
        "                temp_map = get_map(self.MINOVERLAP, False, path = self.map_out_path)\n",
        "            self.maps.append(temp_map)\n",
        "            self.epoches.append(epoch)\n",
        "\n",
        "            with open(os.path.join(self.log_dir, \"epoch_map.txt\"), 'a') as f:\n",
        "                f.write(str(temp_map))\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(self.epoches, self.maps, 'red', linewidth = 2, label='train map')\n",
        "\n",
        "            plt.grid(True)\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Map %s'%str(self.MINOVERLAP))\n",
        "            plt.title('A Map Curve')\n",
        "            plt.legend(loc=\"upper right\")\n",
        "\n",
        "            plt.savefig(os.path.join(self.log_dir, \"epoch_map.png\"))\n",
        "            plt.cla()\n",
        "            plt.close(\"all\")\n",
        "\n",
        "            print(\"Get map done.\")\n",
        "            shutil.rmtree(self.map_out_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8fg0HAtCti1"
      },
      "source": [
        "# Requirements for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cETV2BY6BzAE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def select_candidates_in_gts(xy_centers, gt_bboxes, eps=1e-9, roll_out=False):\n",
        "    \"\"\"select the positive anchor center in gt\n",
        "\n",
        "    Args:\n",
        "        xy_centers (Tensor): shape(h*w, 4)\n",
        "        gt_bboxes (Tensor): shape(b, n_boxes, 4)\n",
        "    Return:\n",
        "        (Tensor): shape(b, n_boxes, h*w)\n",
        "    \"\"\"\n",
        "    n_anchors       = xy_centers.shape[0]\n",
        "    bs, n_boxes, _  = gt_bboxes.shape\n",
        "    # Calculate the distance from each ground truth box to each anchor point in terms of top-left and bottom-right corners, then take the minimum\n",
        "    # Ensure the ground truth box is near the anchor point, surrounding the anchor point\n",
        "    if roll_out:\n",
        "        bbox_deltas = torch.empty((bs, n_boxes, n_anchors), device=gt_bboxes.device)\n",
        "        for b in range(bs):\n",
        "            lt, rb          = gt_bboxes[b].view(-1, 1, 4).chunk(2, 2)  # left-top, right-bottom\n",
        "            bbox_deltas[b]  = torch.cat((xy_centers[None] - lt, rb - xy_centers[None]),\n",
        "                                       dim=2).view(n_boxes, n_anchors, -1).amin(2).gt_(eps)\n",
        "        return bbox_deltas\n",
        "    else:\n",
        "        # left-top, right-bottom of the ground truth box\n",
        "        lt, rb      = gt_bboxes.view(-1, 1, 4).chunk(2, 2)\n",
        "        # Distance from each ground truth box to each anchor point in terms of top-left and bottom-right corners\n",
        "        bbox_deltas = torch.cat((xy_centers[None] - lt, rb - xy_centers[None]), dim=2).view(bs, n_boxes, n_anchors, -1)\n",
        "        # return (bbox_deltas.min(3)[0] > eps).to(gt_bboxes.dtype)\n",
        "        return bbox_deltas.amin(3).gt_(eps)\n",
        "\n",
        "\n",
        "def select_highest_overlaps(mask_pos, overlaps, n_max_boxes):\n",
        "    \"\"\"if an anchor box is assigned to multiple gts,\n",
        "        the one with the highest iou will be selected.\n",
        "\n",
        "    Args:\n",
        "        mask_pos (Tensor): shape(b, n_max_boxes, h*w)\n",
        "        overlaps (Tensor): shape(b, n_max_boxes, h*w)\n",
        "    Return:\n",
        "        target_gt_idx (Tensor): shape(b, h*w)\n",
        "        fg_mask (Tensor): shape(b, h*w)\n",
        "        mask_pos (Tensor): shape(b, n_max_boxes, h*w)\n",
        "    \"\"\"\n",
        "    # b, n_max_boxes, 8400 -> b, 8400\n",
        "    fg_mask = mask_pos.sum(-2)\n",
        "    # If an anchor is assigned to predict multiple ground truth boxes\n",
        "    if fg_mask.max() > 1:\n",
        "        # b, n_max_boxes, 8400\n",
        "        mask_multi_gts      = (fg_mask.unsqueeze(1) > 1).repeat([1, n_max_boxes, 1])\n",
        "        # If an anchor is assigned to predict multiple ground truth boxes, first calculate the ground truth box with the highest overlap for this anchor\n",
        "        # Then create a one-hot encoding\n",
        "        # b, 8400\n",
        "        max_overlaps_idx    = overlaps.argmax(1)\n",
        "        # b, 8400, n_max_boxes\n",
        "        is_max_overlaps     = F.one_hot(max_overlaps_idx, n_max_boxes)\n",
        "        # b, n_max_boxes, 8400\n",
        "        is_max_overlaps     = is_max_overlaps.permute(0, 2, 1).to(overlaps.dtype)\n",
        "        # b, n_max_boxes, 8400\n",
        "        mask_pos            = torch.where(mask_multi_gts, is_max_overlaps, mask_pos)\n",
        "        fg_mask             = mask_pos.sum(-2)\n",
        "    # Find which ground truth box each anchor corresponds to\n",
        "    target_gt_idx = mask_pos.argmax(-2)  # (b, h*w)\n",
        "    return target_gt_idx, fg_mask, mask_pos\n",
        "\n",
        "\n",
        "class TaskAlignedAssigner(nn.Module):\n",
        "\n",
        "    def __init__(self, topk=13, num_classes=80, alpha=1.0, beta=6.0, eps=1e-9, roll_out_thr=0):\n",
        "        super().__init__()\n",
        "        self.topk           = topk\n",
        "        self.num_classes    = num_classes\n",
        "        self.bg_idx         = num_classes\n",
        "        self.alpha          = alpha\n",
        "        self.beta           = beta\n",
        "        self.eps            = eps\n",
        "        # roll_out_thr64\n",
        "        self.roll_out_thr   = roll_out_thr\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n",
        "        \"\"\"This code referenced to\n",
        "           https://github.com/Nioolek/PPYOLOE_pytorch/blob/master/ppyoloe/assigner/tal_assigner.py\n",
        "\n",
        "        Args:\n",
        "            pd_scores (Tensor)  : shape(bs, num_total_anchors, num_classes)\n",
        "            pd_bboxes (Tensor)  : shape(bs, num_total_anchors, 4)\n",
        "            anc_points (Tensor) : shape(num_total_anchors, 2)\n",
        "            gt_labels (Tensor)  : shape(bs, n_max_boxes, 1)\n",
        "            gt_bboxes (Tensor)  : shape(bs, n_max_boxes, 4)\n",
        "            mask_gt (Tensor)    : shape(bs, n_max_boxes, 1)\n",
        "        Returns:\n",
        "            target_labels (Tensor)  : shape(bs, num_total_anchors)\n",
        "            target_bboxes (Tensor)  : shape(bs, num_total_anchors, 4)\n",
        "            target_scores (Tensor)  : shape(bs, num_total_anchors, num_classes)\n",
        "            fg_mask (Tensor)        : shape(bs, num_total_anchors)\n",
        "        \"\"\"\n",
        "        self.bs             = pd_scores.size(0)\n",
        "        self.n_max_boxes    = gt_bboxes.size(1)\n",
        "        # self.n_max_boxesself.roll_out_thrroll_out\n",
        "        self.roll_out       = self.n_max_boxes > self.roll_out_thr if self.roll_out_thr else False\n",
        "\n",
        "        if self.n_max_boxes == 0:\n",
        "            device = gt_bboxes.device\n",
        "            return (torch.full_like(pd_scores[..., 0], self.bg_idx).to(device), torch.zeros_like(pd_bboxes).to(device),\n",
        "                    torch.zeros_like(pd_scores).to(device), torch.zeros_like(pd_scores[..., 0]).to(device),\n",
        "                    torch.zeros_like(pd_scores[..., 0]).to(device))\n",
        "\n",
        "        # b, max_num_obj, 8400\n",
        "        # mask_pos      Anchors that satisfy being inside the ground truth box, are the top-k most overlapping positive samples of the ground truth box, and satisfy mask_gt\n",
        "        # align_metric  The probability that a prior point belongs to a certain ground truth box class multiplied by the overlap degree of the prior point with the ground truth box\n",
        "        # overlaps      The overlap degree of all ground truth boxes and anchors\n",
        "        mask_pos, align_metric, overlaps = self.get_pos_mask(pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt)\n",
        "\n",
        "        # target_gt_idx     b, 8400     Each anchor corresponds to which ground truth box\n",
        "        # fg_mask           b, 8400     Whether each anchor has a corresponding ground truth box\n",
        "        # mask_pos          b, max_num_obj, 8400    One-hot encoded target_gt_idx\n",
        "        target_gt_idx, fg_mask, mask_pos = select_highest_overlaps(mask_pos, overlaps, self.n_max_boxes)\n",
        "\n",
        "\n",
        "        target_labels, target_bboxes, target_scores = self.get_targets(gt_labels, gt_bboxes, target_gt_idx, fg_mask)\n",
        "\n",
        "        align_metric        *= mask_pos\n",
        "        # b, max_num_obj\n",
        "        pos_align_metrics   = align_metric.amax(axis=-1, keepdim=True)\n",
        "        # b, max_num_obj\n",
        "        pos_overlaps        = (overlaps * mask_pos).amax(axis=-1, keepdim=True)\n",
        "        norm_align_metric   = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2).unsqueeze(-1)\n",
        "        target_scores       = target_scores * norm_align_metric\n",
        "\n",
        "        return target_labels, target_bboxes, target_scores, fg_mask.bool(), target_gt_idx\n",
        "\n",
        "    def get_pos_mask(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt):\n",
        "        # pd_scores bs, num_total_anchors, num_classes\n",
        "        # pd_bboxes bs, num_total_anchors, 4\n",
        "        # gt_labels bs, n_max_boxes, 1\n",
        "        # gt_bboxes bs, n_max_boxes, 4\n",
        "        #\n",
        "        # align_metric, overlaps    bs, max_num_obj, 8400\n",
        "        align_metric, overlaps  = self.get_box_metrics(pd_scores, pd_bboxes, gt_labels, gt_bboxes)\n",
        "\n",
        "\n",
        "\n",
        "        # get in_gts mask           b, max_num_obj, 8400\n",
        "        mask_in_gts             = select_candidates_in_gts(anc_points, gt_bboxes, roll_out=self.roll_out)\n",
        "        # get topk_metric mask      b, max_num_obj, 8400\n",
        "        mask_topk               = self.select_topk_candidates(align_metric * mask_in_gts, topk_mask=mask_gt.repeat([1, 1, self.topk]).bool())\n",
        "        # merge all mask to a final mask, b, max_num_obj, h*w\n",
        "        mask_pos                = mask_topk * mask_in_gts * mask_gt\n",
        "\n",
        "        return mask_pos, align_metric, overlaps\n",
        "\n",
        "    def get_box_metrics(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes):\n",
        "        if self.roll_out:\n",
        "            align_metric    = torch.empty((self.bs, self.n_max_boxes, pd_scores.shape[1]), device=pd_scores.device)\n",
        "            overlaps        = torch.empty((self.bs, self.n_max_boxes, pd_scores.shape[1]), device=pd_scores.device)\n",
        "            ind_0           = torch.empty(self.n_max_boxes, dtype=torch.long)\n",
        "            for b in range(self.bs):\n",
        "                ind_0[:], ind_2 = b, gt_labels[b].squeeze(-1).long()\n",
        "                # bs, max_num_obj, 8400\n",
        "                bbox_scores     = pd_scores[ind_0, :, ind_2]\n",
        "                # bs, max_num_obj, 8400\n",
        "                overlaps[b]     = bbox_iou(gt_bboxes[b].unsqueeze(1), pd_bboxes[b].unsqueeze(0), xywh=False, CIoU=True).squeeze(2).clamp(0)\n",
        "                align_metric[b] = bbox_scores.pow(self.alpha) * overlaps[b].pow(self.beta)\n",
        "        else:\n",
        "            # 2, b, max_num_obj\n",
        "            ind = torch.zeros([2, self.bs, self.n_max_boxes], dtype=torch.long)\n",
        "            # b, max_num_obj\n",
        "            ind[0] = torch.arange(end=self.bs).view(-1, 1).repeat(1, self.n_max_boxes)\n",
        "            ind[1] = gt_labels.long().squeeze(-1)\n",
        "\n",
        "            # b, max_num_obj, 8400\n",
        "            bbox_scores = pd_scores[ind[0], :, ind[1]]\n",
        "\n",
        "            # bs, max_num_obj, 8400\n",
        "            overlaps        = bbox_iou(gt_bboxes.unsqueeze(2), pd_bboxes.unsqueeze(1), xywh=False, CIoU=True).squeeze(3).clamp(0)\n",
        "            align_metric    = bbox_scores.pow(self.alpha) * overlaps.pow(self.beta)\n",
        "        return align_metric, overlaps\n",
        "\n",
        "    def select_topk_candidates(self, metrics, largest=True, topk_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            metrics     : (b, max_num_obj, h*w).\n",
        "            topk_mask   : (b, max_num_obj, topk) or None\n",
        "        \"\"\"\n",
        "        # 8400\n",
        "        num_anchors             = metrics.shape[-1]\n",
        "        # b, max_num_obj, topk\n",
        "        topk_metrics, topk_idxs = torch.topk(metrics, self.topk, dim=-1, largest=largest)\n",
        "        if topk_mask is None:\n",
        "            topk_mask = (topk_metrics.max(-1, keepdim=True) > self.eps).tile([1, 1, self.topk])\n",
        "        # b, max_num_obj, topk\n",
        "        topk_idxs[~topk_mask] = 0\n",
        "        if self.roll_out:\n",
        "            is_in_topk = torch.empty(metrics.shape, dtype=torch.long, device=metrics.device)\n",
        "            for b in range(len(topk_idxs)):\n",
        "                is_in_topk[b] = F.one_hot(topk_idxs[b], num_anchors).sum(-2)\n",
        "        else:\n",
        "            is_in_topk = F.one_hot(topk_idxs, num_anchors).sum(-2)\n",
        "\n",
        "        is_in_topk = torch.where(is_in_topk > 1, 0, is_in_topk)\n",
        "        return is_in_topk.to(metrics.dtype)\n",
        "\n",
        "    def get_targets(self, gt_labels, gt_bboxes, target_gt_idx, fg_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            gt_labels       : (b, max_num_obj, 1)\n",
        "            gt_bboxes       : (b, max_num_obj, 4)\n",
        "            target_gt_idx   : (b, h*w)\n",
        "            fg_mask         : (b, h*w)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_ind       = torch.arange(end=self.bs, dtype=torch.int64, device=gt_labels.device)[..., None]\n",
        "        target_gt_idx   = target_gt_idx + batch_ind * self.n_max_boxes\n",
        "        target_labels   = gt_labels.long().flatten()[target_gt_idx]\n",
        "        target_bboxes   = gt_bboxes.view(-1, 4)[target_gt_idx]\n",
        "\n",
        "        # assigned target scores\n",
        "        target_labels.clamp(0)\n",
        "        target_scores   = F.one_hot(target_labels, self.num_classes)  # (b, h*w, 80)\n",
        "        fg_scores_mask  = fg_mask[:, :, None].repeat(1, 1, self.num_classes)  # (b, h*w, 80)\n",
        "        target_scores   = torch.where(fg_scores_mask > 0, target_scores, 0)\n",
        "\n",
        "        return target_labels, target_bboxes, target_scores\n",
        "\n",
        "def bbox_iou(box1, box2, xywh=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n",
        "    # Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n",
        "\n",
        "    # Get the coordinates of bounding boxes\n",
        "    if xywh:  # transform from xywh to xyxy\n",
        "        (x1, y1, w1, h1), (x2, y2, w2, h2) = box1.chunk(4, -1), box2.chunk(4, -1)\n",
        "        w1_, h1_, w2_, h2_ = w1 / 2, h1 / 2, w2 / 2, h2 / 2\n",
        "        b1_x1, b1_x2, b1_y1, b1_y2 = x1 - w1_, x1 + w1_, y1 - h1_, y1 + h1_\n",
        "        b2_x1, b2_x2, b2_y1, b2_y2 = x2 - w2_, x2 + w2_, y2 - h2_, y2 + h2_\n",
        "    else:  # x1, y1, x2, y2 = box1\n",
        "        b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
        "        b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
        "        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
        "        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
        "\n",
        "    # Intersection area\n",
        "    inter = (b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)).clamp(0) * \\\n",
        "            (b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)).clamp(0)\n",
        "\n",
        "    # Union Area\n",
        "    union = w1 * h1 + w2 * h2 - inter + eps\n",
        "\n",
        "    # IoU\n",
        "    iou = inter / union\n",
        "    if CIoU or DIoU or GIoU:\n",
        "        cw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)  # convex (smallest enclosing box) width\n",
        "        ch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)  # convex height\n",
        "        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n",
        "            c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
        "            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center dist ** 2\n",
        "            if CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
        "                v = (4 / math.pi ** 2) * (torch.atan(w2 / h2) - torch.atan(w1 / h1)).pow(2)\n",
        "                with torch.no_grad():\n",
        "                    alpha = v / (v - iou + (1 + eps))\n",
        "                return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
        "            return iou - rho2 / c2  # DIoU\n",
        "        c_area = cw * ch + eps  # convex area\n",
        "        return iou - (c_area - union) / c_area  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n",
        "    return iou  # IoU\n",
        "\n",
        "def bbox2dist(anchor_points, bbox, reg_max):\n",
        "    \"\"\"Transform bbox(xyxy) to dist(ltrb).\"\"\"\n",
        "    x1y1, x2y2 = torch.split(bbox, 2, -1)\n",
        "    return torch.cat((anchor_points - x1y1, x2y2 - anchor_points), -1).clamp(0, reg_max - 0.01)  # dist (lt, rb)\n",
        "\n",
        "class BboxLoss(nn.Module):\n",
        "    def __init__(self, reg_max=16, use_dfl=False):\n",
        "        super().__init__()\n",
        "        self.reg_max = reg_max\n",
        "        self.use_dfl = use_dfl\n",
        "\n",
        "    def forward(self, pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask):\n",
        "        weight      = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n",
        "        iou         = bbox_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask], xywh=False, CIoU=True)\n",
        "        loss_iou    = ((1.0 - iou) * weight).sum() / target_scores_sum\n",
        "\n",
        "        # DFL\n",
        "        if self.use_dfl:\n",
        "            target_ltrb = bbox2dist(anchor_points, target_bboxes, self.reg_max)\n",
        "            loss_dfl = self._df_loss(pred_dist[fg_mask].view(-1, self.reg_max + 1), target_ltrb[fg_mask]) * weight\n",
        "            loss_dfl = loss_dfl.sum() / target_scores_sum\n",
        "        else:\n",
        "            loss_dfl = torch.tensor(0.0).to(pred_dist.device)\n",
        "\n",
        "        return loss_iou, loss_dfl\n",
        "\n",
        "    @staticmethod\n",
        "    def _df_loss(pred_dist, target):\n",
        "        # Return sum of left and right DFL losses\n",
        "        # Distribution Focal Loss (DFL) proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
        "        tl = target.long()  # target left\n",
        "        tr = tl + 1  # target right\n",
        "        wl = tr - target  # weight left\n",
        "        wr = 1 - wl  # weight right\n",
        "        return (F.cross_entropy(pred_dist, tl.view(-1), reduction=\"none\").view(tl.shape) * wl +\n",
        "                F.cross_entropy(pred_dist, tr.view(-1), reduction=\"none\").view(tl.shape) * wr).mean(-1, keepdim=True)\n",
        "\n",
        "def xywh2xyxy(x):\n",
        "    \"\"\"\n",
        "    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n",
        "    top-left corner and (x2, y2) is the bottom-right corner.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray) or (torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n",
        "    Returns:\n",
        "        y (np.ndarray) or (torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n",
        "    \"\"\"\n",
        "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
        "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
        "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
        "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
        "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
        "    return y\n",
        "\n",
        "# Criterion class for computing training losses\n",
        "class Loss:\n",
        "    def __init__(self, model):\n",
        "        self.bce    = nn.BCEWithLogitsLoss(reduction='none')\n",
        "        self.stride = model.stride  # model strides\n",
        "        self.nc     = model.num_classes  # number of classes\n",
        "        self.no     = model.no\n",
        "        self.reg_max = model.reg_max\n",
        "\n",
        "        self.use_dfl = model.reg_max > 1\n",
        "        roll_out_thr = 64\n",
        "\n",
        "        self.assigner = TaskAlignedAssigner(topk=10,\n",
        "                                            num_classes=self.nc,\n",
        "                                            alpha=0.5,\n",
        "                                            beta=6.0,\n",
        "                                            roll_out_thr=roll_out_thr)\n",
        "        self.bbox_loss  = BboxLoss(model.reg_max - 1, use_dfl=self.use_dfl)\n",
        "        self.proj       = torch.arange(model.reg_max, dtype=torch.float)\n",
        "\n",
        "    def preprocess(self, targets, batch_size, scale_tensor):\n",
        "        if targets.shape[0] == 0:\n",
        "            out = torch.zeros(batch_size, 0, 5, device=targets.device)\n",
        "        else:\n",
        "\n",
        "            i           = targets[:, 0]\n",
        "            _, counts   = i.unique(return_counts=True)\n",
        "            out         = torch.zeros(batch_size, counts.max(), 5, device=targets.device)\n",
        "            for j in range(batch_size):\n",
        "                matches = i == j\n",
        "                n = matches.sum()\n",
        "                if n:\n",
        "                    out[j, :n] = targets[matches, 1:]\n",
        "            out[..., 1:5] = xywh2xyxy(out[..., 1:5].mul_(scale_tensor))\n",
        "        return out\n",
        "\n",
        "    def bbox_decode(self, anchor_points, pred_dist):\n",
        "        if self.use_dfl:\n",
        "            # batch, anchors, channels\n",
        "            b, a, c     = pred_dist.shape\n",
        "            pred_dist   = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.to(pred_dist.device).type(pred_dist.dtype))\n",
        "            # pred_dist = pred_dist.view(b, a, c // 4, 4).transpose(2,3).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n",
        "            # pred_dist = (pred_dist.view(b, a, c // 4, 4).softmax(2) * self.proj.type(pred_dist.dtype).view(1, 1, -1, 1)).sum(2)\n",
        "        return dist2bbox(pred_dist, anchor_points, xywh=False)\n",
        "\n",
        "    def __call__(self, preds, batch):\n",
        "        device  = preds[1].device\n",
        "        loss    = torch.zeros(3, device=device)\n",
        "\n",
        "        feats   = preds[2] if isinstance(preds, tuple) else preds\n",
        "        pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split((self.reg_max * 4, self.nc), 1)\n",
        "\n",
        "        # bs, num_classes + self.reg_max * 4 , 8400 =>  cls bs, num_classes, 8400;\n",
        "        #                                               box bs, self.reg_max * 4, 8400\n",
        "        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
        "        pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n",
        "\n",
        "        dtype       = pred_scores.dtype\n",
        "        batch_size  = pred_scores.shape[0]\n",
        "        imgsz       = torch.tensor(feats[0].shape[2:], device=device, dtype=dtype) * self.stride[0]\n",
        "        anchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n",
        "\n",
        "        targets                 = torch.cat((batch[:, 0].view(-1, 1), batch[:, 1].view(-1, 1), batch[:, 2:]), 1)\n",
        "        # bs, max_boxes_num, 5\n",
        "        targets                 = self.preprocess(targets.to(device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\n",
        "        # bs, max_boxes_num, 5 => bs, max_boxes_num, 1 ; bs, max_boxes_num, 4\n",
        "        gt_labels, gt_bboxes    = targets.split((1, 4), 2)  # cls, xyxy\n",
        "        # bs, max_boxes_num\n",
        "        mask_gt                 = gt_bboxes.sum(2, keepdim=True).gt_(0)\n",
        "\n",
        "        # pboxes\n",
        "        # bs, 8400, 4\n",
        "        pred_bboxes             = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\n",
        "\n",
        "        # target_bboxes     bs, 8400, 4\n",
        "        # target_scores     bs, 8400, 80\n",
        "        # fg_mask           bs, 8400\n",
        "        _, target_bboxes, target_scores, fg_mask, _ = self.assigner(\n",
        "            pred_scores.detach().sigmoid(), (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\n",
        "            anchor_points * stride_tensor, gt_labels, gt_bboxes, mask_gt\n",
        "        )\n",
        "\n",
        "        target_bboxes       /= stride_tensor\n",
        "        target_scores_sum   = max(target_scores.sum(), 1)\n",
        "\n",
        "        # loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\n",
        "        loss[1] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\n",
        "\n",
        "        if fg_mask.sum():\n",
        "            loss[0], loss[2] = self.bbox_loss(pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores,\n",
        "                                              target_scores_sum, fg_mask)\n",
        "\n",
        "        loss[0] *= 7.5  # box gain\n",
        "        loss[1] *= 0.5  # cls gain\n",
        "        loss[2] *= 1.5  # dfl gain\n",
        "        return loss.sum() # loss(box, cls, dfl) # * batch_size\n",
        "\n",
        "def is_parallel(model):\n",
        "    # Returns True if model is of type DP or DDP\n",
        "    return type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)\n",
        "\n",
        "def de_parallel(model):\n",
        "    # De-parallelize a model: returns single-GPU model if model is of type DP or DDP\n",
        "    return model.module if is_parallel(model) else model\n",
        "\n",
        "def copy_attr(a, b, include=(), exclude=()):\n",
        "    # Copy attributes from b to a, options to only include [...] and to exclude [...]\n",
        "    for k, v in b.__dict__.items():\n",
        "        if (len(include) and k not in include) or k.startswith('_') or k in exclude:\n",
        "            continue\n",
        "        else:\n",
        "            setattr(a, k, v)\n",
        "\n",
        "class ModelEMA:\n",
        "    \"\"\" Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models\n",
        "    Keeps a moving average of everything in the model state_dict (parameters and buffers)\n",
        "    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, decay=0.9999, tau=2000, updates=0):\n",
        "        # Create EMA\n",
        "        self.ema = deepcopy(de_parallel(model)).eval()  # FP32 EMA\n",
        "        # if next(model.parameters()).device.type != 'cpu':\n",
        "        #     self.ema.half()  # FP16 EMA\n",
        "        self.updates = updates  # number of EMA updates\n",
        "        self.decay = lambda x: decay * (1 - math.exp(-x / tau))  # decay exponential ramp (to help early epochs)\n",
        "        for p in self.ema.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "    def update(self, model):\n",
        "        # Update EMA parameters\n",
        "        with torch.no_grad():\n",
        "            self.updates += 1\n",
        "            d = self.decay(self.updates)\n",
        "\n",
        "            msd = de_parallel(model).state_dict()  # model state_dict\n",
        "            for k, v in self.ema.state_dict().items():\n",
        "                if v.dtype.is_floating_point:\n",
        "                    v *= d\n",
        "                    v += (1 - d) * msd[k].detach()\n",
        "\n",
        "    def update_attr(self, model, include=(), exclude=('process_group', 'reducer')):\n",
        "        # Update EMA attributes\n",
        "        copy_attr(self.ema, model, include, exclude)\n",
        "\n",
        "def weights_init(net, init_type='normal', init_gain = 0.02):\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and classname.find('Conv') != -1:\n",
        "            if init_type == 'normal':\n",
        "                torch.nn.init.normal_(m.weight.data, 0.0, init_gain)\n",
        "            elif init_type == 'xavier':\n",
        "                torch.nn.init.xavier_normal_(m.weight.data, gain=init_gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                torch.nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                torch.nn.init.orthogonal_(m.weight.data, gain=init_gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "    print('initialize network with %s type' % init_type)\n",
        "    net.apply(init_func)\n",
        "\n",
        "def get_lr_scheduler(lr_decay_type, lr, min_lr, total_iters, warmup_iters_ratio = 0.05, warmup_lr_ratio = 0.1, no_aug_iter_ratio = 0.05, step_num = 10):\n",
        "    def yolox_warm_cos_lr(lr, min_lr, total_iters, warmup_total_iters, warmup_lr_start, no_aug_iter, iters):\n",
        "        if iters <= warmup_total_iters:\n",
        "            # lr = (lr - warmup_lr_start) * iters / float(warmup_total_iters) + warmup_lr_start\n",
        "            lr = (lr - warmup_lr_start) * pow(iters / float(warmup_total_iters), 2\n",
        "            ) + warmup_lr_start\n",
        "        elif iters >= total_iters - no_aug_iter:\n",
        "            lr = min_lr\n",
        "        else:\n",
        "            lr = min_lr + 0.5 * (lr - min_lr) * (\n",
        "                1.0\n",
        "                + math.cos(\n",
        "                    math.pi\n",
        "                    * (iters - warmup_total_iters)\n",
        "                    / (total_iters - warmup_total_iters - no_aug_iter)\n",
        "                )\n",
        "            )\n",
        "        return lr\n",
        "\n",
        "    def step_lr(lr, decay_rate, step_size, iters):\n",
        "        if step_size < 1:\n",
        "            raise ValueError(\"step_size must above 1.\")\n",
        "        n       = iters // step_size\n",
        "        out_lr  = lr * decay_rate ** n\n",
        "        return out_lr\n",
        "\n",
        "    if lr_decay_type == \"cos\":\n",
        "        warmup_total_iters  = min(max(warmup_iters_ratio * total_iters, 1), 3)\n",
        "        warmup_lr_start     = max(warmup_lr_ratio * lr, 1e-6)\n",
        "        no_aug_iter         = min(max(no_aug_iter_ratio * total_iters, 1), 15)\n",
        "        func = partial(yolox_warm_cos_lr ,lr, min_lr, total_iters, warmup_total_iters, warmup_lr_start, no_aug_iter)\n",
        "    else:\n",
        "        decay_rate  = (min_lr / lr) ** (1 / (step_num - 1))\n",
        "        step_size   = total_iters / step_num\n",
        "        func = partial(step_lr, lr, decay_rate, step_size)\n",
        "\n",
        "    return func\n",
        "\n",
        "def set_optimizer_lr(optimizer, lr_scheduler_func, epoch):\n",
        "    lr = lr_scheduler_func(epoch)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B76Opa4dC_Av"
      },
      "source": [
        "# YoloV8 Model Defining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pvuhfahhDKLt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "def fuse_conv_and_bn(conv, bn):\n",
        "    # Fuse Conv2d() and BatchNorm2d() layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/\n",
        "    fusedconv = nn.Conv2d(conv.in_channels,\n",
        "                          conv.out_channels,\n",
        "                          kernel_size=conv.kernel_size,\n",
        "                          stride=conv.stride,\n",
        "                          padding=conv.padding,\n",
        "                          dilation=conv.dilation,\n",
        "                          groups=conv.groups,\n",
        "                          bias=True).requires_grad_(False).to(conv.weight.device)\n",
        "\n",
        "    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
        "    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\n",
        "    fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape))\n",
        "\n",
        "    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\n",
        "    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
        "    fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)\n",
        "\n",
        "    return fusedconv\n",
        "\n",
        "class DFL(nn.Module):\n",
        "    # Distribution Focal Loss (DFL) proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
        "    def __init__(self, c1=16):\n",
        "        super().__init__()\n",
        "        self.conv   = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)\n",
        "        x           = torch.arange(c1, dtype=torch.float)\n",
        "        self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))\n",
        "        self.c1     = c1\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, a = x.shape\n",
        "        return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)\n",
        "\n",
        "#---------------------------------------------------#\n",
        "#   yolo_body\n",
        "#---------------------------------------------------#\n",
        "class YoloBody(nn.Module):\n",
        "    def __init__(self, input_shape, num_classes,  phi, backbone_name, model_name, pretrained=False):\n",
        "        super(YoloBody, self).__init__()\n",
        "        depth_dict          = {'n' : 0.33, 's' : 0.33, 'm' : 0.67, 'l' : 1.00, 'x' : 1.00,}\n",
        "        width_dict          = {'n' : 0.25, 's' : 0.50, 'm' : 0.75, 'l' : 1.00, 'x' : 1.25,}\n",
        "        deep_width_dict     = {'n' : 1.00, 's' : 1.00, 'm' : 0.75, 'l' : 0.50, 'x' : 0.50,}\n",
        "        dep_mul, wid_mul, deep_mul = depth_dict[phi], width_dict[phi], deep_width_dict[phi]\n",
        "\n",
        "        base_channels       = int(wid_mul * 64)  # 64\n",
        "        base_depth          = max(round(dep_mul * 3), 1)  # 3\n",
        "        #-------------------------------------------BACKBONE--------------------------------------------------------#\n",
        "        if backbone_name == 'nextvit':\n",
        "            self.backbone = NextViT('small', base_channels, base_depth, deep_mul)\n",
        "        elif backbone_name == 'resnet50':\n",
        "            self.backbone = ResNet_Backbone(base_channels, base_depth, deep_mul, pretrained=pretrained)\n",
        "        elif backbone_name == 'backbone':\n",
        "            self.backbone   = Backbone(base_channels, base_depth, deep_mul, phi, pretrained=pretrained)\n",
        "        elif backbone_name == 'mambavision':\n",
        "            self.backbone = MambaVision_Backbone(model_name, base_channels=base_channels, base_depth=base_depth, deep_mul=deep_mul,pretrained=False).to(device)\n",
        "        elif backbone_name == 'visionmamba':\n",
        "            self.backbone = VisionMambaBackbone(get_vision_mamba_backbone(model_name, img_size=640), base_channels, base_depth, deep_mul).to(device)\n",
        "        elif backbone_name == 'deit':\n",
        "            self.backbone = DEIT_Backbone(model_name,base_channels, base_depth, deep_mul)\n",
        "        elif backbone_name == 'vmamba':\n",
        "            self.backbone = VSSMBackbone(model_name, base_channels, base_depth, deep_mul, device).to(device)\n",
        "        else:\n",
        "            raise ValueError(f'The given {backbone_name} is not supported. Please choose from the following : backbone, resnet50, mambavision, visionmamba')\n",
        "        self.upsample   = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
        "\n",
        "        # 1024 * deep_mul + 512, 40, 40 => 512, 40, 40\n",
        "        self.conv3_for_upsample1    = C2f(int(base_channels * 16 * deep_mul) + base_channels * 8, base_channels * 8, base_depth, shortcut=False)\n",
        "        # 768, 80, 80 => 256, 80, 80\n",
        "        self.conv3_for_upsample2    = C2f(base_channels * 8 + base_channels * 4, base_channels * 4, base_depth, shortcut=False)\n",
        "\n",
        "        # 256, 80, 80 => 256, 40, 40\n",
        "        self.down_sample1           = Conv(base_channels * 4, base_channels * 4, 3, 2)\n",
        "        # 512 + 256, 40, 40 => 512, 40, 40\n",
        "        self.conv3_for_downsample1  = C2f(base_channels * 8 + base_channels * 4, base_channels * 8, base_depth, shortcut=False)\n",
        "\n",
        "        # 512, 40, 40 => 512, 20, 20\n",
        "        self.down_sample2           = Conv(base_channels * 8, base_channels * 8, 3, 2)\n",
        "        # 1024 * deep_mul + 512, 20, 20 =>  1024 * deep_mul, 20, 20\n",
        "        self.conv3_for_downsample2  = C2f(int(base_channels * 16 * deep_mul) + base_channels * 8, int(base_channels * 16 * deep_mul), base_depth, shortcut=False)\n",
        "\n",
        "        ch              = [base_channels * 4, base_channels * 8, int(base_channels * 16 * deep_mul)]\n",
        "        self.shape      = None\n",
        "        self.nl         = len(ch)\n",
        "        # self.stride     = torch.zeros(self.nl)\n",
        "        self.stride     = torch.tensor([640 / x.shape[-2] for x in self.backbone.forward(torch.zeros(1, 3, 640, 640))])  # forward\n",
        "        self.reg_max    = 16  # DFL channels (ch[0] // 16 to scale 4/8/12/16/20 for n/s/m/l/x)\n",
        "        self.no         = num_classes + self.reg_max * 4  # number of outputs per anchor\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        c2, c3   = max((16, ch[0] // 4, self.reg_max * 4)), max(ch[0], num_classes)  # channels\n",
        "        self.cv2 = nn.ModuleList(nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch)\n",
        "        self.cv3 = nn.ModuleList(nn.Sequential(Conv(x, c3, 3), Conv(c3, c3, 3), nn.Conv2d(c3, num_classes, 1)) for x in ch)\n",
        "        if not pretrained:\n",
        "            weights_init(self)\n",
        "        self.dfl = DFL(self.reg_max) if self.reg_max > 1 else nn.Identity()\n",
        "\n",
        "\n",
        "    def fuse(self):\n",
        "        print('Fusing layers... ')\n",
        "        for m in self.modules():\n",
        "            if type(m) is Conv and hasattr(m, 'bn'):\n",
        "                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\n",
        "                delattr(m, 'bn')  # remove batchnorm\n",
        "                m.forward = m.forward_fuse  # update forward\n",
        "        return self\n",
        "\n",
        "    def forward(self, x):\n",
        "        #  backbone\n",
        "        feat1, feat2, feat3 = self.backbone.forward(x)\n",
        "\n",
        "        # 1024 * deep_mul, 20, 20 => 1024 * deep_mul, 40, 40\n",
        "        P5_upsample = self.upsample(feat3)\n",
        "\n",
        "        # 1024 * deep_mul, 40, 40 cat 512, 40, 40 => 1024 * deep_mul + 512, 40, 40\n",
        "        P4          = torch.cat([P5_upsample, feat2], 1)\n",
        "\n",
        "        # 1024 * deep_mul + 512, 40, 40 => 512, 40, 40\n",
        "        P4          = self.conv3_for_upsample1(P4)\n",
        "\n",
        "\n",
        "        # 512, 40, 40 => 512, 80, 80\n",
        "        P4_upsample = self.upsample(P4)\n",
        "\n",
        "        # 512, 80, 80 cat 256, 80, 80 => 768, 80, 80\n",
        "        P3          = torch.cat([P4_upsample, feat1], 1)\n",
        "\n",
        "        # 768, 80, 80 => 256, 80, 80\n",
        "        P3          = self.conv3_for_upsample2(P3)\n",
        "\n",
        "        # 256, 80, 80 => 256, 40, 40\n",
        "        P3_downsample = self.down_sample1(P3)\n",
        "\n",
        "        # 512, 40, 40 cat 256, 40, 40 => 768, 40, 40\n",
        "        P4 = torch.cat([P3_downsample, P4], 1)\n",
        "\n",
        "        # 768, 40, 40 => 512, 40, 40\n",
        "        P4 = self.conv3_for_downsample1(P4)\n",
        "\n",
        "\n",
        "        # 512, 40, 40 => 512, 20, 20\n",
        "        P4_downsample = self.down_sample2(P4)\n",
        "\n",
        "        # 512, 20, 20 cat 1024 * deep_mul, 20, 20 => 1024 * deep_mul + 512, 20, 20\n",
        "        P5 = torch.cat([P4_downsample, feat3], 1)\n",
        "\n",
        "        # 1024 * deep_mul + 512, 20, 20 => 1024 * deep_mul, 20, 20\n",
        "        P5 = self.conv3_for_downsample2(P5)\n",
        "\n",
        "        # P3 256, 80, 80\n",
        "        # P4 512, 40, 40\n",
        "        # P5 1024 * deep_mul, 20, 20\n",
        "        shape = P3.shape  # BCHW\n",
        "\n",
        "        # P3 256, 80, 80 => num_classes + self.reg_max * 4, 80, 80\n",
        "        # P4 512, 40, 40 => num_classes + self.reg_max * 4, 40, 40\n",
        "        # P5 1024 * deep_mul, 20, 20 => num_classes + self.reg_max * 4, 20, 20\n",
        "        x = [P3, P4, P5]\n",
        "        for i in range(self.nl):\n",
        "            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)\n",
        "\n",
        "        if self.shape != shape:\n",
        "            self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\n",
        "            self.shape = shape\n",
        "\n",
        "        # num_classes + self.reg_max * 4 , 8400 =>  cls num_classes, 8400;\n",
        "        #                                           box self.reg_max * 4, 8400\n",
        "        box, cls        = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2).split((self.reg_max * 4, self.num_classes), 1)\n",
        "        # origin_cls      = [xi.split((self.reg_max * 4, self.num_classes), 1)[1] for xi in x]\n",
        "        dbox            = self.dfl(box)\n",
        "        return dbox, cls, x, self.anchors.to(dbox.device), self.strides.to(dbox.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRMqi4KgD8i8"
      },
      "source": [
        "# Summary\n",
        "### Get to know about the model parameters and GLOPs with different backbones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI9PKUjyEG6A",
        "outputId": "7e73ee64-8e60-42ed-b56f-9fa4b3595b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initialize network with normal type\n",
            "YoloV8 Version : n\n",
            "Total GFLOPS: 23.429G\n",
            "Total params: 9.086M\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    input_shape     = [640, 640]\n",
        "    anchors_mask    = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
        "    num_classes     = 80\n",
        "    phi             = 'n'\n",
        "\n",
        "    device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    m       = YoloBody(input_shape, num_classes, phi,'backbone','base', False).to(device)\n",
        "    # for i in m.children():\n",
        "        # print(i)\n",
        "        # print('==============================')\n",
        "\n",
        "    dummy_input     = torch.randn(1, 3, input_shape[0], input_shape[1]).to(device)\n",
        "    flops, params   = profile(m.to(device), (dummy_input, ), verbose=False)\n",
        "\n",
        "    flops           = flops * 2\n",
        "    flops, params   = clever_format([flops, params], \"%.3f\")\n",
        "    print(\"YoloV8 Version :\", phi)\n",
        "    print('Total GFLOPS: %s' % (flops))\n",
        "    print('Total params: %s' % (params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RKze_7WEm0p"
      },
      "source": [
        "# Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA8S5NdEJ0kC",
        "outputId": "de4f5d40-638c-4815-fe93-2939fc1593c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File 'voc_classes.txt' created successfully!\n"
          ]
        }
      ],
      "source": [
        "# List of VOC classes\n",
        "voc_classes = \"\"\"\n",
        "aeroplane\n",
        "bicycle\n",
        "bird\n",
        "boat\n",
        "bottle\n",
        "bus\n",
        "car\n",
        "cat\n",
        "chair\n",
        "cow\n",
        "diningtable\n",
        "dog\n",
        "horse\n",
        "motorbike\n",
        "person\n",
        "pottedplant\n",
        "sheep\n",
        "sofa\n",
        "train\n",
        "tvmonitor\n",
        "\"\"\"\n",
        "\n",
        "# Create and save the txt file\n",
        "with open(\"voc_classes.txt\", \"w\") as file:\n",
        "    file.write(voc_classes)\n",
        "\n",
        "print(\"File 'voc_classes.txt' created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hejm9FK4J3ug"
      },
      "outputs": [],
      "source": [
        "# # List of COCO classes\n",
        "# coco_classes = \"\"\"\n",
        "# person\n",
        "# bicycle\n",
        "# car\n",
        "# motorbike\n",
        "# aeroplane\n",
        "# bus\n",
        "# train\n",
        "# truck\n",
        "# boat\n",
        "# traffic light\n",
        "# fire hydrant\n",
        "# stop sign\n",
        "# parking meter\n",
        "# bench\n",
        "# bird\n",
        "# cat\n",
        "# dog\n",
        "# horse\n",
        "# sheep\n",
        "# cow\n",
        "# elephant\n",
        "# bear\n",
        "# zebra\n",
        "# giraffe\n",
        "# backpack\n",
        "# umbrella\n",
        "# handbag\n",
        "# tie\n",
        "# suitcase\n",
        "# frisbee\n",
        "# skis\n",
        "# snowboard\n",
        "# sports ball\n",
        "# kite\n",
        "# baseball bat\n",
        "# baseball glove\n",
        "# skateboard\n",
        "# surfboard\n",
        "# tennis racket\n",
        "# bottle\n",
        "# wine glass\n",
        "# cup\n",
        "# fork\n",
        "# knife\n",
        "# spoon\n",
        "# bowl\n",
        "# banana\n",
        "# apple\n",
        "# sandwich\n",
        "# orange\n",
        "# broccoli\n",
        "# carrot\n",
        "# hot dog\n",
        "# pizza\n",
        "# donut\n",
        "# cake\n",
        "# chair\n",
        "# sofa\n",
        "# pottedplant\n",
        "# bed\n",
        "# diningtable\n",
        "# toilet\n",
        "# tvmonitor\n",
        "# laptop\n",
        "# mouse\n",
        "# remote\n",
        "# keyboard\n",
        "# cell phone\n",
        "# microwave\n",
        "# oven\n",
        "# toaster\n",
        "# sink\n",
        "# refrigerator\n",
        "# book\n",
        "# clock\n",
        "# vase\n",
        "# scissors\n",
        "# teddy bear\n",
        "# hair drier\n",
        "# toothbrush\n",
        "# \"\"\"\n",
        "\n",
        "# # Create and save the txt file\n",
        "# with open(\"coco_classes.txt\", \"w\") as file:\n",
        "#     file.write(coco_classes)\n",
        "\n",
        "# print(\"File 'coco_classes.txt' created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABcDyeeXE22p"
      },
      "source": [
        "## VOC Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAb-6KfCE1sm",
        "outputId": "53aa01e3-371d-4026-f7d3-43f49dcd509f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generate txt in ImageSets.\n",
            "train and val size 15412\n",
            "train size 13870\n",
            "Generate txt in ImageSets done.\n",
            "Generate 2007_train.txt and 2007_val.txt for train.\n",
            "Generate 2007_train.txt and 2007_val.txt for train done.\n",
            "|             |     0 | \n",
            "|   aeroplane |   822 | \n",
            "|     bicycle |   691 | \n",
            "|        bird |  1063 | \n",
            "|        boat |   838 | \n",
            "|      bottle |  1190 | \n",
            "|         bus |   560 | \n",
            "|         car |  1868 | \n",
            "|         cat |  1140 | \n",
            "|       chair |  2237 | \n",
            "|         cow |   575 | \n",
            "| diningtable |   577 | \n",
            "|         dog |  1397 | \n",
            "|       horse |   706 | \n",
            "|   motorbike |   692 | \n",
            "|      person | 14132 | \n",
            "| pottedplant |   954 | \n",
            "|       sheep |   770 | \n",
            "|        sofa |   537 | \n",
            "|       train |   625 | \n",
            "|   tvmonitor |   768 | \n"
          ]
        }
      ],
      "source": [
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------#\n",
        "#   annotation_mode is used to specify the content to be calculated when this file is run\n",
        "#   annotation_mode = 0 represents the entire label processing process, including obtaining the txt files in VOCdevkit/VOC2007/ImageSets and the 2007_train.txt and 2007_val.txt files for training\n",
        "#   annotation_mode = 1 represents obtaining the txt files in VOCdevkit/VOC2007/ImageSets\n",
        "#   annotation_mode = 2 represents obtaining the 2007_train.txt and 2007_val.txt files for training\n",
        "#--------------------------------------------------------------------------------------------------------------------------------#\n",
        "annotation_mode     = 0\n",
        "#-------------------------------------------------------------------#\n",
        "#   Must be modified to generate target information for 2007_train.txt and 2007_val.txt\n",
        "#   Should be consistent with the classes_path used for training and prediction\n",
        "#   If there is no target information in the generated 2007_train.txt\n",
        "#   Then it is because the classes are not set correctly\n",
        "#   Only effective when annotation_mode is 0 or 2\n",
        "#-------------------------------------------------------------------#\n",
        "classes_path        = '/content/voc_classes.txt'\n",
        "#--------------------------------------------------------------------------------------------------------------------------------#\n",
        "#   trainval_percent is used to specify the ratio of (training set + validation set) to the test set. By default, (training set + validation set): test set = 9:1\n",
        "#   train_percent is used to specify the ratio of the training set to the validation set within (training set + validation set). By default, training set: validation set = 9:1\n",
        "#   Only effective when annotation_mode is 0 or 1\n",
        "#--------------------------------------------------------------------------------------------------------------------------------#\n",
        "trainval_percent    = 0.9\n",
        "train_percent       = 0.9\n",
        "#-------------------------------------------------------#\n",
        "#   Points to the folder where the VOC dataset is located\n",
        "#   Defaults to the VOC dataset in the root directory\n",
        "#-------------------------------------------------------#\n",
        "VOCdevkit_path  = '/content/VOC2012_train_val'\n",
        "\n",
        "VOCdevkit_sets  = [('2012', 'train'), ('2012', 'val')]\n",
        "classes, _      = get_classes(classes_path)\n",
        "\n",
        "#-------------------------------------------------------#\n",
        "#   Count the number of targets\n",
        "#-------------------------------------------------------#\n",
        "photo_nums  = np.zeros(len(VOCdevkit_sets))\n",
        "nums        = np.zeros(len(classes))\n",
        "def convert_annotation(year, image_id, list_file):\n",
        "    in_file = open(os.path.join(VOCdevkit_path, 'VOC%s_train_val/Annotations/%s.xml'%(year, image_id)), encoding='utf-8')\n",
        "    tree=ET.parse(in_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    for obj in root.iter('object'):\n",
        "        difficult = 0\n",
        "        if obj.find('difficult')!=None:\n",
        "            difficult = obj.find('difficult').text\n",
        "        cls = obj.find('name').text\n",
        "        if cls not in classes or int(difficult)==1:\n",
        "            continue\n",
        "        cls_id = classes.index(cls)\n",
        "        xmlbox = obj.find('bndbox')\n",
        "        b = (int(float(xmlbox.find('xmin').text)), int(float(xmlbox.find('ymin').text)), int(float(xmlbox.find('xmax').text)), int(float(xmlbox.find('ymax').text)))\n",
        "        list_file.write(\" \" + \",\".join([str(a) for a in b]) + ',' + str(cls_id))\n",
        "\n",
        "        nums[classes.index(cls)] = nums[classes.index(cls)] + 1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    if \" \" in os.path.abspath(VOCdevkit_path):\n",
        "        raise ValueError(\"The dataset folder path and image names cannot contain spaces, as this will affect normal model training. Please make the necessary adjustments.\")\n",
        "\n",
        "    if annotation_mode == 0 or annotation_mode == 1:\n",
        "        print(\"Generate txt in ImageSets.\")\n",
        "        xmlfilepath     = os.path.join(VOCdevkit_path, 'VOC2012_train_val/Annotations')\n",
        "        saveBasePath    = os.path.join(VOCdevkit_path, 'VOC2012_train_val/ImageSets/Main')\n",
        "        temp_xml        = os.listdir(xmlfilepath)\n",
        "        total_xml       = []\n",
        "        for xml in temp_xml:\n",
        "            if xml.endswith(\".xml\"):\n",
        "                total_xml.append(xml)\n",
        "\n",
        "        num     = len(total_xml)\n",
        "        list    = range(num)\n",
        "        tv      = int(num*trainval_percent)\n",
        "        tr      = int(tv*train_percent)\n",
        "        trainval= random.sample(list,tv)\n",
        "        train   = random.sample(trainval,tr)\n",
        "\n",
        "        print(\"train and val size\",tv)\n",
        "        print(\"train size\",tr)\n",
        "        ftrainval   = open(os.path.join(saveBasePath,'trainval.txt'), 'w')\n",
        "        ftest       = open(os.path.join(saveBasePath,'test.txt'), 'w')\n",
        "        ftrain      = open(os.path.join(saveBasePath,'train.txt'), 'w')\n",
        "        fval        = open(os.path.join(saveBasePath,'val.txt'), 'w')\n",
        "\n",
        "        for i in list:\n",
        "            name=total_xml[i][:-4]+'\\n'\n",
        "            if i in trainval:\n",
        "                ftrainval.write(name)\n",
        "                if i in train:\n",
        "                    ftrain.write(name)\n",
        "                else:\n",
        "                    fval.write(name)\n",
        "            else:\n",
        "                ftest.write(name)\n",
        "\n",
        "        ftrainval.close()\n",
        "        ftrain.close()\n",
        "        fval.close()\n",
        "        ftest.close()\n",
        "        print(\"Generate txt in ImageSets done.\")\n",
        "\n",
        "    if annotation_mode == 0 or annotation_mode == 2:\n",
        "        print(\"Generate 2007_train.txt and 2007_val.txt for train.\")\n",
        "        type_index = 0\n",
        "        for year, image_set in VOCdevkit_sets:\n",
        "            image_ids = open(os.path.join(VOCdevkit_path, 'VOC%s_train_val/ImageSets/Main/%s.txt'%(year, image_set)), encoding='utf-8').read().strip().split()\n",
        "            list_file = open('%s_%s.txt'%(year, image_set), 'w', encoding='utf-8')\n",
        "            for image_id in image_ids:\n",
        "                list_file.write('%s/VOC%s_train_val/JPEGImages/%s.jpg'%(os.path.abspath(VOCdevkit_path), year, image_id))\n",
        "\n",
        "                convert_annotation(year, image_id, list_file)\n",
        "                list_file.write('\\n')\n",
        "            photo_nums[type_index] = len(image_ids)\n",
        "            type_index += 1\n",
        "            list_file.close()\n",
        "        print(\"Generate 2007_train.txt and 2007_val.txt for train done.\")\n",
        "\n",
        "        def printTable(List1, List2):\n",
        "            for i in range(len(List1[0])):\n",
        "                print(\"|\", end=' ')\n",
        "                for j in range(len(List1)):\n",
        "                    print(List1[j][i].rjust(int(List2[j])), end=' ')\n",
        "                    print(\"|\", end=' ')\n",
        "                print()\n",
        "\n",
        "        str_nums = [str(int(x)) for x in nums]\n",
        "        tableData = [\n",
        "            classes, str_nums\n",
        "        ]\n",
        "        colWidths = [0]*len(tableData)\n",
        "        len1 = 0\n",
        "        for i in range(len(tableData)):\n",
        "            for j in range(len(tableData[i])):\n",
        "                if len(tableData[i][j]) > colWidths[i]:\n",
        "                    colWidths[i] = len(tableData[i][j])\n",
        "        printTable(tableData, colWidths)\n",
        "\n",
        "        if photo_nums[0] <= 500:\n",
        "            print(\"The number of training samples is less than 500, which is considered a small dataset. Please ensure to set a larger number of training epochs to achieve sufficient gradient descent steps.\")\n",
        "\n",
        "        if np.sum(nums) == 0:\n",
        "            print(\"No targets were obtained in the dataset. Please ensure to modify the classes_path to match your own dataset and ensure the label names are correct, otherwise the training will have no effect!\")\n",
        "            print(\"No targets were obtained in the dataset. Please ensure to modify the classes_path to match your own dataset and ensure the label names are correct, otherwise the training will have no effect!\")\n",
        "            print(\"No targets were obtained in the dataset. Please ensure to modify the classes_path to match your own dataset and ensure the label names are correct, otherwise the training will have no effect!\")\n",
        "            print(\"(Important things are said three times).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv4QjzUoE5hJ"
      },
      "source": [
        "## COCO Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV019qSkE-gX"
      },
      "outputs": [],
      "source": [
        "# #-------------------------------------------------------#\n",
        "# #   COCOjsontxt\n",
        "# #-------------------------------------------------------#\n",
        "# import json\n",
        "# import os\n",
        "# from collections import defaultdict\n",
        "\n",
        "# #-------------------------------------------------------#\n",
        "# #   COCO\n",
        "# #-------------------------------------------------------#\n",
        "# train_datasets_path     = \"coco_dataset/train2017\"\n",
        "# val_datasets_path       = \"coco_dataset/val2017\"\n",
        "\n",
        "# #-------------------------------------------------------#\n",
        "# #   COCO\n",
        "# #-------------------------------------------------------#\n",
        "# train_annotation_path   = \"coco_dataset/annotations/instances_train2017.json\"\n",
        "# val_annotation_path     = \"coco_dataset/annotations/instances_val2017.json\"\n",
        "\n",
        "# #-------------------------------------------------------#\n",
        "# #   txt\n",
        "# #-------------------------------------------------------#\n",
        "# train_output_path       = \"coco_train.txt\"\n",
        "# val_output_path         = \"coco_val.txt\"\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     name_box_id = defaultdict(list)\n",
        "#     id_name     = dict()\n",
        "#     f           = open(train_annotation_path, encoding='utf-8')\n",
        "#     data        = json.load(f)\n",
        "\n",
        "#     annotations = data['annotations']\n",
        "#     for ant in annotations:\n",
        "#         id = ant['image_id']\n",
        "#         name = os.path.join(train_datasets_path, '%012d.jpg' % id)\n",
        "#         cat = ant['category_id']\n",
        "#         if cat >= 1 and cat <= 11:\n",
        "#             cat = cat - 1\n",
        "#         elif cat >= 13 and cat <= 25:\n",
        "#             cat = cat - 2\n",
        "#         elif cat >= 27 and cat <= 28:\n",
        "#             cat = cat - 3\n",
        "#         elif cat >= 31 and cat <= 44:\n",
        "#             cat = cat - 5\n",
        "#         elif cat >= 46 and cat <= 65:\n",
        "#             cat = cat - 6\n",
        "#         elif cat == 67:\n",
        "#             cat = cat - 7\n",
        "#         elif cat == 70:\n",
        "#             cat = cat - 9\n",
        "#         elif cat >= 72 and cat <= 82:\n",
        "#             cat = cat - 10\n",
        "#         elif cat >= 84 and cat <= 90:\n",
        "#             cat = cat - 11\n",
        "#         name_box_id[name].append([ant['bbox'], cat])\n",
        "\n",
        "#     f = open(train_output_path, 'w')\n",
        "#     for key in name_box_id.keys():\n",
        "#         f.write(key)\n",
        "#         box_infos = name_box_id[key]\n",
        "#         for info in box_infos:\n",
        "#             x_min = int(info[0][0])\n",
        "#             y_min = int(info[0][1])\n",
        "#             x_max = x_min + int(info[0][2])\n",
        "#             y_max = y_min + int(info[0][3])\n",
        "\n",
        "#             box_info = \" %d,%d,%d,%d,%d\" % (\n",
        "#                 x_min, y_min, x_max, y_max, int(info[1]))\n",
        "#             f.write(box_info)\n",
        "#         f.write('\\n')\n",
        "#     f.close()\n",
        "\n",
        "#     name_box_id = defaultdict(list)\n",
        "#     id_name     = dict()\n",
        "#     f           = open(val_annotation_path, encoding='utf-8')\n",
        "#     data        = json.load(f)\n",
        "\n",
        "#     annotations = data['annotations']\n",
        "#     for ant in annotations:\n",
        "#         id = ant['image_id']\n",
        "#         name = os.path.join(val_datasets_path, '%012d.jpg' % id)\n",
        "#         cat = ant['category_id']\n",
        "#         if cat >= 1 and cat <= 11:\n",
        "#             cat = cat - 1\n",
        "#         elif cat >= 13 and cat <= 25:\n",
        "#             cat = cat - 2\n",
        "#         elif cat >= 27 and cat <= 28:\n",
        "#             cat = cat - 3\n",
        "#         elif cat >= 31 and cat <= 44:\n",
        "#             cat = cat - 5\n",
        "#         elif cat >= 46 and cat <= 65:\n",
        "#             cat = cat - 6\n",
        "#         elif cat == 67:\n",
        "#             cat = cat - 7\n",
        "#         elif cat == 70:\n",
        "#             cat = cat - 9\n",
        "#         elif cat >= 72 and cat <= 82:\n",
        "#             cat = cat - 10\n",
        "#         elif cat >= 84 and cat <= 90:\n",
        "#             cat = cat - 11\n",
        "#         name_box_id[name].append([ant['bbox'], cat])\n",
        "\n",
        "#     f = open(val_output_path, 'w')\n",
        "#     for key in name_box_id.keys():\n",
        "#         f.write(key)\n",
        "#         box_infos = name_box_id[key]\n",
        "#         for info in box_infos:\n",
        "#             x_min = int(info[0][0])\n",
        "#             y_min = int(info[0][1])\n",
        "#             x_max = x_min + int(info[0][2])\n",
        "#             y_max = y_min + int(info[0][3])\n",
        "\n",
        "#             box_info = \" %d,%d,%d,%d,%d\" % (\n",
        "#                 x_min, y_min, x_max, y_max, int(info[1]))\n",
        "#             f.write(box_info)\n",
        "#         f.write('\\n')\n",
        "#     f.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49rcPxNeEeOz"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS_fubrMqyzV"
      },
      "outputs": [],
      "source": [
        "# Cuda= True\n",
        "# seed=11\n",
        "# distributed     = False\n",
        "# sync_bn         = False\n",
        "# fp16            = False\n",
        "# classes_path    = '/content/voc_classes.txt'\n",
        "# model_path      = ''\n",
        "# input_shape     = [640, 640]\n",
        "# phi = 'n'\n",
        "# pretrained      = False\n",
        "# mosaic              = True\n",
        "# mosaic_prob         = 0.5\n",
        "# mixup               = True\n",
        "# mixup_prob          = 0.5\n",
        "# special_aug_ratio   = 0.7\n",
        "# label_smoothing     = 0\n",
        "# Init_Epoch          = 0\n",
        "# Freeze_Epoch        = 50\n",
        "# Freeze_batch_size   = 32\n",
        "# UnFreeze_Epoch      = 200\n",
        "# Unfreeze_batch_size = 16\n",
        "# Freeze_Train        = False\n",
        "# Init_lr             = 1e-2\n",
        "# Min_lr              = Init_lr * 0.01\n",
        "# optimizer_type      = \"sgd\"\n",
        "# momentum            = 0.937\n",
        "# weight_decay        = 5e-4\n",
        "# lr_decay_type       = \"cos\"\n",
        "# save_period         = 10\n",
        "# save_dir            = 'logs'\n",
        "# eval_flag           = True\n",
        "# eval_period         = 10\n",
        "# num_workers         = 4\n",
        "# train_annotation_path   = '/content/2012_train.txt'\n",
        "# val_annotation_path     = '/content/2012_val.txt'\n",
        "\n",
        "# seed_everything(seed)\n",
        "\n",
        "# ngpus_per_node  = torch.cuda.device_count()\n",
        "# device          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# local_rank      = 0\n",
        "# rank            = 0\n",
        "# class_names, num_classes = get_classes(classes_path)\n",
        "# backbone_name = 'resnet50'\n",
        "# model_size = 'tiny'\n",
        "# model = YoloBody(input_shape, num_classes, phi, backbone_name, model_size, pretrained=pretrained)\n",
        "# yolo_loss = Loss(model)\n",
        "# time_str        = datetime.datetime.strftime(datetime.datetime.now(),'%Y_%m_%d_%H_%M_%S')\n",
        "# log_dir         = os.path.join(save_dir, \"loss_\" + str(time_str))\n",
        "# loss_history    = LossHistory(log_dir, model, input_shape=input_shape)\n",
        "# scaler = None\n",
        "\n",
        "# model_train     = model.train()\n",
        "# ema = ModelEMA(model_train)\n",
        "# with open(train_annotation_path, encoding='utf-8') as f:\n",
        "#     train_lines = f.readlines()\n",
        "# with open(val_annotation_path, encoding='utf-8') as f:\n",
        "#     val_lines   = f.readlines()\n",
        "# num_train   = len(train_lines)\n",
        "# num_val     = len(val_lines)\n",
        "\n",
        "# show_config(\n",
        "#             classes_path = classes_path, model_path = model_path, input_shape = input_shape, \\\n",
        "#             Init_Epoch = Init_Epoch, Freeze_Epoch = Freeze_Epoch, UnFreeze_Epoch = UnFreeze_Epoch, Freeze_batch_size = Freeze_batch_size, Unfreeze_batch_size = Unfreeze_batch_size, Freeze_Train = Freeze_Train, \\\n",
        "#             Init_lr = Init_lr, Min_lr = Min_lr, optimizer_type = optimizer_type, momentum = momentum, lr_decay_type = lr_decay_type, \\\n",
        "#             save_period = save_period, save_dir = save_dir, num_workers = num_workers, num_train = num_train, num_val = num_val\n",
        "#         )\n",
        "# wanted_step = 5e4 if optimizer_type == \"sgd\" else 1.5e4\n",
        "# total_step  = num_train // Unfreeze_batch_size * UnFreeze_Epoch\n",
        "# if total_step <= wanted_step:\n",
        "#     if num_train // Unfreeze_batch_size == 0:\n",
        "#         raise ValueError('The dataset is too small to continue training, please expand the dataset.')\n",
        "#     wanted_epoch = wanted_step // (num_train // Unfreeze_batch_size) + 1\n",
        "#     print(\"\\n\\033[1;33;44m[Warning] It is recommended to set the total training steps to at least %d when using the %s optimizer.\\033[0m\" % (wanted_step, optimizer_type))\n",
        "#     print(\"\\033[1;33;44m[Warning] The total training data for this run is %d, Unfreeze_batch_size is %d, training for %d Epochs, resulting in a total training step of %d.\\033[0m\" % (num_train, Unfreeze_batch_size, UnFreeze_Epoch, total_step))\n",
        "#     print(\"\\033[1;33;44m[Warning] Since the total training step is %d, which is less than the recommended total step %d, it is recommended to set the total epochs to %d.\\033[0m\" % (total_step, wanted_step, wanted_epoch))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5TFNB_xtAov"
      },
      "outputs": [],
      "source": [
        "# UnFreeze_flag = False\n",
        "# if Freeze_Train:\n",
        "#             for param in model.backbone.parameters():\n",
        "#                 param.requires_grad = False\n",
        "\n",
        "# batch_size = Freeze_batch_size if Freeze_Train else Unfreeze_batch_size\n",
        "# nbs             = 64\n",
        "# lr_limit_max    = 1e-3 if optimizer_type == 'adam' else 5e-2\n",
        "# lr_limit_min    = 3e-4 if optimizer_type == 'adam' else 5e-4\n",
        "# Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
        "# Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
        "# pg0, pg1, pg2 = [], [], []\n",
        "# for k, v in model.named_modules():\n",
        "#     if hasattr(v, \"bias\") and isinstance(v.bias, nn.Parameter):\n",
        "#         pg2.append(v.bias)\n",
        "#     if isinstance(v, nn.BatchNorm2d) or \"bn\" in k:\n",
        "#         pg0.append(v.weight)\n",
        "#     elif hasattr(v, \"weight\") and isinstance(v.weight, nn.Parameter):\n",
        "#         pg1.append(v.weight)\n",
        "# optimizer = {\n",
        "#     'adam'  : optim.Adam(pg0, Init_lr_fit, betas = (momentum, 0.999)),\n",
        "#     'sgd'   : optim.SGD(pg0, Init_lr_fit, momentum = momentum, nesterov=True)\n",
        "# }[optimizer_type]\n",
        "# optimizer.add_param_group({\"params\": pg1, \"weight_decay\": weight_decay})\n",
        "# optimizer.add_param_group({\"params\": pg2})\n",
        "\n",
        "# lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n",
        "# epoch_step      = num_train // batch_size\n",
        "# epoch_step_val  = num_val // batch_size\n",
        "# if epoch_step == 0 or epoch_step_val == 0:\n",
        "#     raise ValueError(\"The dataset is too small for training. Please expand the dataset.\")\n",
        "\n",
        "# if ema:\n",
        "#     ema.updates     = epoch_step * Init_Epoch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBlnqpjTuI7L"
      },
      "outputs": [],
      "source": [
        "# train_dataset   = YoloDataset(train_lines, input_shape, num_classes, epoch_length=UnFreeze_Epoch, mosaic=mosaic, mixup=mixup, mosaic_prob=mosaic_prob, mixup_prob=mixup_prob, train=True, special_aug_ratio=special_aug_ratio)\n",
        "# val_dataset     = YoloDataset(val_lines, input_shape, num_classes, epoch_length=UnFreeze_Epoch,mosaic=False, mixup=False, mosaic_prob=0, mixup_prob=0, train=False, special_aug_ratio=0)\n",
        "\n",
        "# train_sampler   = None\n",
        "# val_sampler     = None\n",
        "# shuffle_flag         = True\n",
        "\n",
        "# gen             = DataLoader(train_dataset, shuffle = shuffle_flag, batch_size = batch_size, num_workers = num_workers, pin_memory=True,drop_last=True, collate_fn=yolo_dataset_collate, sampler=train_sampler, worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
        "# gen_val         = DataLoader(val_dataset  , shuffle = shuffle_flag, batch_size = batch_size, num_workers = num_workers, pin_memory=True, drop_last=True, collate_fn=yolo_dataset_collate, sampler=val_sampler, worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
        "\n",
        "# eval_callback   = EvalCallback(model, input_shape, class_names, num_classes, val_lines, log_dir, Cuda,eval_flag=eval_flag, period=eval_period)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7FQGyda00NY"
      },
      "outputs": [],
      "source": [
        "# for epoch in range(Init_Epoch, UnFreeze_Epoch):\n",
        "#     if epoch >= Freeze_Epoch and not UnFreeze_flag and Freeze_Train:\n",
        "#         batch_size = Unfreeze_batch_size\n",
        "#         nbs             = 64\n",
        "#         lr_limit_max    = 1e-3 if optimizer_type == 'adam' else 5e-2\n",
        "#         lr_limit_min    = 3e-4 if optimizer_type == 'adam' else 5e-4\n",
        "#         Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
        "#         Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
        "\n",
        "#         lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n",
        "\n",
        "#         for param in model.backbone.parameters():\n",
        "#             param.requires_grad = True\n",
        "\n",
        "#         epoch_step      = num_train // batch_size\n",
        "#         epoch_step_val  = num_val // batch_size\n",
        "\n",
        "#         if epoch_step == 0 or epoch_step_val == 0:\n",
        "#             raise ValueError(\"The dataset is too small for training. Please expand the dataset.\")\n",
        "\n",
        "#         if ema:\n",
        "#             ema.updates     = epoch_step * epoch\n",
        "\n",
        "#         if distributed:\n",
        "#             batch_size  = batch_size // ngpus_per_node\n",
        "\n",
        "#         gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True,\n",
        "#                                     drop_last=True, collate_fn=yolo_dataset_collate, sampler=train_sampler,\n",
        "#                                     worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
        "#         gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True,\n",
        "#                                     drop_last=True, collate_fn=yolo_dataset_collate, sampler=val_sampler,\n",
        "#                                     worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
        "\n",
        "#         UnFreeze_flag   = True\n",
        "\n",
        "#     gen.dataset.epoch_now       = epoch\n",
        "#     gen_val.dataset.epoch_now   = epoch\n",
        "\n",
        "\n",
        "\n",
        "#     set_optimizer_lr(optimizer, lr_scheduler_func, epoch)\n",
        "\n",
        "#     fit_one_epoch(model_train, model, ema, yolo_loss, loss_history, eval_callback, optimizer, epoch, epoch_step, epoch_step_val, gen, gen_val, UnFreeze_Epoch, Cuda, fp16, scaler, save_period, save_dir, local_rank)\n",
        "\n",
        "\n",
        "\n",
        "# if local_rank == 0:\n",
        "#     loss_history.writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lnemxFwTEgIY",
        "outputId": "710b3925-9496-4be1-ff2a-ccc2d3314d58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initialize network with normal type\n",
            "Configurations:\n",
            "----------------------------------------------------------------------\n",
            "|                     keys |                                   values|\n",
            "----------------------------------------------------------------------\n",
            "|             classes_path |                 /content/voc_classes.txt|\n",
            "|               model_path |                                         |\n",
            "|              input_shape |                               [640, 640]|\n",
            "|               Init_Epoch |                                        0|\n",
            "|             Freeze_Epoch |                                       20|\n",
            "|           UnFreeze_Epoch |                                      100|\n",
            "|        Freeze_batch_size |                                       16|\n",
            "|      Unfreeze_batch_size |                                        8|\n",
            "|             Freeze_Train |                                    False|\n",
            "|                  Init_lr |                                     0.01|\n",
            "|                   Min_lr |                                   0.0001|\n",
            "|           optimizer_type |                                      sgd|\n",
            "|                 momentum |                                    0.937|\n",
            "|            lr_decay_type |                                      cos|\n",
            "|              save_period |                                       10|\n",
            "|                 save_dir |                                     logs|\n",
            "|              num_workers |                                        4|\n",
            "|                num_train |                                    13870|\n",
            "|                  num_val |                                     1542|\n",
            "----------------------------------------------------------------------\n",
            "Start Train\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/100:   0%|          | 0/1733 [00:00<?, ?it/s<class 'dict'>]\u001b[A\n",
            "Epoch 1/100:   0%|          | 0/1733 [00:12<?, ?it/s, loss=8.85e+3, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 1/1733 [00:12<5:58:10, 12.41s/it, loss=8.85e+3, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 1/1733 [00:12<5:58:10, 12.41s/it, loss=7.58e+3, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 2/1733 [00:12<2:37:22,  5.46s/it, loss=7.58e+3, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 2/1733 [00:13<2:37:22,  5.46s/it, loss=9.81e+3, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 3/1733 [00:13<1:33:23,  3.24s/it, loss=9.81e+3, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 3/1733 [00:14<1:33:23,  3.24s/it, loss=1.11e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 4/1733 [00:14<1:03:11,  2.19s/it, loss=1.11e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 4/1733 [00:14<1:03:11,  2.19s/it, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 5/1733 [00:14<46:41,  1.62s/it, loss=1.06e+4, lr=0.000125]  \u001b[A\n",
            "Epoch 1/100:   0%|          | 5/1733 [00:15<46:41,  1.62s/it, loss=1.26e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 6/1733 [00:15<36:44,  1.28s/it, loss=1.26e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 6/1733 [00:15<36:44,  1.28s/it, loss=1.18e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 7/1733 [00:15<30:18,  1.05s/it, loss=1.18e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 7/1733 [00:16<30:18,  1.05s/it, loss=1.22e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 8/1733 [00:16<26:07,  1.10it/s, loss=1.22e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   0%|          | 8/1733 [00:17<26:07,  1.10it/s, loss=1.21e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 9/1733 [00:17<23:18,  1.23it/s, loss=1.21e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 9/1733 [00:17<23:18,  1.23it/s, loss=1.16e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 10/1733 [00:17<21:44,  1.32it/s, loss=1.16e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 10/1733 [00:18<21:44,  1.32it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 11/1733 [00:18<21:04,  1.36it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 11/1733 [00:19<21:04,  1.36it/s, loss=1.11e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 12/1733 [00:19<20:33,  1.40it/s, loss=1.11e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 12/1733 [00:19<20:33,  1.40it/s, loss=1.23e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 13/1733 [00:19<20:00,  1.43it/s, loss=1.23e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 13/1733 [00:20<20:00,  1.43it/s, loss=1.28e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 14/1733 [00:20<19:35,  1.46it/s, loss=1.28e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 14/1733 [00:21<19:35,  1.46it/s, loss=1.27e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 15/1733 [00:21<19:15,  1.49it/s, loss=1.27e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 15/1733 [00:21<19:15,  1.49it/s, loss=1.23e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 16/1733 [00:21<19:25,  1.47it/s, loss=1.23e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 16/1733 [00:22<19:25,  1.47it/s, loss=1.21e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 17/1733 [00:22<19:19,  1.48it/s, loss=1.21e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 17/1733 [00:23<19:19,  1.48it/s, loss=1.2e+4, lr=0.000125] \u001b[A\n",
            "Epoch 1/100:   1%|          | 18/1733 [00:23<19:26,  1.47it/s, loss=1.2e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 18/1733 [00:23<19:26,  1.47it/s, loss=1.19e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 19/1733 [00:23<20:07,  1.42it/s, loss=1.19e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 19/1733 [00:24<20:07,  1.42it/s, loss=1.17e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 20/1733 [00:24<19:42,  1.45it/s, loss=1.17e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 20/1733 [00:25<19:42,  1.45it/s, loss=1.17e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 21/1733 [00:25<19:15,  1.48it/s, loss=1.17e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|          | 21/1733 [00:25<19:15,  1.48it/s, loss=1.19e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|         | 22/1733 [00:25<18:35,  1.53it/s, loss=1.19e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|         | 22/1733 [00:26<18:35,  1.53it/s, loss=1.2e+4, lr=0.000125] \u001b[A\n",
            "Epoch 1/100:   1%|         | 23/1733 [00:26<18:10,  1.57it/s, loss=1.2e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|         | 23/1733 [00:27<18:10,  1.57it/s, loss=1.2e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|         | 24/1733 [00:27<17:52,  1.59it/s, loss=1.2e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|         | 24/1733 [00:27<17:52,  1.59it/s, loss=1.19e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|         | 25/1733 [00:27<17:41,  1.61it/s, loss=1.19e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   1%|         | 25/1733 [00:28<17:41,  1.61it/s, loss=1.16e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 26/1733 [00:28<17:48,  1.60it/s, loss=1.16e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 26/1733 [00:28<17:48,  1.60it/s, loss=1.15e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 27/1733 [00:28<18:04,  1.57it/s, loss=1.15e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 27/1733 [00:29<18:04,  1.57it/s, loss=1.15e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 28/1733 [00:29<18:10,  1.56it/s, loss=1.15e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 28/1733 [00:30<18:10,  1.56it/s, loss=1.14e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 29/1733 [00:30<18:28,  1.54it/s, loss=1.14e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 29/1733 [00:30<18:28,  1.54it/s, loss=1.12e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 30/1733 [00:30<18:26,  1.54it/s, loss=1.12e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 30/1733 [00:31<18:26,  1.54it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 31/1733 [00:31<18:57,  1.50it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 31/1733 [00:32<18:57,  1.50it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 32/1733 [00:32<19:02,  1.49it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 32/1733 [00:33<19:02,  1.49it/s, loss=1.14e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 33/1733 [00:33<19:09,  1.48it/s, loss=1.14e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 33/1733 [00:33<19:09,  1.48it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 34/1733 [00:33<19:05,  1.48it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 34/1733 [00:34<19:05,  1.48it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 35/1733 [00:34<19:05,  1.48it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 35/1733 [00:35<19:05,  1.48it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 36/1733 [00:35<20:00,  1.41it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 36/1733 [00:35<20:00,  1.41it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 37/1733 [00:35<19:59,  1.41it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 37/1733 [00:36<19:59,  1.41it/s, loss=1.16e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 38/1733 [00:36<19:22,  1.46it/s, loss=1.16e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 38/1733 [00:37<19:22,  1.46it/s, loss=1.15e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 39/1733 [00:37<19:23,  1.46it/s, loss=1.15e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 39/1733 [00:37<19:23,  1.46it/s, loss=1.14e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 40/1733 [00:37<19:15,  1.47it/s, loss=1.14e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 40/1733 [00:38<19:15,  1.47it/s, loss=1.15e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 41/1733 [00:38<19:12,  1.47it/s, loss=1.15e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 41/1733 [00:39<19:12,  1.47it/s, loss=1.19e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 42/1733 [00:39<19:52,  1.42it/s, loss=1.19e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 42/1733 [00:39<19:52,  1.42it/s, loss=1.18e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 43/1733 [00:40<20:03,  1.40it/s, loss=1.18e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   2%|         | 43/1733 [00:40<20:03,  1.40it/s, loss=1.16e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 44/1733 [00:40<20:46,  1.35it/s, loss=1.16e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 44/1733 [00:41<20:46,  1.35it/s, loss=1.17e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 45/1733 [00:41<20:20,  1.38it/s, loss=1.17e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 45/1733 [00:42<20:20,  1.38it/s, loss=1.17e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 46/1733 [00:42<19:41,  1.43it/s, loss=1.17e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 46/1733 [00:42<19:41,  1.43it/s, loss=1.17e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 47/1733 [00:42<19:21,  1.45it/s, loss=1.17e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 47/1733 [00:43<19:21,  1.45it/s, loss=1.16e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 48/1733 [00:43<18:56,  1.48it/s, loss=1.16e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 48/1733 [00:44<18:56,  1.48it/s, loss=1.15e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 49/1733 [00:44<18:28,  1.52it/s, loss=1.15e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 49/1733 [00:44<18:28,  1.52it/s, loss=1.14e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 50/1733 [00:44<17:59,  1.56it/s, loss=1.14e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 50/1733 [00:45<17:59,  1.56it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 51/1733 [00:45<17:39,  1.59it/s, loss=1.13e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 51/1733 [00:45<17:39,  1.59it/s, loss=1.12e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 52/1733 [00:45<17:21,  1.61it/s, loss=1.12e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 52/1733 [00:46<17:21,  1.61it/s, loss=1.11e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 53/1733 [00:46<17:09,  1.63it/s, loss=1.11e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 53/1733 [00:47<17:09,  1.63it/s, loss=1.11e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 54/1733 [00:47<16:58,  1.65it/s, loss=1.11e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 54/1733 [00:47<16:58,  1.65it/s, loss=1.11e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 55/1733 [00:47<16:49,  1.66it/s, loss=1.11e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 55/1733 [00:48<16:49,  1.66it/s, loss=1.1e+4, lr=0.000125] \u001b[A\n",
            "Epoch 1/100:   3%|         | 56/1733 [00:48<16:47,  1.67it/s, loss=1.1e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 56/1733 [00:48<16:47,  1.67it/s, loss=1.09e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 57/1733 [00:48<16:45,  1.67it/s, loss=1.09e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 57/1733 [00:49<16:45,  1.67it/s, loss=1.09e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 58/1733 [00:49<16:43,  1.67it/s, loss=1.09e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 58/1733 [00:50<16:43,  1.67it/s, loss=1.08e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 59/1733 [00:50<16:40,  1.67it/s, loss=1.08e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 59/1733 [00:50<16:40,  1.67it/s, loss=1.08e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 60/1733 [00:50<16:39,  1.67it/s, loss=1.08e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   3%|         | 60/1733 [00:51<16:39,  1.67it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 61/1733 [00:51<16:35,  1.68it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 61/1733 [00:51<16:35,  1.68it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 62/1733 [00:51<16:33,  1.68it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 62/1733 [00:52<16:33,  1.68it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 63/1733 [00:52<16:32,  1.68it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 63/1733 [00:52<16:32,  1.68it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 64/1733 [00:53<16:31,  1.68it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 64/1733 [00:53<16:31,  1.68it/s, loss=1.05e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 65/1733 [00:53<16:32,  1.68it/s, loss=1.05e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 65/1733 [00:54<16:32,  1.68it/s, loss=1.05e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 66/1733 [00:54<16:50,  1.65it/s, loss=1.05e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 66/1733 [00:54<16:50,  1.65it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 67/1733 [00:54<17:04,  1.63it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 67/1733 [00:55<17:04,  1.63it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 68/1733 [00:55<17:26,  1.59it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 68/1733 [00:56<17:26,  1.59it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 69/1733 [00:56<17:39,  1.57it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 69/1733 [00:56<17:39,  1.57it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 70/1733 [00:56<17:57,  1.54it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 70/1733 [00:57<17:57,  1.54it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 71/1733 [00:57<17:57,  1.54it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 71/1733 [00:58<17:57,  1.54it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 72/1733 [00:58<18:18,  1.51it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 72/1733 [00:58<18:18,  1.51it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 73/1733 [00:58<18:36,  1.49it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 73/1733 [00:59<18:36,  1.49it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 74/1733 [00:59<19:00,  1.45it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 74/1733 [01:00<19:00,  1.45it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 75/1733 [01:00<18:53,  1.46it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 75/1733 [01:00<18:53,  1.46it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 76/1733 [01:00<18:28,  1.49it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 76/1733 [01:01<18:28,  1.49it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 77/1733 [01:01<17:58,  1.53it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   4%|         | 77/1733 [01:02<17:58,  1.53it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 78/1733 [01:02<17:25,  1.58it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 78/1733 [01:02<17:25,  1.58it/s, loss=1.08e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 79/1733 [01:02<17:02,  1.62it/s, loss=1.08e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 79/1733 [01:03<17:02,  1.62it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 80/1733 [01:03<16:44,  1.65it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 80/1733 [01:03<16:44,  1.65it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 81/1733 [01:03<16:37,  1.66it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 81/1733 [01:04<16:37,  1.66it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 82/1733 [01:04<16:28,  1.67it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 82/1733 [01:05<16:28,  1.67it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 83/1733 [01:05<16:21,  1.68it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 83/1733 [01:05<16:21,  1.68it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 84/1733 [01:05<16:20,  1.68it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 84/1733 [01:06<16:20,  1.68it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 85/1733 [01:06<16:17,  1.69it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 85/1733 [01:06<16:17,  1.69it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 86/1733 [01:06<16:13,  1.69it/s, loss=1.06e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 86/1733 [01:07<16:13,  1.69it/s, loss=1.07e+4, lr=0.000125]\u001b[A\n",
            "Epoch 1/100:   5%|         | 87/1733 [01:07<16:12,  1.69it/s, loss=1.07e+4, lr=0.000125]\u001b[A"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-84544aae23d3>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mset_optimizer_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0mfit_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_step_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnFreeze_Epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-f91bb3236039>\u001b[0m in \u001b[0;36mfit_one_epoch\u001b[0;34m(model_train, model, ema, yolo_loss, loss_history, eval_callback, optimizer, epoch, epoch_step, epoch_step_val, gen, gen_val, Epoch, cuda, fp16, scaler, save_period, save_dir, local_rank)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m#   \u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m#----------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clip gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "'''\n",
        "When training your own object detection model, be sure to pay attention to the following points:\n",
        "1. Before training, carefully check whether your format meets the requirements. This library requires the dataset format to be VOC format. The prepared content should include input images and labels.\n",
        "    The input images should be .jpg images, with no fixed size required. They will be automatically resized before being passed into training.\n",
        "    Grayscale images will be automatically converted to RGB images for training, no need to modify them yourself.\n",
        "    If the input images have a suffix other than jpg, you need to batch convert them to jpg before starting training.\n",
        "\n",
        "    The labels should be in .xml format, with the file containing the target information to be detected. The label files correspond to the input image files.\n",
        "\n",
        "2. The size of the loss value is used to judge whether the model has converged. It is important to have a trend of convergence, that is, the validation set loss keeps decreasing. If the validation set loss does not change much, the model has basically converged.\n",
        "    The specific size of the loss value does not have much significance. Large and small only depend on the way the loss is calculated. It does not have to be close to 0 to be good. If you want the loss to look better, you can directly divide the corresponding loss function by 10000.\n",
        "    The loss value during the training process will be saved in the loss_%Y_%m_%d_%H_%M_%S folder in the logs directory.\n",
        "\n",
        "3. The trained weight files are saved in the logs directory. Each training epoch contains several training steps. Each training step performs a gradient descent.\n",
        "    If you only train a few steps, it will not be saved. The concepts of epoch and step should be clarified.\n",
        "'''\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #---------------------------------#\n",
        "    #   Cuda    Whether to use Cuda\n",
        "    #           Set to False if no GPU is available\n",
        "    #---------------------------------#\n",
        "    Cuda            = True\n",
        "    #----------------------------------------------#\n",
        "    #   Seed    Used to fix the random seed\n",
        "    #           Ensures that each independent training run produces the same results\n",
        "    #----------------------------------------------#\n",
        "    seed            = 11\n",
        "    #---------------------------------------------------------------------#\n",
        "    #   distributed     Used to specify whether to use single-machine multi-GPU distributed training\n",
        "    #                   Terminal commands only support Ubuntu. CUDA_VISIBLE_DEVICES is used to specify GPUs on Ubuntu.\n",
        "    #                   On Windows, it defaults to using DP mode to call all GPUs and does not support DDP.\n",
        "    #   DP mode:\n",
        "    #       Set             distributed = False\n",
        "    #       Enter in terminal    CUDA_VISIBLE_DEVICES=0,1 python train.py\n",
        "    #   DDP mode:\n",
        "    #       Set             distributed = True\n",
        "    #       Enter in terminal    CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 train.py\n",
        "    #---------------------------------------------------------------------#\n",
        "    distributed     = False\n",
        "    #---------------------------------------------------------------------#\n",
        "    #   sync_bn     Whether to use sync_bn, available in DDP mode with multiple GPUs\n",
        "    #---------------------------------------------------------------------#\n",
        "    sync_bn         = False\n",
        "    #---------------------------------------------------------------------#\n",
        "    #   fp16        Whether to use mixed precision training\n",
        "    #               Can reduce memory usage by about half, requires pytorch 1.7.1 or above\n",
        "    #---------------------------------------------------------------------#\n",
        "    fp16            = False\n",
        "    #---------------------------------------------------------------------#\n",
        "    #   classes_path    Points to the txt file under model_data, related to the dataset used for training\n",
        "    #                   Be sure to modify classes_path before training to correspond to your own dataset\n",
        "    #---------------------------------------------------------------------#\n",
        "    classes_path    = '/content/voc_classes.txt'\n",
        "    #----------------------------------------------------------------------------------------------------------------------------#\n",
        "    #   The download of the weight file can be found in the README, and can be downloaded via a network disk. The pre-trained weights of the model are universal for different datasets because the features are universal.\n",
        "    #   The important part of the pre-trained weights of the model is the weight part of the backbone feature extraction network, which is used for feature extraction.\n",
        "    #   Pre-trained weights must be used in 99% of cases. Without them, the weights of the backbone part are too random, the feature extraction effect is not obvious, and the network training results will not be good.\n",
        "    #\n",
        "    #   If there is an interruption in the training process, you can set the model_path to the weight file in the logs folder to reload the weights that have already been partially trained.\n",
        "    #   At the same time, modify the parameters of the freezing stage or the unfreezing stage below to ensure the continuity of the model epoch.\n",
        "    #\n",
        "    #   When model_path = '', the weights of the entire model are not loaded.\n",
        "    #\n",
        "    #   Here, the weights of the entire model are used, so they are loaded in train.py.\n",
        "    #   If you want the model to start training from scratch, set model_path = '', and set Freeze_Train = False below. In this case, training starts from scratch, and there is no freezing process for the backbone.\n",
        "    #\n",
        "    #   Generally speaking, the training effect of the network starting from scratch will be very poor because the weights are too random and the feature extraction effect is not obvious. Therefore, it is very, very, very not recommended to train from scratch!\n",
        "    #   There are two solutions for training from scratch:\n",
        "    #   1. Thanks to the powerful data augmentation capability of the Mosaic data augmentation method, when UnFreeze_Epoch is set to a large value (300 or above), batch size is large (16 or above), and data is large (more than ten thousand),\n",
        "    #      you can set mosaic=True and start training directly with randomly initialized parameters, but the effect obtained is still not as good as with pre-training. (This can be done with large datasets like COCO)\n",
        "    #   2. Understand the imagenet dataset, first train the classification model to obtain the weights of the backbone part of the network. The backbone part of the classification model is universal with this model, and training is based on this.\n",
        "    #----------------------------------------------------------------------------------------------------------------------------#\n",
        "    model_path      = ''\n",
        "    #------------------------------------------------------#\n",
        "    #   input_shape     The input shape size, must be a multiple of 32\n",
        "    #------------------------------------------------------#\n",
        "    input_shape     = [640, 640]\n",
        "    #------------------------------------------------------#\n",
        "    #   phi             The version of yolov8 being used\n",
        "    #                   n : corresponds to yolov8_n\n",
        "    #                   s : corresponds to yolov8_s\n",
        "    #                   m : corresponds to yolov8_m\n",
        "    #                   l : corresponds to yolov8_l\n",
        "    #                   x : corresponds to yolov8_x\n",
        "    #------------------------------------------------------#\n",
        "    phi             = 'n'\n",
        "    #----------------------------------------------------------------------------------------------------------------------------#\n",
        "    #   pretrained      Whether to use the pretrained weights of the backbone network. Here, the weights of the backbone are used, so they are loaded during model construction.\n",
        "    #                   If model_path is set, the weights of the backbone do not need to be loaded, and the value of pretrained is meaningless.\n",
        "    #                   If model_path is not set and pretrained = True, only the backbone is loaded to start training.\n",
        "    #                   If model_path is not set and pretrained = False, and Freeze_Train = False, training starts from scratch without freezing the backbone.\n",
        "    #----------------------------------------------------------------------------------------------------------------------------#\n",
        "    pretrained      = False\n",
        "    #------------------------------------------------------------------#\n",
        "    #   mosaic              Mosaic data augmentation.\n",
        "    #   mosaic_prob         Probability of using mosaic data augmentation for each step, default is 50%.\n",
        "    #\n",
        "    #   mixup               Whether to use mixup data augmentation, only effective when mosaic=True.\n",
        "    #                       Mixup processing will only be applied to images enhanced by mosaic.\n",
        "    #   mixup_prob          Probability of using mixup data augmentation after mosaic, default is 50%.\n",
        "    #                       The total mixup probability is mosaic_prob * mixup_prob.\n",
        "    #\n",
        "    #   special_aug_ratio   Referencing YoloX, since the training images generated by Mosaic are far from the real distribution of natural images.\n",
        "    #                       When mosaic=True, this code will enable mosaic within the special_aug_ratio range.\n",
        "    #                       Default is the first 70% of epochs, for 100 epochs, mosaic will be enabled for 70 epochs.\n",
        "    #------------------------------------------------------------------#\n",
        "    mosaic              = True\n",
        "    mosaic_prob         = 0.5\n",
        "    mixup               = True\n",
        "    mixup_prob          = 0.5\n",
        "    special_aug_ratio   = 0.7\n",
        "    #------------------------------------------------------------------#\n",
        "    #   label_smoothing     Label smoothing. Generally below 0.01, such as 0.01, 0.005.\n",
        "    #------------------------------------------------------------------#\n",
        "    label_smoothing     = 0\n",
        "\n",
        "    #----------------------------------------------------------------------------------------------------------------------------#\n",
        "    #   Training is divided into two stages: freezing stage and unfreezing stage. The freezing stage is set to meet the training needs of users with insufficient machine performance.\n",
        "    #   Freezing training requires less memory. If the GPU is very poor, you can set Freeze_Epoch equal to UnFreeze_Epoch, Freeze_Train = True, and only perform freezing training.\n",
        "    #\n",
        "    #   Here are some parameter setting suggestions, which can be flexibly adjusted according to your needs:\n",
        "    #   (1) Training from the pre-trained weights of the entire model:\n",
        "    #       Adam:\n",
        "    #           Init_Epoch = 0, Freeze_Epoch = 50, UnFreeze_Epoch = 100, Freeze_Train = True, optimizer_type = 'adam', Init_lr = 1e-3, weight_decay = 0. (Freezing)\n",
        "    #           Init_Epoch = 0, UnFreeze_Epoch = 100, Freeze_Train = False, optimizer_type = 'adam', Init_lr = 1e-3, weight_decay = 0. (Not freezing)\n",
        "    #       SGD:\n",
        "    #           Init_Epoch = 0, Freeze_Epoch = 50, UnFreeze_Epoch = 300, Freeze_Train = True, optimizer_type = 'sgd', Init_lr = 1e-2, weight_decay = 5e-4. (Freezing)\n",
        "    #           Init_Epoch = 0, UnFreeze_Epoch = 300, Freeze_Train = False, optimizer_type = 'sgd', Init_lr = 1e-2, weight_decay = 5e-4. (Not freezing)\n",
        "    #       Among them: UnFreeze_Epoch can be adjusted between 100-300.\n",
        "    #   (2) Training from scratch:\n",
        "    #       Init_Epoch = 0, UnFreeze_Epoch >= 300, Unfreeze_batch_size >= 16, Freeze_Train = False (Not freezing training)\n",
        "    #       Among them: UnFreeze_Epoch should be no less than 300. optimizer_type = 'sgd', Init_lr = 1e-2, mosaic = True.\n",
        "    #   (3) Setting of batch_size:\n",
        "    #       Within the acceptable range of the GPU, the larger the better. If there is insufficient memory (OOM or CUDA out of memory), reduce the batch_size.\n",
        "    #       Due to the influence of the BatchNorm layer, the minimum batch_size is 2 and cannot be 1.\n",
        "    #       Normally, Freeze_batch_size is recommended to be 1-2 times the Unfreeze_batch_size. It is not recommended to set a large gap because it is related to the automatic adjustment of the learning rate.\n",
        "    #----------------------------------------------------------------------------------------------------------------------------#\n",
        "    #------------------------------------------------------------------#\n",
        "    #   Freezing stage training parameters\n",
        "    #   At this time, the backbone of the model is frozen, and the feature extraction network does not change\n",
        "    #   It occupies less memory and only fine-tunes the network\n",
        "    #   Init_Epoch          The current starting epoch of the model, its value can be greater than Freeze_Epoch, such as setting:\n",
        "    #                       Init_Epoch = 60, Freeze_Epoch = 50, UnFreeze_Epoch = 100\n",
        "    #                       It will skip the freezing stage and start directly from epoch 60, and adjust the corresponding learning rate.\n",
        "    #                       (Used for resuming training from a checkpoint)\n",
        "    #   Freeze_Epoch        The Freeze_Epoch for freezing training of the model\n",
        "    #                       (Invalid when Freeze_Train=False)\n",
        "    #   Freeze_batch_size   The batch_size for freezing training of the model\n",
        "    #                       (Invalid when Freeze_Train=False)\n",
        "    #------------------------------------------------------------------#\n",
        "    Init_Epoch          = 0\n",
        "    Freeze_Epoch        = 20\n",
        "    Freeze_batch_size   = 16\n",
        "    #------------------------------------------------------------------#\n",
        "    #   Unfreezing stage training parameters\n",
        "    #   At this time, the backbone of the model is not frozen, and the feature extraction network will change\n",
        "    #   It occupies more memory, and all parameters of the network will change\n",
        "    #   UnFreeze_Epoch          The total number of epochs the model is trained for\n",
        "    #                           SGD requires a longer time to converge, so a larger UnFreeze_Epoch is set\n",
        "    #                           Adam can use a relatively smaller UnFreeze_Epoch\n",
        "    #   Unfreeze_batch_size     The batch_size of the model after unfreezing\n",
        "    #------------------------------------------------------------------#\n",
        "    UnFreeze_Epoch      = 100\n",
        "    Unfreeze_batch_size = 8\n",
        "    #------------------------------------------------------------------#\n",
        "    #   Freeze_Train    Whether to perform freeze training\n",
        "    #                   By default, the backbone is frozen first and then unfrozen for training.\n",
        "    #------------------------------------------------------------------#\n",
        "    Freeze_Train        = False\n",
        "\n",
        "    #------------------------------------------------------------------#\n",
        "    #   Other training parameters: learning rate, optimizer, learning rate decay, etc.\n",
        "    #------------------------------------------------------------------#\n",
        "    #------------------------------------------------------------------#\n",
        "    #   Init_lr         The maximum learning rate of the model\n",
        "    #   Min_lr          The minimum learning rate of the model, default is 0.01 times the maximum learning rate\n",
        "    #------------------------------------------------------------------#\n",
        "    Init_lr             = 1e-2\n",
        "    Min_lr              = Init_lr * 0.01\n",
        "    #------------------------------------------------------------------#\n",
        "    #   optimizer_type  The type of optimizer to use, options are adam and sgd\n",
        "    #                   When using the Adam optimizer, it is recommended to set Init_lr=1e-3\n",
        "    #                   When using the SGD optimizer, it is recommended to set Init_lr=1e-2\n",
        "    #   momentum        The momentum parameter used within the optimizer\n",
        "    #   weight_decay    Weight decay, can prevent overfitting\n",
        "    #                   Adam can cause weight_decay errors, it is recommended to set it to 0 when using Adam.\n",
        "    #------------------------------------------------------------------#\n",
        "    optimizer_type      = \"sgd\"\n",
        "    momentum            = 0.937\n",
        "    weight_decay        = 5e-4\n",
        "    #------------------------------------------------------------------#\n",
        "    #   lr_decay_type   The type of learning rate decay to use, options are step and cos\n",
        "    #------------------------------------------------------------------#\n",
        "    lr_decay_type       = \"cos\"\n",
        "    #------------------------------------------------------------------#\n",
        "    #   save_period     How many epochs to save the weights once\n",
        "    #------------------------------------------------------------------#\n",
        "    save_period         = 10\n",
        "    #------------------------------------------------------------------#\n",
        "    #   save_dir        \n",
        "    #------------------------------------------------------------------#\n",
        "    save_dir            = 'logs'\n",
        "    #------------------------------------------------------------------#\n",
        "    #   eval_flag       Whether to perform evaluation during training, the evaluation target is the validation set\n",
        "    #                   After installing the pycocotools library, the evaluation experience is better.\n",
        "    #   eval_period     Represents how many epochs to evaluate once, it is not recommended to evaluate frequently\n",
        "    #                   Evaluation consumes a lot of time, frequent evaluation will cause training to be very slow\n",
        "    #   The mAP obtained here will be different from the one obtained by get_map.py for two reasons:\n",
        "    #   (1) The mAP obtained here is the mAP of the validation set.\n",
        "    #   (2) The evaluation parameters set here are relatively conservative, aiming to speed up the evaluation.\n",
        "    #------------------------------------------------------------------#\n",
        "    eval_flag           = True\n",
        "    eval_period         = 10\n",
        "    #------------------------------------------------------------------#\n",
        "    #   num_workers     Used to set whether to use multi-threading to read data\n",
        "    #                   Enabling it will speed up data reading, but will occupy more memory\n",
        "    #                   Computers with less memory can set it to 2 or 0\n",
        "    #------------------------------------------------------------------#\n",
        "    num_workers         = 4\n",
        "\n",
        "    #------------------------------------------------------#\n",
        "    #   train_annotation_path   Path to training images and labels\n",
        "    #   val_annotation_path     Path to validation images and labels\n",
        "    #------------------------------------------------------#\n",
        "    train_annotation_path   = '/content/2012_train.txt'\n",
        "    val_annotation_path     = '/content/2012_val.txt'\n",
        "\n",
        "    seed_everything(seed)\n",
        "    #------------------------------------------------------#\n",
        "    #   Set the GPU to be used\n",
        "    #------------------------------------------------------#\n",
        "    ngpus_per_node  = torch.cuda.device_count()\n",
        "    if distributed:\n",
        "        dist.init_process_group(backend=\"nccl\")\n",
        "        local_rank  = int(os.environ[\"LOCAL_RANK\"])\n",
        "        rank        = int(os.environ[\"RANK\"])\n",
        "        device      = torch.device(\"cuda\", local_rank)\n",
        "        if local_rank == 0:\n",
        "            print(f\"[{os.getpid()}] (rank = {rank}, local_rank = {local_rank}) training...\")\n",
        "            print(\"Gpu Device Count : \", ngpus_per_node)\n",
        "    else:\n",
        "        device          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        local_rank      = 0\n",
        "        rank            = 0\n",
        "\n",
        "    #------------------------------------------------------#\n",
        "    #   Get classes and anchors\n",
        "    #------------------------------------------------------#\n",
        "    class_names, num_classes = get_classes(classes_path)\n",
        "\n",
        "    #----------------------------------------------------#\n",
        "    #   Download pre-trained weights\n",
        "    #----------------------------------------------------#\n",
        "    if pretrained:\n",
        "        if distributed:\n",
        "            if local_rank == 0:\n",
        "                download_weights(phi)\n",
        "            dist.barrier()\n",
        "        else:\n",
        "            download_weights(phi)\n",
        "\n",
        "\n",
        "    #------------------------------------------------------#\n",
        "    #  Set the backbone : resnet50, nextvit, mambavision and visionmamba\n",
        "    #  Set the backbone model size : tiny, small, base\n",
        "    #------------------------------------------------------#\n",
        "    backbone_name = 'deit'\n",
        "    model_size = 'tiny'\n",
        "    #------------------------------------------------------#\n",
        "    #   Create YOLO model\n",
        "    #------------------------------------------------------#\n",
        "    model = YoloBody(input_shape, num_classes, phi, backbone_name, model_size, pretrained=pretrained)\n",
        "\n",
        "    if model_path != '':\n",
        "        #------------------------------------------------------#\n",
        "        #   Please refer to the README for the weight file, download from Baidu Netdisk\n",
        "        #------------------------------------------------------#\n",
        "        if local_rank == 0:\n",
        "            print('Load weights {}.'.format(model_path))\n",
        "\n",
        "        #------------------------------------------------------#\n",
        "        #   Load according to the keys of the pre-trained weights and the model's keys\n",
        "        #------------------------------------------------------#\n",
        "        model_dict      = model.state_dict()\n",
        "        pretrained_dict = torch.load(model_path, map_location = device)\n",
        "        load_key, no_load_key, temp_dict = [], [], {}\n",
        "        for k, v in pretrained_dict.items():\n",
        "            if k in model_dict.keys() and np.shape(model_dict[k]) == np.shape(v):\n",
        "                temp_dict[k] = v\n",
        "                load_key.append(k)\n",
        "            else:\n",
        "                no_load_key.append(k)\n",
        "        model_dict.update(temp_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "        #------------------------------------------------------#\n",
        "        #    Display keys that did not match\n",
        "        #------------------------------------------------------#\n",
        "        if local_rank == 0:\n",
        "            print(\"\\nSuccessful Load Key:\", str(load_key)[:500], \"\\nSuccessful Load Key Num:\", len(load_key))\n",
        "            print(\"\\nFail To Load Key:\", str(no_load_key)[:500], \"\\nFail To Load Key num:\", len(no_load_key))\n",
        "            print(\"\\n\\033[1;33;44mNote: It is normal if the head part is not loaded, but it is an error if the Backbone part is not loaded.\\033[0m\")\n",
        "\n",
        "    #----------------------#\n",
        "    #  Get the loss function\n",
        "    #----------------------#\n",
        "    yolo_loss = Loss(model)\n",
        "    #----------------------#\n",
        "    #   Record Loss\n",
        "    #----------------------#\n",
        "    if local_rank == 0:\n",
        "        time_str        = datetime.datetime.strftime(datetime.datetime.now(),'%Y_%m_%d_%H_%M_%S')\n",
        "        log_dir         = os.path.join(save_dir, \"loss_\" + str(time_str))\n",
        "        loss_history    = LossHistory(log_dir, model, input_shape=input_shape)\n",
        "    else:\n",
        "        loss_history    = None\n",
        "\n",
        "    #------------------------------------------------------------------#\n",
        "    #   torch 1.2 does not support amp, it is recommended to use torch 1.7.1 or above to correctly use fp16\n",
        "    #   Therefore, torch1.2 here shows \"could not be resolved\"\n",
        "    #------------------------------------------------------------------#\n",
        "    if fp16:\n",
        "        from torch.cuda.amp import GradScaler as GradScaler\n",
        "        scaler = GradScaler()\n",
        "    else:\n",
        "        scaler = None\n",
        "\n",
        "    model_train     = model.train()\n",
        "\n",
        "\n",
        "\n",
        "    if sync_bn and ngpus_per_node > 1 and distributed:\n",
        "        model_train = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model_train)\n",
        "    elif sync_bn:\n",
        "        print(\"Sync_bn is not support in one gpu or not distributed.\")\n",
        "\n",
        "    if Cuda:\n",
        "        if distributed:\n",
        "            model_train = model_train.cuda(local_rank)\n",
        "            model_train = torch.nn.parallel.DistributedDataParallel(model_train, device_ids=[local_rank], find_unused_parameters=True)\n",
        "        else:\n",
        "            model_train = torch.nn.DataParallel(model)\n",
        "            cudnn.benchmark = True\n",
        "            model_train = model_train.cuda()\n",
        "\n",
        "    #----------------------------#\n",
        "    #   Weight Smoothing\n",
        "    #----------------------------#\n",
        "    ema = ModelEMA(model_train)\n",
        "\n",
        "    #---------------------------#\n",
        "    #   Read the dataset corresponding txt\n",
        "    #---------------------------#\n",
        "    with open(train_annotation_path, encoding='utf-8') as f:\n",
        "        train_lines = f.readlines()\n",
        "    with open(val_annotation_path, encoding='utf-8') as f:\n",
        "        val_lines   = f.readlines()\n",
        "    num_train   = len(train_lines)\n",
        "    num_val     = len(val_lines)\n",
        "\n",
        "    if local_rank == 0:\n",
        "        show_config(\n",
        "            classes_path = classes_path, model_path = model_path, input_shape = input_shape, \\\n",
        "            Init_Epoch = Init_Epoch, Freeze_Epoch = Freeze_Epoch, UnFreeze_Epoch = UnFreeze_Epoch, Freeze_batch_size = Freeze_batch_size, Unfreeze_batch_size = Unfreeze_batch_size, Freeze_Train = Freeze_Train, \\\n",
        "            Init_lr = Init_lr, Min_lr = Min_lr, optimizer_type = optimizer_type, momentum = momentum, lr_decay_type = lr_decay_type, \\\n",
        "            save_period = save_period, save_dir = save_dir, num_workers = num_workers, num_train = num_train, num_val = num_val\n",
        "        )\n",
        "        #---------------------------------------------------------#\n",
        "        #   Total training epochs refer to the total number of times the entire dataset is traversed.\n",
        "        #   Total training steps refer to the total number of gradient descent steps.\n",
        "        #   Each training epoch contains several training steps, and each training step performs a gradient descent.\n",
        "        #   Here, only the minimum training epochs are suggested, with no upper limit, considering only the unfreezing part.\n",
        "        #----------------------------------------------------------#\n",
        "        wanted_step = 5e4 if optimizer_type == \"sgd\" else 1.5e4\n",
        "        total_step  = num_train // Unfreeze_batch_size * UnFreeze_Epoch\n",
        "        if total_step <= wanted_step:\n",
        "            if num_train // Unfreeze_batch_size == 0:\n",
        "                raise ValueError('The dataset is too small to continue training, please expand the dataset.')\n",
        "            wanted_epoch = wanted_step // (num_train // Unfreeze_batch_size) + 1\n",
        "            print(\"\\n\\033[1;33;44m[Warning] It is recommended to set the total training steps to at least %d when using the %s optimizer.\\033[0m\" % (wanted_step, optimizer_type))\n",
        "            print(\"\\033[1;33;44m[Warning] The total training data for this run is %d, Unfreeze_batch_size is %d, training for %d Epochs, resulting in a total training step of %d.\\033[0m\" % (num_train, Unfreeze_batch_size, UnFreeze_Epoch, total_step))\n",
        "            print(\"\\033[1;33;44m[Warning] Since the total training step is %d, which is less than the recommended total step %d, it is recommended to set the total epochs to %d.\\033[0m\" % (total_step, wanted_step, wanted_epoch))\n",
        "\n",
        "    #------------------------------------------------------#\n",
        "    #   The backbone feature extraction network features are universal, freezing training can speed up training\n",
        "    #   It can also prevent the weights from being destroyed in the early stages of training.\n",
        "    #   Init_Epoch is the starting epoch\n",
        "    #   Freeze_Epoch is the epoch for freezing training\n",
        "    #   UnFreeze_Epoch is the total training epoch\n",
        "    #   If you encounter OOM or insufficient memory, please reduce the Batch_size\n",
        "    #------------------------------------------------------#\n",
        "    if True:\n",
        "        UnFreeze_flag = False\n",
        "        #------------------------------------#\n",
        "        #   Freeze part of the training\n",
        "        #------------------------------------#\n",
        "        if Freeze_Train:\n",
        "            for param in model.backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        #-------------------------------------------------------------------#\n",
        "        #   If not freezing training, directly set batch_size to Unfreeze_batch_size\n",
        "        #-------------------------------------------------------------------#\n",
        "        batch_size = Freeze_batch_size if Freeze_Train else Unfreeze_batch_size\n",
        "\n",
        "        #-------------------------------------------------------------------#\n",
        "        #   Adjust the learning rate adaptively based on the current batch_size\n",
        "        #-------------------------------------------------------------------#\n",
        "        nbs             = 64\n",
        "        lr_limit_max    = 1e-3 if optimizer_type == 'adam' else 5e-2\n",
        "        lr_limit_min    = 3e-4 if optimizer_type == 'adam' else 5e-4\n",
        "        Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
        "        Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
        "\n",
        "        #---------------------------------------#\n",
        "        #  Select optimizer based on optimizer_type\n",
        "        #---------------------------------------#\n",
        "        pg0, pg1, pg2 = [], [], []\n",
        "        for k, v in model.named_modules():\n",
        "            if hasattr(v, \"bias\") and isinstance(v.bias, nn.Parameter):\n",
        "                pg2.append(v.bias)\n",
        "            if isinstance(v, nn.BatchNorm2d) or \"bn\" in k:\n",
        "                pg0.append(v.weight)\n",
        "            elif hasattr(v, \"weight\") and isinstance(v.weight, nn.Parameter):\n",
        "                pg1.append(v.weight)\n",
        "        optimizer = {\n",
        "            'adam'  : optim.Adam(pg0, Init_lr_fit, betas = (momentum, 0.999)),\n",
        "            'sgd'   : optim.SGD(pg0, Init_lr_fit, momentum = momentum, nesterov=True)\n",
        "        }[optimizer_type]\n",
        "        optimizer.add_param_group({\"params\": pg1, \"weight_decay\": weight_decay})\n",
        "        optimizer.add_param_group({\"params\": pg2})\n",
        "\n",
        "        #---------------------------------------#\n",
        "        #   Get the learning rate decay formula\n",
        "        #---------------------------------------#\n",
        "        lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n",
        "\n",
        "        #---------------------------------------#\n",
        "        #   Determine the length of each epoch\n",
        "        #---------------------------------------#\n",
        "        epoch_step      = num_train // batch_size\n",
        "        epoch_step_val  = num_val // batch_size\n",
        "\n",
        "        if epoch_step == 0 or epoch_step_val == 0:\n",
        "            raise ValueError(\"The dataset is too small to continue training, please expand the dataset.\")\n",
        "\n",
        "        if ema:\n",
        "            ema.updates     = epoch_step * Init_Epoch\n",
        "\n",
        "        #---------------------------------------#\n",
        "        #    Construct dataset loaders.\n",
        "        #---------------------------------------#\n",
        "        train_dataset   = YoloDataset(train_lines, input_shape, num_classes, epoch_length=UnFreeze_Epoch, \\\n",
        "                                        mosaic=mosaic, mixup=mixup, mosaic_prob=mosaic_prob, mixup_prob=mixup_prob, train=True, special_aug_ratio=special_aug_ratio)\n",
        "        val_dataset     = YoloDataset(val_lines, input_shape, num_classes, epoch_length=UnFreeze_Epoch, \\\n",
        "                                        mosaic=False, mixup=False, mosaic_prob=0, mixup_prob=0, train=False, special_aug_ratio=0)\n",
        "\n",
        "        if distributed:\n",
        "            train_sampler   = torch.utils.data.distributed.DistributedSampler(train_dataset, shuffle=True,)\n",
        "            val_sampler     = torch.utils.data.distributed.DistributedSampler(val_dataset, shuffle=False,)\n",
        "            batch_size      = batch_size // ngpus_per_node\n",
        "            shuffle         = False\n",
        "        else:\n",
        "            train_sampler   = None\n",
        "            val_sampler     = None\n",
        "            shuffle         = True\n",
        "\n",
        "        gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True,\n",
        "                                    drop_last=True, collate_fn=yolo_dataset_collate, sampler=train_sampler,\n",
        "                                    worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
        "        gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True,\n",
        "                                    drop_last=True, collate_fn=yolo_dataset_collate, sampler=val_sampler,\n",
        "                                    worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
        "\n",
        "        #----------------------#\n",
        "        #   Record the evaluation mAP curve\n",
        "        #----------------------#\n",
        "        if local_rank == 0:\n",
        "            eval_callback   = EvalCallback(model, input_shape, class_names, num_classes, val_lines, log_dir, Cuda, \\\n",
        "                                            eval_flag=eval_flag, period=eval_period)\n",
        "        else:\n",
        "            eval_callback   = None\n",
        "\n",
        "        #---------------------------------------#\n",
        "        #   Start model training\n",
        "        #---------------------------------------#\n",
        "        for epoch in range(Init_Epoch, UnFreeze_Epoch):\n",
        "            #---------------------------------------#\n",
        "            #   If the model has a frozen part\n",
        "            #   then unfreeze it and set parameters\n",
        "            #---------------------------------------#\n",
        "            if epoch >= Freeze_Epoch and not UnFreeze_flag and Freeze_Train:\n",
        "                batch_size = Unfreeze_batch_size\n",
        "\n",
        "                #-------------------------------------------------------------------#\n",
        "                #   Adjust the learning rate adaptively based on the current batch_size\n",
        "                #-------------------------------------------------------------------#\n",
        "                nbs             = 64\n",
        "                lr_limit_max    = 1e-3 if optimizer_type == 'adam' else 5e-2\n",
        "                lr_limit_min    = 3e-4 if optimizer_type == 'adam' else 5e-4\n",
        "                Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
        "                Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
        "                #---------------------------------------#\n",
        "                #   Get the learning rate decay formula\n",
        "                #---------------------------------------#\n",
        "                lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n",
        "\n",
        "                for param in model.backbone.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "                epoch_step      = num_train // batch_size\n",
        "                epoch_step_val  = num_val // batch_size\n",
        "\n",
        "                if epoch_step == 0 or epoch_step_val == 0:\n",
        "                    raise ValueError(\"The dataset is too small to continue training, please expand the dataset.\")\n",
        "\n",
        "                if ema:\n",
        "                    ema.updates     = epoch_step * epoch\n",
        "\n",
        "                if distributed:\n",
        "                    batch_size  = batch_size // ngpus_per_node\n",
        "\n",
        "                gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True,\n",
        "                                            drop_last=True, collate_fn=yolo_dataset_collate, sampler=train_sampler,\n",
        "                                            worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
        "                gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True,\n",
        "                                            drop_last=True, collate_fn=yolo_dataset_collate, sampler=val_sampler,\n",
        "                                            worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
        "\n",
        "                UnFreeze_flag   = True\n",
        "\n",
        "            gen.dataset.epoch_now       = epoch\n",
        "            gen_val.dataset.epoch_now   = epoch\n",
        "\n",
        "            if distributed:\n",
        "                train_sampler.set_epoch(epoch)\n",
        "\n",
        "            set_optimizer_lr(optimizer, lr_scheduler_func, epoch)\n",
        "            fit_one_epoch(model_train, model, ema, yolo_loss, loss_history, eval_callback, optimizer, epoch, epoch_step, epoch_step_val, gen, gen_val, UnFreeze_Epoch, Cuda, fp16, scaler, save_period, save_dir, local_rank)\n",
        "\n",
        "            if distributed:\n",
        "                dist.barrier()\n",
        "\n",
        "        if local_rank == 0:\n",
        "            loss_history.writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz8sARZ9GCWP"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQmXoQosGKkP"
      },
      "source": [
        "## Predict (Visualisations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTU969v-GRrz"
      },
      "outputs": [],
      "source": [
        "#-----------------------------------------------------------------------#\n",
        "#   predict.py integrates functions such as single image prediction, camera detection, FPS testing, and directory traversal detection\n",
        "#   into one Python file, allowing mode modification by specifying the mode.\n",
        "#-----------------------------------------------------------------------#\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    yolo = YOLO()\n",
        "    #----------------------------------------------------------------------------------------------------------#\n",
        "    #   mode is used to specify the test mode:\n",
        "    #   'predict'           Indicates single image prediction. If you want to modify the prediction process, such as saving images, cropping objects, etc., you can refer to the detailed comments below.\n",
        "    #   'video'             Indicates video detection, which can call the camera or video for detection. For details, see the comments below.\n",
        "    #   'fps'               Indicates testing fps, using the street.jpg image in the img folder. For details, see the comments below.\n",
        "    #   'dir_predict'       Indicates traversing the folder for detection and saving. By default, it traverses the img folder and saves to the img_out folder. For details, see the comments below.\n",
        "    #   'heatmap'           Indicates visualizing the heatmap of the prediction results. For details, see the comments below.\n",
        "    #   'export_onnx'       Indicates exporting the model to onnx, which requires pytorch version 1.7.1 or above.\n",
        "    #----------------------------------------------------------------------------------------------------------#\n",
        "    mode = \"predict\"\n",
        "    #-------------------------------------------------------------------------#\n",
        "    #   crop                Specifies whether to crop the target after single image prediction\n",
        "    #   count               Specifies whether to count the targets\n",
        "    #   crop and count are only effective when mode='predict'\n",
        "    #-------------------------------------------------------------------------#\n",
        "    crop            = False\n",
        "    count           = False\n",
        "    #----------------------------------------------------------------------------------------------------------#\n",
        "    #   video_path          Used to specify the path of the video. When video_path=0, it means detecting the camera.\n",
        "    #                       To detect a video, set it like video_path = \"xxx.mp4\", which means reading the xxx.mp4 file in the root directory.\n",
        "    #   video_save_path     Indicates the path to save the video. When video_save_path=\"\", it means not saving.\n",
        "    #                       To save the video, set it like video_save_path = \"yyy.mp4\", which means saving it as yyy.mp4 in the root directory.\n",
        "    #   video_fps           Used for the fps of the saved video.\n",
        "    #\n",
        "    #   video_path, video_save_path, and video_fps are only effective when mode='video'.\n",
        "    #   To save the video, you need to exit with ctrl+c or run to the last frame to complete the full saving process.\n",
        "    #----------------------------------------------------------------------------------------------------------#\n",
        "    video_path      = 0\n",
        "    video_save_path = \"\"\n",
        "    video_fps       = 25.0\n",
        "    #----------------------------------------------------------------------------------------------------------#\n",
        "    #   test_interval       Used to specify the number of times the image is detected when measuring fps. Theoretically, the larger the test_interval, the more accurate the fps.\n",
        "    #   fps_image_path      Used to specify the image for testing fps.\n",
        "    #\n",
        "    #   test_interval and fps_image_path are only effective when mode='fps'.\n",
        "    #----------------------------------------------------------------------------------------------------------#\n",
        "    test_interval   = 100\n",
        "    fps_image_path  = \"img/street.jpg\"\n",
        "    #-------------------------------------------------------------------------#\n",
        "    #   dir_origin_path     Specifies the folder path for images to be detected\n",
        "    #   dir_save_path       Specifies the path to save the detected images\n",
        "    #\n",
        "    #   dir_origin_path and dir_save_path are only effective when mode='dir_predict'\n",
        "    #-------------------------------------------------------------------------#\n",
        "    dir_origin_path = \"img/\"\n",
        "    dir_save_path   = \"img_out/\"\n",
        "    #-------------------------------------------------------------------------#\n",
        "    #   heatmap_save_path   Specifies the path to save the heatmap, default is saved in model_data\n",
        "    #\n",
        "    #   heatmap_save_path is only effective when mode='heatmap'\n",
        "    #-------------------------------------------------------------------------#\n",
        "    heatmap_save_path = \"model_data/heatmap_vision.png\"\n",
        "    #-------------------------------------------------------------------------#\n",
        "    #   simplify            Specifies whether to use Simplify onnx\n",
        "    #   onnx_save_path      Specifies the path to save the onnx model\n",
        "    #-------------------------------------------------------------------------#\n",
        "    simplify        = True\n",
        "    onnx_save_path  = \"model_data/models.onnx\"\n",
        "\n",
        "    if mode == \"predict\":\n",
        "        '''\n",
        "        1. If you want to save the detected image, you can use r_image.save(\"img.jpg\") to save it. You can modify it directly in predict.py.\n",
        "        2. If you want to get the coordinates of the prediction box, you can go into the yolo.detect_image function and read the values of top, left, bottom, and right in the drawing part.\n",
        "        3. If you want to crop the target using the prediction box, you can go into the yolo.detect_image function and use the top, left, bottom, and right values obtained in the drawing part to crop the original image using matrix operations.\n",
        "        4. If you want to write additional text on the prediction image, such as the number of detected specific targets, you can go into the yolo.detect_image function and judge the predicted_class in the drawing part. For example, you can determine if the current target is a car by judging if predicted_class == 'car', and then record the number. You can use draw.text to write the text.\n",
        "        '''\n",
        "        while True:\n",
        "            img = input('Input image filename:')\n",
        "            try:\n",
        "                image = Image.open(img)\n",
        "            except:\n",
        "                print('Open Error! Try again!')\n",
        "                continue\n",
        "            else:\n",
        "                r_image = yolo.detect_image(image, crop = crop, count=count)\n",
        "                r_image.show()\n",
        "\n",
        "    elif mode == \"video\":\n",
        "        capture = cv2.VideoCapture(video_path)\n",
        "        if video_save_path!=\"\":\n",
        "            fourcc  = cv2.VideoWriter_fourcc(*'XVID')\n",
        "            size    = (int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)), int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "            out     = cv2.VideoWriter(video_save_path, fourcc, video_fps, size)\n",
        "\n",
        "        ref, frame = capture.read()\n",
        "        if not ref:\n",
        "            raise ValueError(\"Failed to correctly read from the camera (video). Please ensure the camera is properly installed (or the video path is correctly specified).\")\n",
        "\n",
        "        fps = 0.0\n",
        "        while(True):\n",
        "            t1 = time.time()\n",
        "            # \n",
        "            ref, frame = capture.read()\n",
        "            if not ref:\n",
        "                break\n",
        "            # BGRtoRGB\n",
        "            frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "            # Image\n",
        "            frame = Image.fromarray(np.uint8(frame))\n",
        "            # \n",
        "            frame = np.array(yolo.detect_image(frame))\n",
        "            # RGBtoBGRopencv\n",
        "            frame = cv2.cvtColor(frame,cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            fps  = ( fps + (1./(time.time()-t1)) ) / 2\n",
        "            print(\"fps= %.2f\"%(fps))\n",
        "            frame = cv2.putText(frame, \"fps= %.2f\"%(fps), (0, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "            cv2.imshow(\"video\",frame)\n",
        "            c= cv2.waitKey(1) & 0xff\n",
        "            if video_save_path!=\"\":\n",
        "                out.write(frame)\n",
        "\n",
        "            if c==27:\n",
        "                capture.release()\n",
        "                break\n",
        "\n",
        "        print(\"Video Detection Done!\")\n",
        "        capture.release()\n",
        "        if video_save_path!=\"\":\n",
        "            print(\"Save processed video to the path :\" + video_save_path)\n",
        "            out.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "    elif mode == \"fps\":\n",
        "        img = Image.open(fps_image_path)\n",
        "        tact_time = yolo.get_FPS(img, test_interval)\n",
        "        print(str(tact_time) + ' seconds, ' + str(1/tact_time) + 'FPS, @batch_size 1')\n",
        "\n",
        "    elif mode == \"dir_predict\":\n",
        "        import os\n",
        "\n",
        "        from tqdm import tqdm\n",
        "\n",
        "        img_names = os.listdir(dir_origin_path)\n",
        "        for img_name in tqdm(img_names):\n",
        "            if img_name.lower().endswith(('.bmp', '.dib', '.png', '.jpg', '.jpeg', '.pbm', '.pgm', '.ppm', '.tif', '.tiff')):\n",
        "                image_path  = os.path.join(dir_origin_path, img_name)\n",
        "                image       = Image.open(image_path)\n",
        "                r_image     = yolo.detect_image(image)\n",
        "                if not os.path.exists(dir_save_path):\n",
        "                    os.makedirs(dir_save_path)\n",
        "                r_image.save(os.path.join(dir_save_path, img_name.replace(\".jpg\", \".png\")), quality=95, subsampling=0)\n",
        "\n",
        "    elif mode == \"heatmap\":\n",
        "        while True:\n",
        "            img = input('Input image filename:')\n",
        "            try:\n",
        "                image = Image.open(img)\n",
        "            except:\n",
        "                print('Open Error! Try again!')\n",
        "                continue\n",
        "            else:\n",
        "                yolo.detect_heatmap(image, heatmap_save_path)\n",
        "\n",
        "    elif mode == \"export_onnx\":\n",
        "        yolo.convert_to_onnx(simplify, onnx_save_path)\n",
        "\n",
        "    else:\n",
        "        raise AssertionError(\"Please specify the correct mode: 'predict', 'video', 'fps', 'heatmap', 'export_onnx', 'dir_predict'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6iiIwPPGDjQ"
      },
      "source": [
        "## Get Map for VOC Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZevMW-j2GVi_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    '''\n",
        "    Recall and Precision are not area concepts like AP, so the network's Recall and Precision values are different at different threshold values (Confidence).\n",
        "    By default, this code calculates the Recall and Precision values corresponding to a threshold value (Confidence) of 0.5.\n",
        "\n",
        "    Due to the limitations of the mAP calculation principle, the network needs to obtain almost all prediction boxes when calculating mAP, so that Recall and Precision values under different threshold conditions can be calculated.\n",
        "    Therefore, the number of boxes in the map_out/detection-results/ txt files obtained by this code will generally be more than those obtained by direct prediction, with the aim of listing all possible prediction boxes.\n",
        "    '''\n",
        "    #------------------------------------------------------------------------------------------------------------------#\n",
        "    #   map_mode is used to specify the content calculated when this file is run\n",
        "    #   map_mode = 0 represents the entire map calculation process, including obtaining prediction results, obtaining ground truth boxes, and calculating VOC_map.\n",
        "    #   map_mode = 1 represents only obtaining prediction results.\n",
        "    #   map_mode = 2 represents only obtaining ground truth boxes.\n",
        "    #   map_mode = 3 represents only calculating VOC_map.\n",
        "    #   map_mode=4 means using the COCO toolbox to calculate the 0.50:0.95 mAP for the current dataset.\n",
        "    #   This requires obtaining prediction results, obtaining ground truth boxes, and installing pycocotools.\n",
        "    #-------------------------------------------------------------------------------------------------------------------#\n",
        "    map_mode        = 0\n",
        "    #--------------------------------------------------------------------------------------#\n",
        "    #   The classes_path here is used to specify the classes for which VOC_map needs to be measured.\n",
        "    #   Generally, it is consistent with the classes_path used for training and prediction.\n",
        "    #--------------------------------------------------------------------------------------#\n",
        "    classes_path    = 'model_data/voc_classes.txt'\n",
        "    #--------------------------------------------------------------------------------------#\n",
        "    #   MINOVERLAP is used to specify the desired mAP0.x. For the meaning of mAP0.x, please search online.\n",
        "    #   For example, to calculate mAP0.75, you can set MINOVERLAP = 0.75.\n",
        "    #\n",
        "    #   When a predicted box has an overlap greater than MINOVERLAP with the ground truth box, the predicted box is considered a positive sample; otherwise, it is considered a negative sample.\n",
        "    #   Therefore, the larger the value of MINOVERLAP, the more accurate the predicted box needs to be to be considered a positive sample, and the lower the calculated mAP value.\n",
        "    #--------------------------------------------------------------------------------------#\n",
        "    MINOVERLAP      = 0.5\n",
        "    #--------------------------------------------------------------------------------------#\n",
        "    #   Due to the limitations of the mAP calculation principle, the network needs to obtain almost all prediction boxes when calculating mAP.\n",
        "    #   Therefore, the value of confidence should be set as low as possible to obtain all possible prediction boxes.\n",
        "    #\n",
        "    #   This value is generally not adjusted. Since calculating mAP requires obtaining almost all prediction boxes, the confidence here should not be changed arbitrarily.\n",
        "    #   To obtain Recall and Precision values at different threshold values, please modify the score_threhold below.\n",
        "    #--------------------------------------------------------------------------------------#\n",
        "    confidence      = 0.001\n",
        "    #--------------------------------------------------------------------------------------#\n",
        "    #   The size of the non-maximum suppression value used during prediction. The larger the value, the less strict the non-maximum suppression.\n",
        "    #\n",
        "    #   This value is generally not adjusted.\n",
        "    #--------------------------------------------------------------------------------------#\n",
        "    nms_iou         = 0.5\n",
        "    #---------------------------------------------------------------------------------------------------------------#\n",
        "    #   Recall and Precision are not area concepts like AP, so the network's Recall and Precision values are different at different threshold values (Confidence).\n",
        "    #\n",
        "    #   By default, this code calculates the Recall and Precision values corresponding to a threshold value (Confidence) of 0.5 (defined here as score_threhold).\n",
        "    #   Since calculating mAP requires obtaining almost all prediction boxes, the confidence value defined above should not be changed arbitrarily.\n",
        "    #   Here, a separate score_threhold is defined to represent the threshold value, so that the Recall and Precision values corresponding to the threshold value can be found when calculating mAP.\n",
        "    #---------------------------------------------------------------------------------------------------------------#\n",
        "    score_threhold  = 0.5\n",
        "    #-------------------------------------------------------#\n",
        "    #   map_vis is used to specify whether to enable visualization of VOC_map calculation\n",
        "    #-------------------------------------------------------#\n",
        "    map_vis         = False\n",
        "    #-------------------------------------------------------#\n",
        "    #   Point to the folder where the VOC dataset is located\n",
        "    #   Default is to point to the VOC dataset in the root directory\n",
        "    #-------------------------------------------------------#\n",
        "    VOCdevkit_path  = 'VOCdevkit'\n",
        "    #-------------------------------------------------------#\n",
        "    #   The folder where the results are output, default is map_out\n",
        "    #-------------------------------------------------------#\n",
        "    map_out_path    = 'map_out'\n",
        "\n",
        "    image_ids = open(os.path.join(VOCdevkit_path, \"VOC2007/ImageSets/Main/test.txt\")).read().strip().split()\n",
        "\n",
        "    if not os.path.exists(map_out_path):\n",
        "        os.makedirs(map_out_path)\n",
        "    if not os.path.exists(os.path.join(map_out_path, 'ground-truth')):\n",
        "        os.makedirs(os.path.join(map_out_path, 'ground-truth'))\n",
        "    if not os.path.exists(os.path.join(map_out_path, 'detection-results')):\n",
        "        os.makedirs(os.path.join(map_out_path, 'detection-results'))\n",
        "    if not os.path.exists(os.path.join(map_out_path, 'images-optional')):\n",
        "        os.makedirs(os.path.join(map_out_path, 'images-optional'))\n",
        "\n",
        "    class_names, _ = get_classes(classes_path)\n",
        "\n",
        "    if map_mode == 0 or map_mode == 1:\n",
        "        print(\"Load model.\")\n",
        "        yolo = YOLO(confidence = confidence, nms_iou = nms_iou)\n",
        "        print(\"Load model done.\")\n",
        "\n",
        "        print(\"Get predict result.\")\n",
        "        for image_id in tqdm(image_ids):\n",
        "            image_path  = os.path.join(VOCdevkit_path, \"VOC2007/JPEGImages/\"+image_id+\".jpg\")\n",
        "            image       = Image.open(image_path)\n",
        "            if map_vis:\n",
        "                image.save(os.path.join(map_out_path, \"images-optional/\" + image_id + \".jpg\"))\n",
        "            yolo.get_map_txt(image_id, image, class_names, map_out_path)\n",
        "        print(\"Get predict result done.\")\n",
        "\n",
        "    if map_mode == 0 or map_mode == 2:\n",
        "        print(\"Get ground truth result.\")\n",
        "        for image_id in tqdm(image_ids):\n",
        "            with open(os.path.join(map_out_path, \"ground-truth/\"+image_id+\".txt\"), \"w\") as new_f:\n",
        "                root = ET.parse(os.path.join(VOCdevkit_path, \"VOC2007/Annotations/\"+image_id+\".xml\")).getroot()\n",
        "                for obj in root.findall('object'):\n",
        "                    difficult_flag = False\n",
        "                    if obj.find('difficult')!=None:\n",
        "                        difficult = obj.find('difficult').text\n",
        "                        if int(difficult)==1:\n",
        "                            difficult_flag = True\n",
        "                    obj_name = obj.find('name').text\n",
        "                    if obj_name not in class_names:\n",
        "                        continue\n",
        "                    bndbox  = obj.find('bndbox')\n",
        "                    left    = bndbox.find('xmin').text\n",
        "                    top     = bndbox.find('ymin').text\n",
        "                    right   = bndbox.find('xmax').text\n",
        "                    bottom  = bndbox.find('ymax').text\n",
        "\n",
        "                    if difficult_flag:\n",
        "                        new_f.write(\"%s %s %s %s %s difficult\\n\" % (obj_name, left, top, right, bottom))\n",
        "                    else:\n",
        "                        new_f.write(\"%s %s %s %s %s\\n\" % (obj_name, left, top, right, bottom))\n",
        "        print(\"Get ground truth result done.\")\n",
        "\n",
        "    if map_mode == 0 or map_mode == 3:\n",
        "        print(\"Get map.\")\n",
        "        get_map(MINOVERLAP, True, score_threhold = score_threhold, path = map_out_path)\n",
        "        print(\"Get map done.\")\n",
        "\n",
        "    if map_mode == 4:\n",
        "        print(\"Get map.\")\n",
        "        get_coco_map(class_names = class_names, path = map_out_path)\n",
        "        print(\"Get map done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNS9sqt5GG2g"
      },
      "source": [
        "## Get Map for COCO Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhtx-Wu9GDJ2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------------#\n",
        "#   map_mode\n",
        "#   map_mode0mapmap\n",
        "#   map_mode1\n",
        "#   map_mode2map\n",
        "#---------------------------------------------------------------------------#\n",
        "map_mode            = 0\n",
        "#-------------------------------------------------------#\n",
        "#   \n",
        "#-------------------------------------------------------#\n",
        "cocoGt_path         = 'coco_dataset/annotations/instances_val2017.json'\n",
        "dataset_img_path    = 'coco_dataset/val2017'\n",
        "#-------------------------------------------------------#\n",
        "#   map_out\n",
        "#-------------------------------------------------------#\n",
        "temp_save_path      = 'map_out/coco_eval'\n",
        "\n",
        "class mAP_YOLO(YOLO):\n",
        "    #---------------------------------------------------#\n",
        "    #   \n",
        "    #---------------------------------------------------#\n",
        "    def detect_image(self, image_id, image, results, clsid2catid):\n",
        "        #---------------------------------------------------#\n",
        "        #   \n",
        "        #---------------------------------------------------#\n",
        "        image_shape = np.array(np.shape(image)[0:2])\n",
        "        #---------------------------------------------------------#\n",
        "        #   RGB\n",
        "        #   RGBRGB\n",
        "        #---------------------------------------------------------#\n",
        "        image       = cvtColor(image)\n",
        "        #---------------------------------------------------------#\n",
        "        #   resize\n",
        "        #   resize\n",
        "        #---------------------------------------------------------#\n",
        "        image_data  = resize_image(image, (self.input_shape[1],self.input_shape[0]), self.letterbox_image)\n",
        "        #---------------------------------------------------------#\n",
        "        #   batch_size\n",
        "        #---------------------------------------------------------#\n",
        "        image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, dtype='float32')), (2, 0, 1)), 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            images = torch.from_numpy(image_data)\n",
        "            if self.cuda:\n",
        "                images = images.cuda()\n",
        "            #---------------------------------------------------------#\n",
        "            #   \n",
        "            #---------------------------------------------------------#\n",
        "            outputs = self.net(images)\n",
        "            outputs = self.bbox_util.decode_box(outputs)\n",
        "            #---------------------------------------------------------#\n",
        "            #   \n",
        "            #---------------------------------------------------------#\n",
        "            outputs = self.bbox_util.non_max_suppression(outputs, self.num_classes, self.input_shape,\n",
        "                        image_shape, self.letterbox_image, conf_thres = self.confidence, nms_thres = self.nms_iou)\n",
        "\n",
        "            if outputs[0] is None:\n",
        "                return outputs\n",
        "\n",
        "            top_label   = np.array(outputs[0][:, 5], dtype = 'int32')\n",
        "            top_conf    = outputs[0][:, 4]\n",
        "            top_boxes   = outputs[0][:, :4]\n",
        "\n",
        "        for i, c in enumerate(top_label):\n",
        "            result                      = {}\n",
        "            top, left, bottom, right    = top_boxes[i]\n",
        "\n",
        "            result[\"image_id\"]      = int(image_id)\n",
        "            result[\"category_id\"]   = clsid2catid[c]\n",
        "            result[\"bbox\"]          = [float(left),float(top),float(right-left),float(bottom-top)]\n",
        "            result[\"score\"]         = float(top_conf[i])\n",
        "            results.append(result)\n",
        "        return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(temp_save_path):\n",
        "        os.makedirs(temp_save_path)\n",
        "\n",
        "    cocoGt      = COCO(cocoGt_path)\n",
        "    ids         = list(cocoGt.imgToAnns.keys())\n",
        "    clsid2catid = cocoGt.getCatIds()\n",
        "\n",
        "    if map_mode == 0 or map_mode == 1:\n",
        "        yolo = mAP_YOLO(confidence = 0.001, nms_iou = 0.65)\n",
        "\n",
        "        with open(os.path.join(temp_save_path, 'eval_results.json'),\"w\") as f:\n",
        "            results = []\n",
        "            for image_id in tqdm(ids):\n",
        "                image_path  = os.path.join(dataset_img_path, cocoGt.loadImgs(image_id)[0]['file_name'])\n",
        "                image       = Image.open(image_path)\n",
        "                results     = yolo.detect_image(image_id, image, results, clsid2catid)\n",
        "            json.dump(results, f)\n",
        "\n",
        "    if map_mode == 0 or map_mode == 2:\n",
        "        cocoDt      = cocoGt.loadRes(os.path.join(temp_save_path, 'eval_results.json'))\n",
        "        cocoEval    = COCOeval(cocoGt, cocoDt, 'bbox')\n",
        "        cocoEval.evaluate()\n",
        "        cocoEval.accumulate()\n",
        "        cocoEval.summarize()\n",
        "        print(\"Get map done.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "multitask_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
